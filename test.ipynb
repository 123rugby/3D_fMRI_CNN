{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TF net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Start working on fold(s) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Beginning fold 1 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/0\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params: 185650\n",
      "2017-08-19 10:17:45.775293: step 0, loss = 0.69 (32.2 examples/sec; 1.985 sec/batch)\n",
      "2017-08-19 10:18:27.933624: step 100, loss = 0.70 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-19 10:19:09.432385: step 200, loss = 0.68 (160.7 examples/sec; 0.398 sec/batch)\n",
      "2017-08-19 10:19:51.230944: step 300, loss = 0.64 (159.9 examples/sec; 0.400 sec/batch)\n",
      "Epoch 1 of 10 took 155.092s\n",
      "  training loss:\t\t0.678286\n",
      "  training accuracy:\t\t56.96 %\n",
      "  validation loss:\t\t0.589545\n",
      "  validation accuracy:\t\t79.88 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.668569\n",
      "  test accuracy:\t\t58.57 %\n",
      "2017-08-19 10:20:45.707862: step 400, loss = 0.66 (163.9 examples/sec; 0.391 sec/batch)\n",
      "2017-08-19 10:21:26.621349: step 500, loss = 0.65 (163.3 examples/sec; 0.392 sec/batch)\n",
      "2017-08-19 10:22:07.557372: step 600, loss = 0.62 (160.8 examples/sec; 0.398 sec/batch)\n",
      "2017-08-19 10:22:48.732311: step 700, loss = 0.58 (156.2 examples/sec; 0.410 sec/batch)\n",
      "Epoch 2 of 10 took 150.430s\n",
      "  training loss:\t\t0.631202\n",
      "  training accuracy:\t\t67.90 %\n",
      "  validation loss:\t\t0.661918\n",
      "  validation accuracy:\t\t71.76 %\n",
      "2017-08-19 10:23:36.200647: step 800, loss = 0.56 (167.2 examples/sec; 0.383 sec/batch)\n",
      "2017-08-19 10:24:17.690836: step 900, loss = 0.58 (161.7 examples/sec; 0.396 sec/batch)\n",
      "2017-08-19 10:24:59.150959: step 1000, loss = 0.49 (163.9 examples/sec; 0.390 sec/batch)\n",
      "Epoch 3 of 10 took 154.121s\n",
      "  training loss:\t\t0.562562\n",
      "  training accuracy:\t\t75.75 %\n",
      "  validation loss:\t\t0.676268\n",
      "  validation accuracy:\t\t67.84 %\n",
      "2017-08-19 10:25:49.457422: step 1100, loss = 0.43 (164.2 examples/sec; 0.390 sec/batch)\n",
      "2017-08-19 10:26:30.542689: step 1200, loss = 0.30 (162.6 examples/sec; 0.394 sec/batch)\n",
      "2017-08-19 10:27:11.676722: step 1300, loss = 0.39 (163.3 examples/sec; 0.392 sec/batch)\n",
      "2017-08-19 10:27:52.839581: step 1400, loss = 0.44 (165.7 examples/sec; 0.386 sec/batch)\n",
      "Epoch 4 of 10 took 150.940s\n",
      "  training loss:\t\t0.413035\n",
      "  training accuracy:\t\t88.48 %\n",
      "  validation loss:\t\t0.670325\n",
      "  validation accuracy:\t\t74.28 %\n",
      "2017-08-19 10:28:40.800439: step 1500, loss = 0.40 (165.0 examples/sec; 0.388 sec/batch)\n",
      "2017-08-19 10:29:22.054819: step 1600, loss = 0.28 (159.5 examples/sec; 0.401 sec/batch)\n",
      "2017-08-19 10:30:03.527887: step 1700, loss = 0.31 (163.1 examples/sec; 0.392 sec/batch)\n",
      "Epoch 5 of 10 took 151.933s\n",
      "  training loss:\t\t0.377189\n",
      "  training accuracy:\t\t91.02 %\n",
      "  validation loss:\t\t0.732823\n",
      "  validation accuracy:\t\t73.40 %\n",
      "2017-08-19 10:30:51.647993: step 1800, loss = 0.31 (164.4 examples/sec; 0.389 sec/batch)\n",
      "2017-08-19 10:31:33.192183: step 1900, loss = 0.34 (162.4 examples/sec; 0.394 sec/batch)\n",
      "2017-08-19 10:32:14.629114: step 2000, loss = 0.31 (158.5 examples/sec; 0.404 sec/batch)\n",
      "2017-08-19 10:32:58.455523: step 2100, loss = 0.33 (160.5 examples/sec; 0.399 sec/batch)\n",
      "Epoch 6 of 10 took 154.389s\n",
      "  training loss:\t\t0.352325\n",
      "  training accuracy:\t\t92.62 %\n",
      "  validation loss:\t\t0.758746\n",
      "  validation accuracy:\t\t73.44 %\n",
      "2017-08-19 10:33:46.201981: step 2200, loss = 0.30 (161.2 examples/sec; 0.397 sec/batch)\n",
      "2017-08-19 10:34:28.351976: step 2300, loss = 0.30 (162.0 examples/sec; 0.395 sec/batch)\n",
      "2017-08-19 10:35:09.693153: step 2400, loss = 0.46 (163.7 examples/sec; 0.391 sec/batch)\n",
      "Epoch 7 of 10 took 152.620s\n",
      "  training loss:\t\t0.339547\n",
      "  training accuracy:\t\t93.68 %\n",
      "  validation loss:\t\t0.746292\n",
      "  validation accuracy:\t\t74.20 %\n",
      "2017-08-19 10:35:57.754087: step 2500, loss = 0.35 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-19 10:36:39.005462: step 2600, loss = 0.44 (157.3 examples/sec; 0.407 sec/batch)\n",
      "2017-08-19 10:37:20.161487: step 2700, loss = 0.31 (162.3 examples/sec; 0.394 sec/batch)\n",
      "2017-08-19 10:38:02.013037: step 2800, loss = 0.26 (161.3 examples/sec; 0.397 sec/batch)\n",
      "Epoch 8 of 10 took 151.708s\n",
      "  training loss:\t\t0.336734\n",
      "  training accuracy:\t\t93.82 %\n",
      "  validation loss:\t\t0.757746\n",
      "  validation accuracy:\t\t74.16 %\n",
      "2017-08-19 10:38:49.788935: step 2900, loss = 0.31 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-19 10:39:31.037716: step 3000, loss = 0.37 (163.3 examples/sec; 0.392 sec/batch)\n",
      "2017-08-19 10:40:13.731955: step 3100, loss = 0.30 (162.7 examples/sec; 0.393 sec/batch)\n",
      "Epoch 9 of 10 took 152.774s\n",
      "  training loss:\t\t0.337831\n",
      "  training accuracy:\t\t93.74 %\n",
      "  validation loss:\t\t0.748327\n",
      "  validation accuracy:\t\t74.28 %\n",
      "2017-08-19 10:41:01.476300: step 3200, loss = 0.40 (161.4 examples/sec; 0.397 sec/batch)\n",
      "2017-08-19 10:41:42.733922: step 3300, loss = 0.28 (162.6 examples/sec; 0.394 sec/batch)\n",
      "2017-08-19 10:42:24.278024: step 3400, loss = 0.34 (162.4 examples/sec; 0.394 sec/batch)\n",
      "2017-08-19 10:43:05.631438: step 3500, loss = 0.31 (162.5 examples/sec; 0.394 sec/batch)\n",
      "Epoch 10 of 10 took 151.674s\n",
      "  training loss:\t\t0.334318\n",
      "  training accuracy:\t\t93.91 %\n",
      "  validation loss:\t\t0.750541\n",
      "  validation accuracy:\t\t74.43 %\n",
      "Beginning fold 2 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/1\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-19 10:43:37.353115: step 0, loss = 0.70 (46.4 examples/sec; 1.380 sec/batch)\n",
      "2017-08-19 10:44:20.000584: step 100, loss = 0.67 (162.7 examples/sec; 0.393 sec/batch)\n",
      "2017-08-19 10:45:00.915741: step 200, loss = 0.69 (167.3 examples/sec; 0.382 sec/batch)\n",
      "2017-08-19 10:45:42.170119: step 300, loss = 0.63 (162.6 examples/sec; 0.394 sec/batch)\n",
      "Epoch 1 of 10 took 150.673s\n",
      "  training loss:\t\t0.679893\n",
      "  training accuracy:\t\t57.52 %\n",
      "  validation loss:\t\t0.563739\n",
      "  validation accuracy:\t\t81.15 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.746181\n",
      "  test accuracy:\t\t52.43 %\n",
      "2017-08-19 10:46:37.865831: step 400, loss = 0.68 (161.1 examples/sec; 0.397 sec/batch)\n",
      "2017-08-19 10:47:19.061624: step 500, loss = 0.66 (161.5 examples/sec; 0.396 sec/batch)\n",
      "2017-08-19 10:48:00.421526: step 600, loss = 0.64 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 2 of 10 took 148.063s\n",
      "  training loss:\t\t0.605517\n",
      "  training accuracy:\t\t72.44 %\n",
      "  validation loss:\t\t0.544289\n",
      "  validation accuracy:\t\t78.54 %\n",
      "2017-08-19 10:48:48.670811: step 700, loss = 0.56 (162.4 examples/sec; 0.394 sec/batch)\n",
      "2017-08-19 10:49:30.049491: step 800, loss = 0.42 (161.9 examples/sec; 0.395 sec/batch)\n",
      "2017-08-19 10:50:11.478302: step 900, loss = 0.41 (164.9 examples/sec; 0.388 sec/batch)\n",
      "2017-08-19 10:50:53.103611: step 1000, loss = 0.30 (163.8 examples/sec; 0.391 sec/batch)\n",
      "Epoch 3 of 10 took 150.724s\n",
      "  training loss:\t\t0.428273\n",
      "  training accuracy:\t\t86.98 %\n",
      "  validation loss:\t\t1.305253\n",
      "  validation accuracy:\t\t44.41 %\n",
      "2017-08-19 10:51:43.167768: step 1100, loss = 0.24 (163.2 examples/sec; 0.392 sec/batch)\n",
      "2017-08-19 10:52:24.798666: step 1200, loss = 0.36 (160.3 examples/sec; 0.399 sec/batch)\n",
      "2017-08-19 10:53:06.193911: step 1300, loss = 0.25 (144.7 examples/sec; 0.442 sec/batch)\n",
      "Epoch 4 of 10 took 148.523s\n",
      "  training loss:\t\t0.264923\n",
      "  training accuracy:\t\t97.51 %\n",
      "  validation loss:\t\t0.848937\n",
      "  validation accuracy:\t\t72.26 %\n",
      "2017-08-19 10:53:54.449662: step 1400, loss = 0.23 (161.5 examples/sec; 0.396 sec/batch)\n",
      "2017-08-19 10:54:35.672161: step 1500, loss = 0.28 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-19 10:55:17.063504: step 1600, loss = 0.23 (167.3 examples/sec; 0.383 sec/batch)\n",
      "2017-08-19 10:55:58.331868: step 1700, loss = 0.25 (162.0 examples/sec; 0.395 sec/batch)\n",
      "Epoch 5 of 10 took 148.298s\n",
      "  training loss:\t\t0.249905\n",
      "  training accuracy:\t\t98.56 %\n",
      "  validation loss:\t\t1.187507\n",
      "  validation accuracy:\t\t55.00 %\n",
      "2017-08-19 10:56:46.593084: step 1800, loss = 0.30 (158.4 examples/sec; 0.404 sec/batch)\n",
      "2017-08-19 10:57:27.985524: step 1900, loss = 0.23 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-19 10:58:09.083682: step 2000, loss = 0.26 (162.0 examples/sec; 0.395 sec/batch)\n",
      "Epoch 6 of 10 took 150.658s\n",
      "  training loss:\t\t0.239837\n",
      "  training accuracy:\t\t99.08 %\n",
      "  validation loss:\t\t0.986283\n",
      "  validation accuracy:\t\t66.84 %\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/')\n",
    "import os\n",
    "from tf_trainer import Trainer\n",
    "import logging \n",
    "from tf_model import TFModel\n",
    "from tf_dataset import TFDataset\n",
    "from tf_trainer import log_info_string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,2'\n",
    "\n",
    "FLAGS.initial_learning_rate = 0.0001\n",
    "FLAGS.batch_size = 64\n",
    "FLAGS.num_gpus = 1\n",
    "FLAGS.num_epochs_per_decay = 3\n",
    "FLAGS.num_time_steps = 64\n",
    "FLAGS.data_dir = '/braintree/data2/active/users/bashivan/Data/fmri_conv'\n",
    "\n",
    "\n",
    "if FLAGS.fold_to_run == -1:\n",
    "    fold_to_run = range(FLAGS.num_folds)\n",
    "else:\n",
    "    fold_to_run = [FLAGS.fold_to_run]\n",
    "\n",
    "# Load the dataset\n",
    "fold_pairs = []\n",
    "\n",
    "model = TFModel()\n",
    "dataset = TFDataset(data_dir=FLAGS.data_dir)\n",
    "tr = Trainer(model=model, dataset=dataset)\n",
    "print(\"Loading data...\")\n",
    "tr.load_data(random=False)\n",
    "\n",
    "sub_nums = tr.subjects\n",
    "subs_in_fold = np.ceil(np.max(sub_nums) / float(10))\n",
    "# n-fold cross validation\n",
    "fold_results = []\n",
    "for i in fold_to_run:\n",
    "    '''\n",
    "    for each kfold selects fold window to collect indices for test dataset and the rest becomes train\n",
    "    '''\n",
    "    test_ids = np.bitwise_and(sub_nums >= subs_in_fold * (i), sub_nums < subs_in_fold * (i + 1))\n",
    "    train_ids = ~ test_ids\n",
    "    fold_pairs.append((np.nonzero(train_ids)[0], np.nonzero(test_ids)[0]))\n",
    "\n",
    "train_dir = FLAGS.train_dir\n",
    "log_info_string('Start working on fold(s) {0}'.format(fold_to_run))\n",
    "for fold_num, fold in enumerate([fold_pairs[i] for i in fold_to_run]):\n",
    "    log_info_string('Beginning fold {0} out of {1}'.format(fold_num + 1, len(fold_pairs)))\n",
    "    FLAGS.train_dir = os.path.join(train_dir, str(fold_num))\n",
    "\n",
    "    print('Splitting the data...')\n",
    "    tr.split_data(fold)\n",
    "    print('Preprocessing data...')\n",
    "    tr.preprocess_data()\n",
    "\n",
    "    fold_results.append(tr.train(fold_num=fold_num))\n",
    "\n",
    "# Aggregate results and save as a pickle\n",
    "fold_results = pd.concat(fold_results)\n",
    "fold_results.to_pickle(\n",
    "'cnn_{0}_results_sgd_{1}_fold{2}.pkl'.format(FLAGS.model_type, FLAGS.initial_learning_rate, ''.join([str(i) for i in fold_to_run])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv_orig\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/0/0\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "total_params: 185650\n",
      "2017-08-19 09:55:45.806484: step 0, loss = 0.69 (43.8 examples/sec; 1.461 sec/batch)\n",
      "2017-08-19 09:57:06.799842: step 100, loss = 0.71 (174.5 examples/sec; 0.367 sec/batch)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-728cce94e669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, fold_num, clean)\u001b[0m\n\u001b[1;32m    539\u001b[0m           \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m           _, l, acc = sess.run([train_op, loss, batch_accuracy],\n\u001b[0;32m--> 541\u001b[0;31m                                feed_dict={inputs_ph: inputs, labels_ph: targets})\n\u001b[0m\u001b[1;32m    542\u001b[0m           \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m           \u001b[0mepoch_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "fold_num = 0\n",
    "fold = fold_pairs[0]\n",
    "\n",
    "FLAGS.train_dir = os.path.join(FLAGS.train_dir, str(fold_num))\n",
    "\n",
    "print('Splitting the data...')\n",
    "tr.split_data(fold)\n",
    "print('Preprocessing data...')\n",
    "tr.preprocess_data()\n",
    "\n",
    "tr.train(fold_num=fold_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named _lib._version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b1af3495e789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN_copy/results/conv_2_1/cnn_lstm_results_adam_0.001_LSO_fold1.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/scipy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyVersion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_NumpyVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_NumpyVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__numpy_version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m'1.8.2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named _lib._version"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data = cPickle.load(open('/braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN_copy/results/conv_2_1/cnn_lstm_results_adam_0.001_LSO_fold1.mat'))\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import pandas as pd\n",
    "\n",
    "results = cPickle.load(open('/braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/cnn_lstm_results_adam_0.0001_10fold_.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>training_acc</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.553496</td>\n",
       "      <td>0.688185</td>\n",
       "      <td>0.522710</td>\n",
       "      <td>0.694362</td>\n",
       "      <td>0.387447</td>\n",
       "      <td>0.711750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.607521</td>\n",
       "      <td>0.803726</td>\n",
       "      <td>0.709158</td>\n",
       "      <td>0.586928</td>\n",
       "      <td>0.626589</td>\n",
       "      <td>0.782541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.607521</td>\n",
       "      <td>0.803726</td>\n",
       "      <td>0.919431</td>\n",
       "      <td>0.342473</td>\n",
       "      <td>0.606462</td>\n",
       "      <td>0.928678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.627913</td>\n",
       "      <td>0.990931</td>\n",
       "      <td>0.984994</td>\n",
       "      <td>0.249511</td>\n",
       "      <td>0.627648</td>\n",
       "      <td>0.914467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.627913</td>\n",
       "      <td>0.990931</td>\n",
       "      <td>0.987840</td>\n",
       "      <td>0.243746</td>\n",
       "      <td>0.596663</td>\n",
       "      <td>0.986135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_acc  test_loss  training_acc  training_loss  valid_acc  valid_loss\n",
       "0  0.553496   0.688185      0.522710       0.694362   0.387447    0.711750\n",
       "1  0.607521   0.803726      0.709158       0.586928   0.626589    0.782541\n",
       "2  0.607521   0.803726      0.919431       0.342473   0.606462    0.928678\n",
       "3  0.627913   0.990931      0.984994       0.249511   0.627648    0.914467\n",
       "4  0.627913   0.990931      0.987840       0.243746   0.596663    0.986135"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
