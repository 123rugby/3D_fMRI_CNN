{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TF net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Start working on fold(s) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Beginning fold 1 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/0\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params: 185650\n",
      "2017-08-20 15:29:15.141713: step 0, loss = 0.69 (26.4 examples/sec; 2.421 sec/batch)\n",
      "2017-08-20 15:30:00.739742: step 100, loss = 0.69 (160.4 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 15:30:42.964705: step 200, loss = 0.67 (164.4 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 15:31:29.863257: step 300, loss = 0.68 (162.4 examples/sec; 0.394 sec/batch)\n",
      "Epoch 1 of 10 took 168.975s\n",
      "  training loss:\t\t0.680416\n",
      "  training accuracy:\t\t56.93 %\n",
      "  validation loss:\t\t0.640002\n",
      "  validation accuracy:\t\t67.95 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.670807\n",
      "  test accuracy:\t\t62.00 %\n",
      "2017-08-20 15:32:29.935195: step 400, loss = 0.64 (162.5 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 15:33:16.532635: step 500, loss = 0.60 (160.2 examples/sec; 0.400 sec/batch)\n",
      "2017-08-20 15:34:03.609891: step 600, loss = 0.62 (161.9 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 15:34:52.646531: step 700, loss = 0.69 (164.5 examples/sec; 0.389 sec/batch)\n",
      "Epoch 2 of 10 took 173.862s\n",
      "  training loss:\t\t0.638097\n",
      "  training accuracy:\t\t67.64 %\n",
      "  validation loss:\t\t0.667709\n",
      "  validation accuracy:\t\t62.35 %\n",
      "2017-08-20 15:35:54.309709: step 800, loss = 0.62 (162.6 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 15:36:52.399462: step 900, loss = 0.51 (157.1 examples/sec; 0.407 sec/batch)\n",
      "2017-08-20 15:37:56.475168: step 1000, loss = 0.45 (163.5 examples/sec; 0.391 sec/batch)\n",
      "Epoch 3 of 10 took 207.911s\n",
      "  training loss:\t\t0.560044\n",
      "  training accuracy:\t\t76.66 %\n",
      "  validation loss:\t\t0.849926\n",
      "  validation accuracy:\t\t52.17 %\n",
      "2017-08-20 15:38:57.876093: step 1100, loss = 0.43 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 15:39:50.392700: step 1200, loss = 0.52 (164.3 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 15:40:46.755297: step 1300, loss = 0.41 (167.4 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 15:41:41.859190: step 1400, loss = 0.40 (162.6 examples/sec; 0.394 sec/batch)\n",
      "Epoch 4 of 10 took 202.566s\n",
      "  training loss:\t\t0.425809\n",
      "  training accuracy:\t\t88.00 %\n",
      "  validation loss:\t\t0.928726\n",
      "  validation accuracy:\t\t50.76 %\n",
      "2017-08-20 15:42:47.473143: step 1500, loss = 0.34 (161.1 examples/sec; 0.397 sec/batch)\n",
      "2017-08-20 15:43:45.998655: step 1600, loss = 0.37 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 15:44:44.151033: step 1700, loss = 0.40 (163.0 examples/sec; 0.393 sec/batch)\n",
      "Epoch 5 of 10 took 208.344s\n",
      "  training loss:\t\t0.391405\n",
      "  training accuracy:\t\t90.16 %\n",
      "  validation loss:\t\t1.033775\n",
      "  validation accuracy:\t\t49.50 %\n",
      "2017-08-20 15:45:47.671202: step 1800, loss = 0.36 (160.9 examples/sec; 0.398 sec/batch)\n",
      "2017-08-20 15:46:45.031773: step 1900, loss = 0.31 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 15:47:37.750676: step 2000, loss = 0.34 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 15:48:32.695286: step 2100, loss = 0.33 (161.7 examples/sec; 0.396 sec/batch)\n",
      "Epoch 6 of 10 took 205.945s\n",
      "  training loss:\t\t0.358821\n",
      "  training accuracy:\t\t91.96 %\n",
      "  validation loss:\t\t1.091620\n",
      "  validation accuracy:\t\t48.78 %\n",
      "2017-08-20 15:49:38.842787: step 2200, loss = 0.36 (166.0 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 15:50:32.157752: step 2300, loss = 0.28 (162.6 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 15:51:30.925277: step 2400, loss = 0.28 (159.5 examples/sec; 0.401 sec/batch)\n",
      "Epoch 7 of 10 took 208.583s\n",
      "  training loss:\t\t0.341140\n",
      "  training accuracy:\t\t93.19 %\n",
      "  validation loss:\t\t1.084347\n",
      "  validation accuracy:\t\t49.28 %\n",
      "2017-08-20 15:52:41.951759: step 2500, loss = 0.33 (163.5 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 15:53:40.030078: step 2600, loss = 0.31 (162.7 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 15:54:36.148282: step 2700, loss = 0.36 (161.8 examples/sec; 0.396 sec/batch)\n",
      "2017-08-20 15:55:32.149311: step 2800, loss = 0.30 (164.4 examples/sec; 0.389 sec/batch)\n",
      "Epoch 8 of 10 took 210.812s\n",
      "  training loss:\t\t0.339005\n",
      "  training accuracy:\t\t93.25 %\n",
      "  validation loss:\t\t1.113590\n",
      "  validation accuracy:\t\t48.48 %\n",
      "2017-08-20 15:56:42.714689: step 2900, loss = 0.32 (165.5 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 15:57:37.676115: step 3000, loss = 0.30 (161.5 examples/sec; 0.396 sec/batch)\n",
      "2017-08-20 15:58:34.770174: step 3100, loss = 0.30 (160.5 examples/sec; 0.399 sec/batch)\n",
      "Epoch 9 of 10 took 212.534s\n",
      "  training loss:\t\t0.336326\n",
      "  training accuracy:\t\t93.45 %\n",
      "  validation loss:\t\t1.101535\n",
      "  validation accuracy:\t\t48.74 %\n",
      "2017-08-20 15:59:43.298981: step 3200, loss = 0.30 (162.2 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 16:00:39.096717: step 3300, loss = 0.32 (159.7 examples/sec; 0.401 sec/batch)\n",
      "2017-08-20 16:01:36.608838: step 3400, loss = 0.29 (164.8 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 16:02:31.388895: step 3500, loss = 0.43 (162.8 examples/sec; 0.393 sec/batch)\n",
      "Epoch 10 of 10 took 208.206s\n",
      "  training loss:\t\t0.334178\n",
      "  training accuracy:\t\t93.67 %\n",
      "  validation loss:\t\t1.114612\n",
      "  validation accuracy:\t\t48.82 %\n",
      "Beginning fold 2 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/1\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 16:03:40.600021: step 0, loss = 0.69 (45.1 examples/sec; 1.420 sec/batch)\n",
      "2017-08-20 16:04:23.235531: step 100, loss = 0.69 (168.1 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 16:05:04.419077: step 200, loss = 0.65 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 16:05:48.882968: step 300, loss = 0.65 (163.7 examples/sec; 0.391 sec/batch)\n",
      "Epoch 1 of 10 took 159.493s\n",
      "  training loss:\t\t0.669659\n",
      "  training accuracy:\t\t60.56 %\n",
      "  validation loss:\t\t0.709189\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.754672\n",
      "  test accuracy:\t\t55.00 %\n",
      "2017-08-20 16:06:49.928372: step 400, loss = 0.59 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 16:07:39.867306: step 500, loss = 0.61 (164.3 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 16:08:32.404139: step 600, loss = 0.69 (165.4 examples/sec; 0.387 sec/batch)\n",
      "Epoch 2 of 10 took 182.143s\n",
      "  training loss:\t\t0.601473\n",
      "  training accuracy:\t\t73.09 %\n",
      "  validation loss:\t\t0.680156\n",
      "  validation accuracy:\t\t72.47 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.824423\n",
      "  test accuracy:\t\t55.38 %\n",
      "2017-08-20 16:09:43.286942: step 700, loss = 0.44 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:10:35.213592: step 800, loss = 0.50 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:11:32.400338: step 900, loss = 0.53 (167.0 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 16:12:29.373940: step 1000, loss = 0.44 (163.0 examples/sec; 0.393 sec/batch)\n",
      "Epoch 3 of 10 took 196.349s\n",
      "  training loss:\t\t0.477934\n",
      "  training accuracy:\t\t83.85 %\n",
      "  validation loss:\t\t0.893590\n",
      "  validation accuracy:\t\t66.11 %\n",
      "2017-08-20 16:13:44.916761: step 1100, loss = 0.33 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:14:54.870010: step 1200, loss = 0.35 (166.6 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 16:15:57.642562: step 1300, loss = 0.25 (166.9 examples/sec; 0.383 sec/batch)\n",
      "Epoch 4 of 10 took 227.815s\n",
      "  training loss:\t\t0.320846\n",
      "  training accuracy:\t\t94.29 %\n",
      "  validation loss:\t\t0.824104\n",
      "  validation accuracy:\t\t69.44 %\n",
      "2017-08-20 16:16:56.342187: step 1400, loss = 0.27 (167.0 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 16:17:47.964927: step 1500, loss = 0.31 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:18:45.482020: step 1600, loss = 0.26 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 16:19:55.156838: step 1700, loss = 0.26 (164.8 examples/sec; 0.388 sec/batch)\n",
      "Epoch 5 of 10 took 214.843s\n",
      "  training loss:\t\t0.292635\n",
      "  training accuracy:\t\t95.92 %\n",
      "  validation loss:\t\t0.826335\n",
      "  validation accuracy:\t\t70.69 %\n",
      "2017-08-20 16:21:20.259603: step 1800, loss = 0.26 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 16:22:33.099491: step 1900, loss = 0.33 (161.4 examples/sec; 0.396 sec/batch)\n",
      "2017-08-20 16:23:42.999961: step 2000, loss = 0.34 (166.9 examples/sec; 0.384 sec/batch)\n",
      "Epoch 6 of 10 took 262.083s\n",
      "  training loss:\t\t0.277801\n",
      "  training accuracy:\t\t96.82 %\n",
      "  validation loss:\t\t0.778846\n",
      "  validation accuracy:\t\t73.44 %\n",
      "Test results:\n",
      "  test loss:\t\t\t1.307046\n",
      "  test accuracy:\t\t46.74 %\n",
      "2017-08-20 16:25:20.530731: step 2100, loss = 0.24 (165.1 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 16:26:36.303485: step 2200, loss = 0.25 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 16:27:56.650919: step 2300, loss = 0.25 (164.9 examples/sec; 0.388 sec/batch)\n",
      "Epoch 7 of 10 took 277.709s\n",
      "  training loss:\t\t0.264731\n",
      "  training accuracy:\t\t97.52 %\n",
      "  validation loss:\t\t0.827791\n",
      "  validation accuracy:\t\t70.87 %\n",
      "2017-08-20 16:29:29.045946: step 2400, loss = 0.32 (165.9 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 16:30:42.446164: step 2500, loss = 0.27 (167.5 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 16:31:51.647604: step 2600, loss = 0.22 (169.4 examples/sec; 0.378 sec/batch)\n",
      "2017-08-20 16:32:50.767136: step 2700, loss = 0.33 (163.0 examples/sec; 0.393 sec/batch)\n",
      "Epoch 8 of 10 took 241.985s\n",
      "  training loss:\t\t0.263356\n",
      "  training accuracy:\t\t97.65 %\n",
      "  validation loss:\t\t0.815748\n",
      "  validation accuracy:\t\t71.70 %\n",
      "2017-08-20 16:34:02.173370: step 2800, loss = 0.22 (162.3 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 16:35:01.488799: step 2900, loss = 0.24 (167.4 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 16:36:10.980704: step 3000, loss = 0.35 (162.9 examples/sec; 0.393 sec/batch)\n",
      "Epoch 9 of 10 took 237.431s\n",
      "  training loss:\t\t0.262614\n",
      "  training accuracy:\t\t97.72 %\n",
      "  validation loss:\t\t0.805011\n",
      "  validation accuracy:\t\t72.47 %\n",
      "2017-08-20 16:37:37.074592: step 3100, loss = 0.28 (165.6 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 16:38:36.197938: step 3200, loss = 0.24 (165.9 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 16:39:37.503922: step 3300, loss = 0.25 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:40:31.952381: step 3400, loss = 0.25 (166.6 examples/sec; 0.384 sec/batch)\n",
      "Epoch 10 of 10 took 215.367s\n",
      "  training loss:\t\t0.259890\n",
      "  training accuracy:\t\t97.84 %\n",
      "  validation loss:\t\t0.822276\n",
      "  validation accuracy:\t\t71.22 %\n",
      "Beginning fold 3 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/2\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 16:41:47.421015: step 0, loss = 0.69 (47.4 examples/sec; 1.349 sec/batch)\n",
      "2017-08-20 16:42:49.730589: step 100, loss = 0.69 (165.0 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 16:43:53.510822: step 200, loss = 0.68 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 16:44:51.111341: step 300, loss = 0.69 (165.3 examples/sec; 0.387 sec/batch)\n",
      "Epoch 1 of 10 took 220.879s\n",
      "  training loss:\t\t0.683019\n",
      "  training accuracy:\t\t56.14 %\n",
      "  validation loss:\t\t0.663785\n",
      "  validation accuracy:\t\t62.85 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.684873\n",
      "  test accuracy:\t\t56.39 %\n",
      "2017-08-20 16:46:14.671120: step 400, loss = 0.65 (158.6 examples/sec; 0.403 sec/batch)\n",
      "2017-08-20 16:47:05.755738: step 500, loss = 0.61 (166.8 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 16:48:07.575899: step 600, loss = 0.68 (165.2 examples/sec; 0.387 sec/batch)\n",
      "Epoch 2 of 10 took 218.851s\n",
      "  training loss:\t\t0.642870\n",
      "  training accuracy:\t\t66.01 %\n",
      "  validation loss:\t\t0.617352\n",
      "  validation accuracy:\t\t72.74 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.707048\n",
      "  test accuracy:\t\t58.51 %\n",
      "2017-08-20 16:49:40.902904: step 700, loss = 0.65 (170.5 examples/sec; 0.375 sec/batch)\n",
      "2017-08-20 16:50:42.159080: step 800, loss = 0.47 (160.6 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 16:51:46.317702: step 900, loss = 0.56 (164.8 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 16:52:54.217384: step 1000, loss = 0.52 (167.9 examples/sec; 0.381 sec/batch)\n",
      "Epoch 3 of 10 took 237.220s\n",
      "  training loss:\t\t0.546159\n",
      "  training accuracy:\t\t77.81 %\n",
      "  validation loss:\t\t0.667362\n",
      "  validation accuracy:\t\t70.97 %\n",
      "2017-08-20 16:54:12.179887: step 1100, loss = 0.36 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 16:55:13.680293: step 1200, loss = 0.37 (166.2 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 16:56:30.871318: step 1300, loss = 0.34 (163.9 examples/sec; 0.390 sec/batch)\n",
      "Epoch 4 of 10 took 248.996s\n",
      "  training loss:\t\t0.407608\n",
      "  training accuracy:\t\t88.91 %\n",
      "  validation loss:\t\t0.720926\n",
      "  validation accuracy:\t\t68.68 %\n",
      "2017-08-20 16:58:03.456364: step 1400, loss = 0.33 (168.3 examples/sec; 0.380 sec/batch)\n",
      "2017-08-20 16:59:11.254392: step 1500, loss = 0.41 (167.4 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 17:00:13.004897: step 1600, loss = 0.53 (166.8 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 17:01:15.137089: step 1700, loss = 0.36 (162.1 examples/sec; 0.395 sec/batch)\n",
      "Epoch 5 of 10 took 237.009s\n",
      "  training loss:\t\t0.373269\n",
      "  training accuracy:\t\t91.26 %\n",
      "  validation loss:\t\t0.761827\n",
      "  validation accuracy:\t\t69.03 %\n",
      "2017-08-20 17:02:27.450414: step 1800, loss = 0.36 (165.5 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:03:32.063794: step 1900, loss = 0.31 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:04:43.672275: step 2000, loss = 0.28 (168.0 examples/sec; 0.381 sec/batch)\n",
      "Epoch 6 of 10 took 245.283s\n",
      "  training loss:\t\t0.348138\n",
      "  training accuracy:\t\t92.67 %\n",
      "  validation loss:\t\t0.811022\n",
      "  validation accuracy:\t\t67.29 %\n",
      "2017-08-20 17:06:18.413378: step 2100, loss = 0.34 (166.3 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 17:07:32.151503: step 2200, loss = 0.40 (166.9 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 17:08:35.084846: step 2300, loss = 0.36 (164.4 examples/sec; 0.389 sec/batch)\n",
      "Epoch 7 of 10 took 244.800s\n",
      "  training loss:\t\t0.330889\n",
      "  training accuracy:\t\t93.74 %\n",
      "  validation loss:\t\t0.810427\n",
      "  validation accuracy:\t\t67.81 %\n",
      "2017-08-20 17:09:46.256145: step 2400, loss = 0.27 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:10:58.983913: step 2500, loss = 0.35 (165.4 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:12:13.507411: step 2600, loss = 0.32 (169.1 examples/sec; 0.378 sec/batch)\n",
      "2017-08-20 17:13:27.524656: step 2700, loss = 0.33 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 8 of 10 took 263.783s\n",
      "  training loss:\t\t0.326307\n",
      "  training accuracy:\t\t93.91 %\n",
      "  validation loss:\t\t0.818416\n",
      "  validation accuracy:\t\t67.47 %\n",
      "2017-08-20 17:14:35.645034: step 2800, loss = 0.37 (166.3 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 17:15:16.302121: step 2900, loss = 0.33 (166.4 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 17:15:56.979683: step 3000, loss = 0.29 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 152.349s\n",
      "  training loss:\t\t0.324340\n",
      "  training accuracy:\t\t94.18 %\n",
      "  validation loss:\t\t0.809690\n",
      "  validation accuracy:\t\t67.60 %\n",
      "2017-08-20 17:16:48.512906: step 3100, loss = 0.34 (160.8 examples/sec; 0.398 sec/batch)\n",
      "2017-08-20 17:17:29.528825: step 3200, loss = 0.27 (162.7 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:18:10.601695: step 3300, loss = 0.36 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:18:52.174653: step 3400, loss = 0.28 (165.9 examples/sec; 0.386 sec/batch)\n",
      "Epoch 10 of 10 took 148.300s\n",
      "  training loss:\t\t0.323817\n",
      "  training accuracy:\t\t94.09 %\n",
      "  validation loss:\t\t0.816704\n",
      "  validation accuracy:\t\t68.02 %\n",
      "Beginning fold 4 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/3\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 17:19:27.668137: step 0, loss = 0.69 (42.2 examples/sec; 1.516 sec/batch)\n",
      "2017-08-20 17:20:09.796803: step 100, loss = 0.69 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:20:50.490162: step 200, loss = 0.68 (161.1 examples/sec; 0.397 sec/batch)\n",
      "2017-08-20 17:21:31.107878: step 300, loss = 0.67 (160.1 examples/sec; 0.400 sec/batch)\n",
      "Epoch 1 of 10 took 149.143s\n",
      "  training loss:\t\t0.681101\n",
      "  training accuracy:\t\t56.82 %\n",
      "  validation loss:\t\t0.652806\n",
      "  validation accuracy:\t\t66.08 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.664627\n",
      "  test accuracy:\t\t63.99 %\n",
      "2017-08-20 17:22:26.059165: step 400, loss = 0.67 (168.3 examples/sec; 0.380 sec/batch)\n",
      "2017-08-20 17:23:06.653210: step 500, loss = 0.66 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:23:47.684319: step 600, loss = 0.64 (163.5 examples/sec; 0.391 sec/batch)\n",
      "Epoch 2 of 10 took 146.212s\n",
      "  training loss:\t\t0.634839\n",
      "  training accuracy:\t\t68.70 %\n",
      "  validation loss:\t\t0.689398\n",
      "  validation accuracy:\t\t63.68 %\n",
      "2017-08-20 17:24:35.385268: step 700, loss = 0.70 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 17:25:16.137158: step 800, loss = 0.57 (171.0 examples/sec; 0.374 sec/batch)\n",
      "2017-08-20 17:25:56.680223: step 900, loss = 0.46 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:26:37.707765: step 1000, loss = 0.55 (163.2 examples/sec; 0.392 sec/batch)\n",
      "Epoch 3 of 10 took 148.337s\n",
      "  training loss:\t\t0.545779\n",
      "  training accuracy:\t\t78.29 %\n",
      "  validation loss:\t\t0.726581\n",
      "  validation accuracy:\t\t60.52 %\n",
      "2017-08-20 17:27:27.117613: step 1100, loss = 0.43 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:28:07.682033: step 1200, loss = 0.31 (166.6 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 17:28:48.412007: step 1300, loss = 0.38 (164.4 examples/sec; 0.389 sec/batch)\n",
      "Epoch 4 of 10 took 146.120s\n",
      "  training loss:\t\t0.407895\n",
      "  training accuracy:\t\t89.46 %\n",
      "  validation loss:\t\t0.918183\n",
      "  validation accuracy:\t\t56.04 %\n",
      "2017-08-20 17:29:36.339058: step 1400, loss = 0.34 (165.7 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 17:30:16.909535: step 1500, loss = 0.36 (168.2 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 17:30:57.807206: step 1600, loss = 0.34 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:31:38.534727: step 1700, loss = 0.35 (167.2 examples/sec; 0.383 sec/batch)\n",
      "Epoch 5 of 10 took 146.442s\n",
      "  training loss:\t\t0.363690\n",
      "  training accuracy:\t\t91.97 %\n",
      "  validation loss:\t\t0.897212\n",
      "  validation accuracy:\t\t57.71 %\n",
      "2017-08-20 17:32:26.197383: step 1800, loss = 0.31 (163.5 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:33:06.922696: step 1900, loss = 0.35 (166.6 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 17:33:47.905120: step 2000, loss = 0.31 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 6 of 10 took 149.331s\n",
      "  training loss:\t\t0.343604\n",
      "  training accuracy:\t\t93.39 %\n",
      "  validation loss:\t\t0.940299\n",
      "  validation accuracy:\t\t57.29 %\n",
      "2017-08-20 17:34:38.525996: step 2100, loss = 0.30 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 17:35:19.778404: step 2200, loss = 0.28 (165.4 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:36:00.644614: step 2300, loss = 0.35 (166.6 examples/sec; 0.384 sec/batch)\n",
      "Epoch 7 of 10 took 146.681s\n",
      "  training loss:\t\t0.327684\n",
      "  training accuracy:\t\t94.33 %\n",
      "  validation loss:\t\t0.925561\n",
      "  validation accuracy:\t\t57.60 %\n",
      "2017-08-20 17:36:48.261593: step 2400, loss = 0.37 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:37:28.999743: step 2500, loss = 0.34 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:38:09.698667: step 2600, loss = 0.26 (165.0 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 17:38:50.363577: step 2700, loss = 0.36 (172.0 examples/sec; 0.372 sec/batch)\n",
      "Epoch 8 of 10 took 146.377s\n",
      "  training loss:\t\t0.326041\n",
      "  training accuracy:\t\t94.51 %\n",
      "  validation loss:\t\t0.949529\n",
      "  validation accuracy:\t\t57.85 %\n",
      "2017-08-20 17:39:38.175470: step 2800, loss = 0.28 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:40:18.861790: step 2900, loss = 0.33 (161.9 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 17:40:59.737794: step 3000, loss = 0.29 (166.1 examples/sec; 0.385 sec/batch)\n",
      "Epoch 9 of 10 took 148.633s\n",
      "  training loss:\t\t0.323931\n",
      "  training accuracy:\t\t94.64 %\n",
      "  validation loss:\t\t0.935318\n",
      "  validation accuracy:\t\t58.51 %\n",
      "2017-08-20 17:41:49.791429: step 3100, loss = 0.29 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:42:30.303083: step 3200, loss = 0.27 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:43:10.972266: step 3300, loss = 0.34 (163.4 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 17:43:51.727848: step 3400, loss = 0.33 (170.2 examples/sec; 0.376 sec/batch)\n",
      "Epoch 10 of 10 took 146.214s\n",
      "  training loss:\t\t0.321816\n",
      "  training accuracy:\t\t94.56 %\n",
      "  validation loss:\t\t0.956669\n",
      "  validation accuracy:\t\t57.92 %\n",
      "Beginning fold 5 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/4\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 17:44:27.386842: step 0, loss = 0.70 (42.7 examples/sec; 1.500 sec/batch)\n",
      "2017-08-20 17:45:09.966906: step 100, loss = 0.69 (160.5 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 17:45:50.436969: step 200, loss = 0.70 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:46:31.050857: step 300, loss = 0.66 (143.4 examples/sec; 0.446 sec/batch)\n",
      "Epoch 1 of 10 took 149.377s\n",
      "  training loss:\t\t0.681809\n",
      "  training accuracy:\t\t56.49 %\n",
      "  validation loss:\t\t0.553734\n",
      "  validation accuracy:\t\t85.49 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.649153\n",
      "  test accuracy:\t\t65.94 %\n",
      "2017-08-20 17:47:26.102467: step 400, loss = 0.59 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:48:06.782130: step 500, loss = 0.63 (166.3 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 17:48:47.775142: step 600, loss = 0.58 (164.4 examples/sec; 0.389 sec/batch)\n",
      "Epoch 2 of 10 took 146.403s\n",
      "  training loss:\t\t0.607821\n",
      "  training accuracy:\t\t72.23 %\n",
      "  validation loss:\t\t0.651332\n",
      "  validation accuracy:\t\t67.60 %\n",
      "2017-08-20 17:49:35.540110: step 700, loss = 0.50 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:50:16.068874: step 800, loss = 0.39 (162.8 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:50:56.764020: step 900, loss = 0.44 (165.9 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 17:51:37.933215: step 1000, loss = 0.26 (165.3 examples/sec; 0.387 sec/batch)\n",
      "Epoch 3 of 10 took 148.243s\n",
      "  training loss:\t\t0.404919\n",
      "  training accuracy:\t\t88.56 %\n",
      "  validation loss:\t\t0.728421\n",
      "  validation accuracy:\t\t66.28 %\n",
      "2017-08-20 17:52:27.202052: step 1100, loss = 0.27 (167.5 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 17:53:07.860038: step 1200, loss = 0.23 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:53:48.690458: step 1300, loss = 0.25 (164.1 examples/sec; 0.390 sec/batch)\n",
      "Epoch 4 of 10 took 146.171s\n",
      "  training loss:\t\t0.270499\n",
      "  training accuracy:\t\t97.25 %\n",
      "  validation loss:\t\t1.155318\n",
      "  validation accuracy:\t\t52.22 %\n",
      "2017-08-20 17:54:36.366697: step 1400, loss = 0.33 (169.5 examples/sec; 0.378 sec/batch)\n",
      "2017-08-20 17:55:17.029831: step 1500, loss = 0.28 (159.5 examples/sec; 0.401 sec/batch)\n",
      "2017-08-20 17:55:59.139898: step 1600, loss = 0.24 (159.1 examples/sec; 0.402 sec/batch)\n",
      "2017-08-20 17:56:39.864693: step 1700, loss = 0.26 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 5 of 10 took 147.605s\n",
      "  training loss:\t\t0.261668\n",
      "  training accuracy:\t\t97.70 %\n",
      "  validation loss:\t\t1.117831\n",
      "  validation accuracy:\t\t56.63 %\n",
      "2017-08-20 17:57:27.548832: step 1800, loss = 0.25 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:58:08.265375: step 1900, loss = 0.29 (166.5 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 17:58:49.147863: step 2000, loss = 0.25 (166.4 examples/sec; 0.385 sec/batch)\n",
      "Epoch 6 of 10 took 148.050s\n",
      "  training loss:\t\t0.256211\n",
      "  training accuracy:\t\t97.99 %\n",
      "  validation loss:\t\t1.152450\n",
      "  validation accuracy:\t\t52.60 %\n",
      "2017-08-20 17:59:38.619655: step 2100, loss = 0.23 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:00:19.574621: step 2200, loss = 0.23 (166.0 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:01:00.398444: step 2300, loss = 0.23 (166.8 examples/sec; 0.384 sec/batch)\n",
      "Epoch 7 of 10 took 147.663s\n",
      "  training loss:\t\t0.248235\n",
      "  training accuracy:\t\t98.51 %\n",
      "  validation loss:\t\t1.158719\n",
      "  validation accuracy:\t\t53.82 %\n",
      "2017-08-20 18:01:49.306157: step 2400, loss = 0.24 (168.1 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 18:02:29.868392: step 2500, loss = 0.25 (163.5 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:03:10.576836: step 2600, loss = 0.22 (167.0 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 18:03:51.313724: step 2700, loss = 0.24 (162.4 examples/sec; 0.394 sec/batch)\n",
      "Epoch 8 of 10 took 146.243s\n",
      "  training loss:\t\t0.247182\n",
      "  training accuracy:\t\t98.63 %\n",
      "  validation loss:\t\t1.190734\n",
      "  validation accuracy:\t\t53.12 %\n",
      "2017-08-20 18:04:38.936728: step 2800, loss = 0.25 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:05:19.831158: step 2900, loss = 0.24 (150.8 examples/sec; 0.424 sec/batch)\n",
      "2017-08-20 18:06:00.678719: step 3000, loss = 0.26 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 148.303s\n",
      "  training loss:\t\t0.246560\n",
      "  training accuracy:\t\t98.64 %\n",
      "  validation loss:\t\t1.210113\n",
      "  validation accuracy:\t\t52.85 %\n",
      "2017-08-20 18:06:50.300415: step 3100, loss = 0.25 (166.8 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:07:30.879207: step 3200, loss = 0.25 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:08:11.616750: step 3300, loss = 0.27 (164.2 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:08:52.439696: step 3400, loss = 0.25 (166.2 examples/sec; 0.385 sec/batch)\n",
      "Epoch 10 of 10 took 146.292s\n",
      "  training loss:\t\t0.245981\n",
      "  training accuracy:\t\t98.62 %\n",
      "  validation loss:\t\t1.200293\n",
      "  validation accuracy:\t\t53.47 %\n",
      "Beginning fold 6 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/5\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 18:09:27.607548: step 0, loss = 0.70 (43.3 examples/sec; 1.477 sec/batch)\n",
      "2017-08-20 18:10:11.434951: step 100, loss = 0.69 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:10:51.629588: step 200, loss = 0.68 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:11:32.018738: step 300, loss = 0.64 (158.7 examples/sec; 0.403 sec/batch)\n",
      "Epoch 1 of 10 took 149.749s\n",
      "  training loss:\t\t0.676409\n",
      "  training accuracy:\t\t58.32 %\n",
      "  validation loss:\t\t0.717621\n",
      "  validation accuracy:\t\t51.94 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.620830\n",
      "  test accuracy:\t\t71.08 %\n",
      "2017-08-20 18:12:26.391240: step 400, loss = 0.64 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:13:06.862371: step 500, loss = 0.62 (161.3 examples/sec; 0.397 sec/batch)\n",
      "2017-08-20 18:13:47.158189: step 600, loss = 0.53 (163.7 examples/sec; 0.391 sec/batch)\n",
      "Epoch 2 of 10 took 145.085s\n",
      "  training loss:\t\t0.600188\n",
      "  training accuracy:\t\t73.06 %\n",
      "  validation loss:\t\t0.802491\n",
      "  validation accuracy:\t\t57.57 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.657806\n",
      "  test accuracy:\t\t68.89 %\n",
      "2017-08-20 18:14:41.544905: step 700, loss = 0.60 (165.0 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 18:15:21.881428: step 800, loss = 0.39 (165.6 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:16:02.392064: step 900, loss = 0.46 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:16:42.813319: step 1000, loss = 0.34 (162.7 examples/sec; 0.393 sec/batch)\n",
      "Epoch 3 of 10 took 146.846s\n",
      "  training loss:\t\t0.502072\n",
      "  training accuracy:\t\t81.09 %\n",
      "  validation loss:\t\t0.845699\n",
      "  validation accuracy:\t\t61.01 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.665054\n",
      "  test accuracy:\t\t71.25 %\n",
      "2017-08-20 18:17:38.775632: step 1100, loss = 0.38 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:18:19.268429: step 1200, loss = 0.32 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:18:59.725664: step 1300, loss = 0.37 (167.9 examples/sec; 0.381 sec/batch)\n",
      "Epoch 4 of 10 took 145.320s\n",
      "  training loss:\t\t0.341860\n",
      "  training accuracy:\t\t93.20 %\n",
      "  validation loss:\t\t0.834935\n",
      "  validation accuracy:\t\t63.51 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.719937\n",
      "  test accuracy:\t\t69.97 %\n",
      "2017-08-20 18:19:54.001032: step 1400, loss = 0.39 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:20:34.314432: step 1500, loss = 0.36 (165.8 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:21:14.668358: step 1600, loss = 0.26 (165.8 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:21:55.187830: step 1700, loss = 0.26 (166.9 examples/sec; 0.383 sec/batch)\n",
      "Epoch 5 of 10 took 145.072s\n",
      "  training loss:\t\t0.318690\n",
      "  training accuracy:\t\t94.64 %\n",
      "  validation loss:\t\t0.841099\n",
      "  validation accuracy:\t\t64.76 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.742549\n",
      "  test accuracy:\t\t69.34 %\n",
      "2017-08-20 18:22:49.212288: step 1800, loss = 0.32 (170.5 examples/sec; 0.375 sec/batch)\n",
      "2017-08-20 18:23:29.624425: step 1900, loss = 0.30 (150.8 examples/sec; 0.424 sec/batch)\n",
      "2017-08-20 18:24:10.193066: step 2000, loss = 0.26 (163.4 examples/sec; 0.392 sec/batch)\n",
      "Epoch 6 of 10 took 146.605s\n",
      "  training loss:\t\t0.298030\n",
      "  training accuracy:\t\t95.87 %\n",
      "  validation loss:\t\t0.867687\n",
      "  validation accuracy:\t\t65.45 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.808780\n",
      "  test accuracy:\t\t65.66 %\n",
      "2017-08-20 18:25:05.866559: step 2100, loss = 0.26 (167.7 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 18:25:46.222981: step 2200, loss = 0.27 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:26:26.682671: step 2300, loss = 0.29 (171.0 examples/sec; 0.374 sec/batch)\n",
      "Epoch 7 of 10 took 145.191s\n",
      "  training loss:\t\t0.287997\n",
      "  training accuracy:\t\t96.54 %\n",
      "  validation loss:\t\t0.844884\n",
      "  validation accuracy:\t\t65.97 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.779448\n",
      "  test accuracy:\t\t68.85 %\n",
      "2017-08-20 18:27:21.122838: step 2400, loss = 0.36 (163.7 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:28:01.405540: step 2500, loss = 0.28 (165.9 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:28:41.776801: step 2600, loss = 0.24 (166.5 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:29:22.102316: step 2700, loss = 0.25 (170.5 examples/sec; 0.375 sec/batch)\n",
      "Epoch 8 of 10 took 144.894s\n",
      "  training loss:\t\t0.285933\n",
      "  training accuracy:\t\t96.68 %\n",
      "  validation loss:\t\t0.856136\n",
      "  validation accuracy:\t\t65.56 %\n",
      "2017-08-20 18:30:09.269970: step 2800, loss = 0.33 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:30:49.489132: step 2900, loss = 0.24 (164.7 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 18:31:29.891913: step 3000, loss = 0.28 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 146.258s\n",
      "  training loss:\t\t0.283224\n",
      "  training accuracy:\t\t96.77 %\n",
      "  validation loss:\t\t0.863087\n",
      "  validation accuracy:\t\t65.97 %\n",
      "2017-08-20 18:32:18.755079: step 3100, loss = 0.31 (166.4 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 18:32:59.134181: step 3200, loss = 0.26 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:33:39.428450: step 3300, loss = 0.23 (163.3 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 18:34:19.726186: step 3400, loss = 0.27 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 10 of 10 took 144.939s\n",
      "  training loss:\t\t0.281375\n",
      "  training accuracy:\t\t96.78 %\n",
      "  validation loss:\t\t0.855611\n",
      "  validation accuracy:\t\t65.76 %\n",
      "Beginning fold 7 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/6\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 18:34:55.410207: step 0, loss = 0.70 (42.2 examples/sec; 1.517 sec/batch)\n",
      "2017-08-20 18:35:37.350411: step 100, loss = 0.70 (166.6 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:36:17.509742: step 200, loss = 0.68 (165.3 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:36:57.716895: step 300, loss = 0.63 (166.0 examples/sec; 0.385 sec/batch)\n",
      "Epoch 1 of 10 took 147.693s\n",
      "  training loss:\t\t0.681593\n",
      "  training accuracy:\t\t56.56 %\n",
      "  validation loss:\t\t0.670868\n",
      "  validation accuracy:\t\t60.24 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.657891\n",
      "  test accuracy:\t\t67.57 %\n",
      "2017-08-20 18:37:52.068502: step 400, loss = 0.62 (162.3 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 18:38:32.389477: step 500, loss = 0.63 (163.2 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 18:39:12.874195: step 600, loss = 0.62 (163.4 examples/sec; 0.392 sec/batch)\n",
      "Epoch 2 of 10 took 144.807s\n",
      "  training loss:\t\t0.619339\n",
      "  training accuracy:\t\t70.63 %\n",
      "  validation loss:\t\t0.620227\n",
      "  validation accuracy:\t\t70.90 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.655417\n",
      "  test accuracy:\t\t67.78 %\n",
      "2017-08-20 18:40:06.833699: step 700, loss = 0.63 (167.2 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 18:40:47.051690: step 800, loss = 0.58 (166.5 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:41:27.350033: step 900, loss = 0.46 (164.9 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 18:42:07.738184: step 1000, loss = 0.41 (161.1 examples/sec; 0.397 sec/batch)\n",
      "Epoch 3 of 10 took 146.281s\n",
      "  training loss:\t\t0.535220\n",
      "  training accuracy:\t\t79.77 %\n",
      "  validation loss:\t\t0.552614\n",
      "  validation accuracy:\t\t75.21 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.703803\n",
      "  test accuracy:\t\t67.26 %\n",
      "2017-08-20 18:43:03.357256: step 1100, loss = 0.46 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:43:43.670453: step 1200, loss = 0.43 (163.4 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 18:44:23.982181: step 1300, loss = 0.30 (165.8 examples/sec; 0.386 sec/batch)\n",
      "Epoch 4 of 10 took 144.762s\n",
      "  training loss:\t\t0.393347\n",
      "  training accuracy:\t\t90.83 %\n",
      "  validation loss:\t\t0.562376\n",
      "  validation accuracy:\t\t77.71 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.805605\n",
      "  test accuracy:\t\t65.62 %\n",
      "2017-08-20 18:45:18.100551: step 1400, loss = 0.29 (166.2 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 18:45:58.320818: step 1500, loss = 0.45 (168.0 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 18:46:38.598722: step 1600, loss = 0.39 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:47:18.876807: step 1700, loss = 0.30 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 5 of 10 took 144.588s\n",
      "  training loss:\t\t0.364152\n",
      "  training accuracy:\t\t92.39 %\n",
      "  validation loss:\t\t0.588287\n",
      "  validation accuracy:\t\t75.21 %\n",
      "2017-08-20 18:48:06.060328: step 1800, loss = 0.38 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:48:46.422452: step 1900, loss = 0.30 (166.7 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:49:26.785135: step 2000, loss = 0.40 (163.0 examples/sec; 0.393 sec/batch)\n",
      "Epoch 6 of 10 took 146.443s\n",
      "  training loss:\t\t0.341187\n",
      "  training accuracy:\t\t93.31 %\n",
      "  validation loss:\t\t0.578726\n",
      "  validation accuracy:\t\t76.98 %\n",
      "2017-08-20 18:50:15.613728: step 2100, loss = 0.31 (166.7 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:50:55.951481: step 2200, loss = 0.30 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:51:36.237475: step 2300, loss = 0.43 (163.8 examples/sec; 0.391 sec/batch)\n",
      "Epoch 7 of 10 took 144.633s\n",
      "  training loss:\t\t0.324865\n",
      "  training accuracy:\t\t94.24 %\n",
      "  validation loss:\t\t0.593918\n",
      "  validation accuracy:\t\t75.94 %\n",
      "2017-08-20 18:52:23.413723: step 2400, loss = 0.29 (165.5 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:53:03.734469: step 2500, loss = 0.26 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:53:44.046124: step 2600, loss = 0.31 (163.7 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:54:24.412116: step 2700, loss = 0.36 (162.9 examples/sec; 0.393 sec/batch)\n",
      "Epoch 8 of 10 took 144.996s\n",
      "  training loss:\t\t0.321988\n",
      "  training accuracy:\t\t94.44 %\n",
      "  validation loss:\t\t0.589565\n",
      "  validation accuracy:\t\t76.70 %\n",
      "2017-08-20 18:55:11.725865: step 2800, loss = 0.31 (166.5 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:55:52.149492: step 2900, loss = 0.31 (164.3 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:56:32.480303: step 3000, loss = 0.31 (163.3 examples/sec; 0.392 sec/batch)\n",
      "Epoch 9 of 10 took 146.724s\n",
      "  training loss:\t\t0.318768\n",
      "  training accuracy:\t\t94.38 %\n",
      "  validation loss:\t\t0.595500\n",
      "  validation accuracy:\t\t76.39 %\n",
      "2017-08-20 18:57:21.576367: step 3100, loss = 0.29 (166.9 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 18:58:02.041387: step 3200, loss = 0.37 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:58:42.417989: step 3300, loss = 0.25 (162.0 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 18:59:22.761303: step 3400, loss = 0.30 (162.9 examples/sec; 0.393 sec/batch)\n",
      "Epoch 10 of 10 took 145.180s\n",
      "  training loss:\t\t0.317111\n",
      "  training accuracy:\t\t94.55 %\n",
      "  validation loss:\t\t0.601745\n",
      "  validation accuracy:\t\t76.49 %\n",
      "Beginning fold 8 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/7\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 18:59:58.039756: step 0, loss = 0.69 (45.2 examples/sec; 1.416 sec/batch)\n",
      "2017-08-20 19:00:40.273533: step 100, loss = 0.68 (163.9 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:01:20.471092: step 200, loss = 0.66 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:02:00.699907: step 300, loss = 0.61 (164.6 examples/sec; 0.389 sec/batch)\n",
      "Epoch 1 of 10 took 147.806s\n",
      "  training loss:\t\t0.672245\n",
      "  training accuracy:\t\t59.10 %\n",
      "  validation loss:\t\t0.698312\n",
      "  validation accuracy:\t\t54.55 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.742998\n",
      "  test accuracy:\t\t56.11 %\n",
      "2017-08-20 19:02:54.958442: step 400, loss = 0.64 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:03:35.065499: step 500, loss = 0.62 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:04:15.378343: step 600, loss = 0.54 (166.3 examples/sec; 0.385 sec/batch)\n",
      "Epoch 2 of 10 took 144.515s\n",
      "  training loss:\t\t0.589576\n",
      "  training accuracy:\t\t73.89 %\n",
      "  validation loss:\t\t0.766431\n",
      "  validation accuracy:\t\t52.81 %\n",
      "2017-08-20 19:05:02.599355: step 700, loss = 0.55 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:05:42.775230: step 800, loss = 0.55 (158.4 examples/sec; 0.404 sec/batch)\n",
      "2017-08-20 19:06:22.966347: step 900, loss = 0.43 (167.5 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 19:07:03.204890: step 1000, loss = 0.40 (165.4 examples/sec; 0.387 sec/batch)\n",
      "Epoch 3 of 10 took 146.149s\n",
      "  training loss:\t\t0.508507\n",
      "  training accuracy:\t\t81.34 %\n",
      "  validation loss:\t\t0.573182\n",
      "  validation accuracy:\t\t75.42 %\n",
      "Test results:\n",
      "  test loss:\t\t\t1.117281\n",
      "  test accuracy:\t\t37.08 %\n",
      "2017-08-20 19:07:58.942948: step 1100, loss = 0.30 (165.6 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:08:39.167201: step 1200, loss = 0.33 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:09:19.497354: step 1300, loss = 0.39 (163.9 examples/sec; 0.391 sec/batch)\n",
      "Epoch 4 of 10 took 144.645s\n",
      "  training loss:\t\t0.351309\n",
      "  training accuracy:\t\t92.82 %\n",
      "  validation loss:\t\t0.765343\n",
      "  validation accuracy:\t\t62.92 %\n",
      "2017-08-20 19:10:06.757515: step 1400, loss = 0.33 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:10:47.103454: step 1500, loss = 0.35 (161.8 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 19:11:27.342839: step 1600, loss = 0.25 (163.3 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:12:07.596652: step 1700, loss = 0.28 (165.0 examples/sec; 0.388 sec/batch)\n",
      "Epoch 5 of 10 took 144.792s\n",
      "  training loss:\t\t0.321142\n",
      "  training accuracy:\t\t94.36 %\n",
      "  validation loss:\t\t0.790306\n",
      "  validation accuracy:\t\t63.16 %\n",
      "2017-08-20 19:12:54.748148: step 1800, loss = 0.27 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:13:35.066907: step 1900, loss = 0.33 (165.8 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 19:14:15.237786: step 2000, loss = 0.35 (163.8 examples/sec; 0.391 sec/batch)\n",
      "Epoch 6 of 10 took 146.096s\n",
      "  training loss:\t\t0.298942\n",
      "  training accuracy:\t\t95.56 %\n",
      "  validation loss:\t\t0.813672\n",
      "  validation accuracy:\t\t63.37 %\n",
      "2017-08-20 19:15:04.088031: step 2100, loss = 0.24 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:15:44.355034: step 2200, loss = 0.31 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:16:24.755741: step 2300, loss = 0.25 (167.0 examples/sec; 0.383 sec/batch)\n",
      "Epoch 7 of 10 took 144.732s\n",
      "  training loss:\t\t0.286413\n",
      "  training accuracy:\t\t96.39 %\n",
      "  validation loss:\t\t0.849662\n",
      "  validation accuracy:\t\t62.71 %\n",
      "2017-08-20 19:17:11.945267: step 2400, loss = 0.25 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:17:52.152055: step 2500, loss = 0.40 (166.1 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 19:18:32.322174: step 2600, loss = 0.23 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:19:12.569308: step 2700, loss = 0.28 (163.8 examples/sec; 0.391 sec/batch)\n",
      "Epoch 8 of 10 took 144.613s\n",
      "  training loss:\t\t0.284295\n",
      "  training accuracy:\t\t96.65 %\n",
      "  validation loss:\t\t0.837669\n",
      "  validation accuracy:\t\t62.92 %\n",
      "2017-08-20 19:19:59.858388: step 2800, loss = 0.33 (163.2 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:20:40.005090: step 2900, loss = 0.29 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 19:21:20.287271: step 3000, loss = 0.30 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 146.004s\n",
      "  training loss:\t\t0.280926\n",
      "  training accuracy:\t\t96.72 %\n",
      "  validation loss:\t\t0.847253\n",
      "  validation accuracy:\t\t62.57 %\n",
      "2017-08-20 19:22:08.985791: step 3100, loss = 0.30 (163.4 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:22:49.257659: step 3200, loss = 0.26 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:23:29.516749: step 3300, loss = 0.28 (162.0 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 19:24:09.685117: step 3400, loss = 0.31 (166.4 examples/sec; 0.385 sec/batch)\n",
      "Epoch 10 of 10 took 144.608s\n",
      "  training loss:\t\t0.281357\n",
      "  training accuracy:\t\t96.77 %\n",
      "  validation loss:\t\t0.845162\n",
      "  validation accuracy:\t\t63.02 %\n",
      "Beginning fold 9 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/8\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 19:24:45.231028: step 0, loss = 0.70 (46.0 examples/sec; 1.392 sec/batch)\n",
      "2017-08-20 19:25:27.371559: step 100, loss = 0.70 (167.0 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 19:26:07.810079: step 200, loss = 0.67 (165.4 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:26:48.405465: step 300, loss = 0.66 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 1 of 10 took 148.598s\n",
      "  training loss:\t\t0.683269\n",
      "  training accuracy:\t\t56.60 %\n",
      "  validation loss:\t\t0.654307\n",
      "  validation accuracy:\t\t67.43 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.675717\n",
      "  test accuracy:\t\t60.14 %\n",
      "2017-08-20 19:27:43.211447: step 400, loss = 0.63 (167.1 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 19:28:23.770040: step 500, loss = 0.67 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 19:29:04.349996: step 600, loss = 0.60 (165.6 examples/sec; 0.386 sec/batch)\n",
      "Epoch 2 of 10 took 145.646s\n",
      "  training loss:\t\t0.636694\n",
      "  training accuracy:\t\t68.32 %\n",
      "  validation loss:\t\t0.588214\n",
      "  validation accuracy:\t\t72.88 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.695989\n",
      "  test accuracy:\t\t59.86 %\n",
      "2017-08-20 19:29:58.972364: step 700, loss = 0.66 (166.7 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 19:30:39.589489: step 800, loss = 0.67 (160.6 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 19:31:20.112946: step 900, loss = 0.52 (169.0 examples/sec; 0.379 sec/batch)\n",
      "2017-08-20 19:32:00.659562: step 1000, loss = 0.51 (165.3 examples/sec; 0.387 sec/batch)\n",
      "Epoch 3 of 10 took 147.435s\n",
      "  training loss:\t\t0.566703\n",
      "  training accuracy:\t\t76.11 %\n",
      "  validation loss:\t\t0.651794\n",
      "  validation accuracy:\t\t66.70 %\n",
      "2017-08-20 19:32:49.921594: step 1100, loss = 0.49 (164.2 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:33:30.451617: step 1200, loss = 0.36 (168.6 examples/sec; 0.379 sec/batch)\n",
      "2017-08-20 19:34:11.517457: step 1300, loss = 0.37 (170.4 examples/sec; 0.376 sec/batch)\n",
      "Epoch 4 of 10 took 146.184s\n",
      "  training loss:\t\t0.418477\n",
      "  training accuracy:\t\t88.48 %\n",
      "  validation loss:\t\t0.681828\n",
      "  validation accuracy:\t\t69.38 %\n",
      "2017-08-20 19:34:59.112073: step 1400, loss = 0.34 (158.0 examples/sec; 0.405 sec/batch)\n",
      "2017-08-20 19:35:39.832094: step 1500, loss = 0.52 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:36:20.534070: step 1600, loss = 0.33 (168.1 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 19:37:01.171184: step 1700, loss = 0.33 (166.7 examples/sec; 0.384 sec/batch)\n",
      "Epoch 5 of 10 took 146.292s\n",
      "  training loss:\t\t0.376453\n",
      "  training accuracy:\t\t91.15 %\n",
      "  validation loss:\t\t0.754591\n",
      "  validation accuracy:\t\t66.81 %\n",
      "2017-08-20 19:37:48.870713: step 1800, loss = 0.28 (165.7 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 19:38:29.445598: step 1900, loss = 0.32 (165.3 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:39:10.054964: step 2000, loss = 0.50 (166.1 examples/sec; 0.385 sec/batch)\n",
      "Epoch 6 of 10 took 147.475s\n",
      "  training loss:\t\t0.346368\n",
      "  training accuracy:\t\t92.78 %\n",
      "  validation loss:\t\t0.725402\n",
      "  validation accuracy:\t\t70.03 %\n",
      "2017-08-20 19:39:59.315997: step 2100, loss = 0.29 (162.6 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 19:40:41.603075: step 2200, loss = 0.45 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:41:22.251304: step 2300, loss = 0.33 (164.8 examples/sec; 0.388 sec/batch)\n",
      "Epoch 7 of 10 took 147.565s\n",
      "  training loss:\t\t0.329086\n",
      "  training accuracy:\t\t93.90 %\n",
      "  validation loss:\t\t0.740043\n",
      "  validation accuracy:\t\t69.38 %\n",
      "2017-08-20 19:42:09.898626: step 2400, loss = 0.31 (164.7 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:42:50.516435: step 2500, loss = 0.27 (172.9 examples/sec; 0.370 sec/batch)\n",
      "2017-08-20 19:43:31.289439: step 2600, loss = 0.27 (148.3 examples/sec; 0.432 sec/batch)\n",
      "2017-08-20 19:44:12.056785: step 2700, loss = 0.28 (166.1 examples/sec; 0.385 sec/batch)\n",
      "Epoch 8 of 10 took 146.331s\n",
      "  training loss:\t\t0.324326\n",
      "  training accuracy:\t\t94.08 %\n",
      "  validation loss:\t\t0.763067\n",
      "  validation accuracy:\t\t68.75 %\n",
      "2017-08-20 19:44:59.747902: step 2800, loss = 0.28 (165.4 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:45:40.519796: step 2900, loss = 0.39 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:46:21.256271: step 3000, loss = 0.28 (165.5 examples/sec; 0.387 sec/batch)\n",
      "Epoch 9 of 10 took 147.939s\n",
      "  training loss:\t\t0.322283\n",
      "  training accuracy:\t\t94.20 %\n",
      "  validation loss:\t\t0.758514\n",
      "  validation accuracy:\t\t69.34 %\n",
      "2017-08-20 19:47:10.615889: step 3100, loss = 0.30 (169.1 examples/sec; 0.379 sec/batch)\n",
      "2017-08-20 19:47:51.231863: step 3200, loss = 0.31 (171.2 examples/sec; 0.374 sec/batch)\n",
      "2017-08-20 19:48:31.906695: step 3300, loss = 0.33 (164.4 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:49:12.753608: step 3400, loss = 0.28 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 10 of 10 took 146.347s\n",
      "  training loss:\t\t0.321392\n",
      "  training accuracy:\t\t94.40 %\n",
      "  validation loss:\t\t0.754682\n",
      "  validation accuracy:\t\t69.41 %\n",
      "Beginning fold 10 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/9\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 19:49:46.174500: step 0, loss = 0.69 (45.0 examples/sec; 1.424 sec/batch)\n",
      "2017-08-20 19:50:28.419918: step 100, loss = 0.69 (170.0 examples/sec; 0.376 sec/batch)\n",
      "2017-08-20 19:51:08.922947: step 200, loss = 0.69 (165.1 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 19:51:49.698361: step 300, loss = 0.68 (166.6 examples/sec; 0.384 sec/batch)\n",
      "Epoch 1 of 10 took 160.612s\n",
      "  training loss:\t\t0.688358\n",
      "  training accuracy:\t\t54.24 %\n",
      "  validation loss:\t\t0.619095\n",
      "  validation accuracy:\t\t76.91 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.636278\n",
      "  test accuracy:\t\t71.70 %\n",
      "2017-08-20 19:52:38.847259: step 400, loss = 0.64 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:53:19.770843: step 500, loss = 0.65 (167.1 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 19:54:00.518642: step 600, loss = 0.60 (162.7 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 19:54:41.388224: step 700, loss = 0.61 (164.0 examples/sec; 0.390 sec/batch)\n",
      "Epoch 2 of 10 took 158.554s\n",
      "  training loss:\t\t0.640583\n",
      "  training accuracy:\t\t66.77 %\n",
      "  validation loss:\t\t0.459790\n",
      "  validation accuracy:\t\t89.81 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.557880\n",
      "  test accuracy:\t\t78.82 %\n",
      "2017-08-20 19:55:30.508265: step 800, loss = 0.68 (164.2 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:56:10.974792: step 900, loss = 0.53 (160.5 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 19:56:51.863710: step 1000, loss = 0.68 (167.2 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 19:57:34.187361: step 1100, loss = 0.59 (168.2 examples/sec; 0.381 sec/batch)\n",
      "Epoch 3 of 10 took 159.532s\n",
      "  training loss:\t\t0.604752\n",
      "  training accuracy:\t\t71.92 %\n",
      "  validation loss:\t\t0.512191\n",
      "  validation accuracy:\t\t85.36 %\n",
      "2017-08-20 19:58:19.066600: step 1200, loss = 0.56 (167.9 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 19:58:59.701854: step 1300, loss = 0.54 (163.9 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:59:40.456841: step 1400, loss = 0.52 (165.5 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 20:00:21.315041: step 1500, loss = 0.54 (163.4 examples/sec; 0.392 sec/batch)\n",
      "Epoch 4 of 10 took 158.378s\n",
      "  training loss:\t\t0.545133\n",
      "  training accuracy:\t\t79.57 %\n",
      "  validation loss:\t\t0.486661\n",
      "  validation accuracy:\t\t85.76 %\n",
      "2017-08-20 20:01:06.754992: step 1600, loss = 0.52 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 20:01:47.488310: step 1700, loss = 0.41 (163.4 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 20:02:28.336738: step 1800, loss = 0.45 (162.0 examples/sec; 0.395 sec/batch)\n",
      "Epoch 5 of 10 took 159.448s\n",
      "  training loss:\t\t0.483737\n",
      "  training accuracy:\t\t84.56 %\n",
      "  validation loss:\t\t0.483889\n",
      "  validation accuracy:\t\t85.07 %\n",
      "2017-08-20 20:03:14.123019: step 1900, loss = 0.43 (163.5 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 20:03:54.795891: step 2000, loss = 0.40 (164.3 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 20:04:40.131916: step 2100, loss = 0.38 (163.2 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 20:05:20.739200: step 2200, loss = 0.51 (164.8 examples/sec; 0.388 sec/batch)\n",
      "Epoch 6 of 10 took 162.710s\n",
      "  training loss:\t\t0.428053\n",
      "  training accuracy:\t\t87.96 %\n",
      "  validation loss:\t\t0.479271\n",
      "  validation accuracy:\t\t84.55 %\n",
      "2017-08-20 20:06:05.692266: step 2300, loss = 0.37 (165.0 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 20:06:46.502815: step 2400, loss = 0.38 (166.0 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 20:07:27.209566: step 2500, loss = 0.39 (164.8 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 20:08:07.910506: step 2600, loss = 0.38 (163.7 examples/sec; 0.391 sec/batch)\n",
      "Epoch 7 of 10 took 158.092s\n",
      "  training loss:\t\t0.417669\n",
      "  training accuracy:\t\t88.92 %\n",
      "  validation loss:\t\t0.493272\n",
      "  validation accuracy:\t\t83.51 %\n",
      "2017-08-20 20:08:52.718101: step 2700, loss = 0.49 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 20:09:33.583131: step 2800, loss = 0.41 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 20:10:14.223303: step 2900, loss = 0.42 (163.7 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 20:10:55.029209: step 3000, loss = 0.35 (163.1 examples/sec; 0.392 sec/batch)\n",
      "Epoch 8 of 10 took 160.692s\n",
      "  training loss:\t\t0.413289\n",
      "  training accuracy:\t\t89.03 %\n",
      "  validation loss:\t\t0.482016\n",
      "  validation accuracy:\t\t84.09 %\n",
      "2017-08-20 20:11:42.307201: step 3100, loss = 0.38 (164.4 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 20:12:22.882071: step 3200, loss = 0.40 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 20:13:03.644685: step 3300, loss = 0.37 (164.7 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 20:13:44.264068: step 3400, loss = 0.37 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 157.785s\n",
      "  training loss:\t\t0.408170\n",
      "  training accuracy:\t\t89.31 %\n",
      "  validation loss:\t\t0.494625\n",
      "  validation accuracy:\t\t82.81 %\n",
      "2017-08-20 20:14:29.016929: step 3500, loss = 0.39 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 20:15:09.730954: step 3600, loss = 0.37 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 20:15:50.431880: step 3700, loss = 0.37 (165.2 examples/sec; 0.387 sec/batch)\n",
      "Epoch 10 of 10 took 157.997s\n",
      "  training loss:\t\t0.406861\n",
      "  training accuracy:\t\t89.39 %\n",
      "  validation loss:\t\t0.492873\n",
      "  validation accuracy:\t\t84.03 %\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-19312a2ecec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Aggregate results and save as a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mfold_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m fold_results.to_pickle(\n\u001b[1;32m     65\u001b[0m 'cnn_{0}_results_sgd_{1}_fold{2}.pkl'.format(FLAGS.model_type, FLAGS.initial_learning_rate, ''.join([str(i) for i in fold_to_run])))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/')\n",
    "import os\n",
    "from tf_trainer import Trainer\n",
    "import logging \n",
    "from tf_model import TFModel\n",
    "from tf_dataset import TFDataset\n",
    "from tf_trainer import log_info_string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,2'\n",
    "\n",
    "FLAGS.initial_learning_rate = 0.0001\n",
    "FLAGS.batch_size = 64\n",
    "FLAGS.num_gpus = 1\n",
    "FLAGS.num_epochs_per_decay = 3\n",
    "FLAGS.num_time_steps = 64\n",
    "FLAGS.data_dir = '/braintree/data2/active/users/bashivan/Data/fmri_conv'\n",
    "\n",
    "\n",
    "if FLAGS.fold_to_run == -1:\n",
    "    fold_to_run = range(FLAGS.num_folds)\n",
    "else:\n",
    "    fold_to_run = [FLAGS.fold_to_run]\n",
    "\n",
    "# Load the dataset\n",
    "fold_pairs = []\n",
    "\n",
    "model = TFModel()\n",
    "dataset = TFDataset(data_dir=FLAGS.data_dir)\n",
    "tr = Trainer(model=model, dataset=dataset)\n",
    "print(\"Loading data...\")\n",
    "tr.load_data(random=False)\n",
    "\n",
    "sub_nums = tr.subjects\n",
    "subs_in_fold = np.ceil(np.max(sub_nums) / float(10))\n",
    "# n-fold cross validation\n",
    "fold_results = []\n",
    "for i in fold_to_run:\n",
    "    '''\n",
    "    for each kfold selects fold window to collect indices for test dataset and the rest becomes train\n",
    "    '''\n",
    "    test_ids = np.bitwise_and(sub_nums >= subs_in_fold * (i), sub_nums < subs_in_fold * (i + 1))\n",
    "    train_ids = ~ test_ids\n",
    "    fold_pairs.append((np.nonzero(train_ids)[0], np.nonzero(test_ids)[0]))\n",
    "\n",
    "train_dir = FLAGS.train_dir\n",
    "log_info_string('Start working on fold(s) {0}'.format(fold_to_run))\n",
    "for fold_num, fold in enumerate([fold_pairs[i] for i in fold_to_run]):\n",
    "    log_info_string('Beginning fold {0} out of {1}'.format(fold_num + 1, len(fold_pairs)))\n",
    "    FLAGS.train_dir = os.path.join(train_dir, str(fold_num))\n",
    "\n",
    "    print('Splitting the data...')\n",
    "    tr.split_data(fold)\n",
    "    print('Preprocessing data...')\n",
    "    tr.preprocess_data()\n",
    "\n",
    "    fold_results.append(tr.train(fold_num=fold_num))\n",
    "\n",
    "# Aggregate results and save as a pickle\n",
    "fold_results = pd.concat(fold_results)\n",
    "fold_results.to_pickle(\n",
    "'cnn_{0}_results_sgd_{1}_fold{2}.pkl'.format(FLAGS.model_type, FLAGS.initial_learning_rate, ''.join([str(i) for i in fold_to_run])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "fold_num = 0\n",
    "fold = fold_pairs[0]\n",
    "\n",
    "FLAGS.train_dir = os.path.join(FLAGS.train_dir, str(fold_num))\n",
    "\n",
    "print('Splitting the data...')\n",
    "tr.split_data(fold)\n",
    "print('Preprocessing data...')\n",
    "tr.preprocess_data()\n",
    "\n",
    "tr.train(fold_num=fold_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch Num</th>\n",
       "      <th>Fold Num</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Training Acc</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Acc</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715234</td>\n",
       "      <td>0.817678</td>\n",
       "      <td>0.823884</td>\n",
       "      <td>0.449204</td>\n",
       "      <td>0.542578</td>\n",
       "      <td>1.270510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715234</td>\n",
       "      <td>0.817678</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.229106</td>\n",
       "      <td>0.464453</td>\n",
       "      <td>1.387059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch Num  Fold Num  Test Acc  Test Loss  Training Acc  Training Loss  \\\n",
       "0        1.0       0.0  0.715234   0.817678      0.823884       0.449204   \n",
       "1        2.0       0.0  0.715234   0.817678      0.999375       0.229106   \n",
       "\n",
       "   Validation Acc  Validation Loss  \n",
       "0        0.542578         1.270510  \n",
       "1        0.464453         1.387059  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cPickle\n",
    "\n",
    "with open('/braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/cnn_lstm_results_adam_0.001_fold0123456789.pkl') as f:\n",
    "    fold_results = cPickle.load(f)\n",
    "\n",
    "fold_results.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAEYCAYAAADPkTRJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXl8VOW9/99nzizZ9z0hCWEnLLIIIohAXVFxo0rr0vZa\ntdd6a6tUvfRntdbaq/XaXrVXvVWvrVblgoJYNyo7iOwGCGv2fd+T2c45z++PgSEhmcxksg1w3q8X\nrxdnzjPPfDPJfOZZvs/nKwkhBDo6Ojo6XTAMdwA6Ojo6gYgujjo6Ojo9oIujjo6OTg/o4qijo6PT\nA7o46ujo6PTAOS+O+/btG+4QdHR0zkPOeXHU0dHRGQx0cdTR0dHpAV0cdXR0dHpAF0cdHR2dHtDF\nUUdHR6cHdHHU0dHR6QFdHHV0dHR6QBdHHR0dnR4wDncAOjrnCo0dFdS1F2I0WEiNzMZsDB7ukLxS\n21ZAk7UCizGU1MjJyAb9I+8r+julo+OFuvYivjz2IgX1u9yPWYxhzEi7hQWj70M2mIYxup4pafyW\nfx5/iYqWXPdjYeZY5oy8i9npy5AkaRijOzfQxVFHpxfq2ot4e/e9WJ0tXR63K218XfQ3atsLue2i\n5zBI8jBF2J3C+r28v//nqMLZ5fE2Rz3/PP4nWqxVXDX+F8MU3bmDvuaoo9MLXx77z27C2JmTtds4\nUvXVEEbUO0JofHrk2W7C2JldJR9Q2XJ0CKM6N9FHjjo6HmjsKKegfrfXdp8dfZ59ZWuGICLv2Jwt\nNFrLvbbbV7qG67MnDEFE5y66OOroeKC2rcCndnaljZLGA4MczcDi6892IaNPq3V0PGCUzcMdwqAR\niJtIgYY+ctTR6QFNUyhp/NantmmRk5maev0gR+Qb7fYGNue/7rXdqLhLhiCacxtdHHV0zqK2rZB1\nh5+mouWI17aSJHPtxEdJCh87BJH5RkXLEU7UbvN432IM5aLUJUMY0bmJPq3W0TmFJlS+KXqPv3xz\ndxdhlKWep6CSJHPDxF8FlDAC3JD9BIkeYjLJwXx36vOEmqOHOKpzD33kqKMDNHSU8cnh31LS1HUq\nnZ10FVeN+wUF9bvYX7aG2rZCjLKZUbGzmZW+jKSIwBJGgBBzJD+8+H/4OG8Dn5e30qgEYzEozIxW\nuWv8XBLD0oY7xHMCSQghhjuI/rBv3z5mzJgx3GHonKMIIdhftoZ/nngJp2p1Px5iiuLaCY8yMek7\nwxidfzg1we8PVfBVZff8zNQQE3+YkU5a6Pm72TRQ6CNHnQuWZls1/8h9plsu47iEy1k84THCLLHD\nFFn/+O9j1T0KI0B5h5Ple0t4e14WQbK+qtYbujjqXHAIIThY8SlfHn8Ru9LuftxiDOOa8Y8wOfna\nc/bscaNdYV1pU69tKqxONlS2cF1a1BBFdW6ii+NZ1LUXkVf7NQ7VSmxoBuPiL8MoW4Y7LI8IIaho\nOUJRwz40oZAUPo5RcZcE1FnfQKLNXs8/jvyek2ft5o6KvYTrs1cQEZQ4TJH1n1anynuF9Th9WCnb\nXKWLozd0cTyF1dnCusNPd0uBCDFFcc2E5WQnXTlMkXmmyVrBmoO/pqz5UJfHo4JTWJL9azJipg1T\nZIFJbtVXfH70uS5npc1yCFeO+xnTUm86Z0aLQghqbAonW22cbLGR12LjZIudKpvn89Rn0+pUBzHC\n8wNdHAFVc/Levp93sXc6TYeziY8O/j9kycT4xAVDH5wHOhxN/G3PAzTbKrvda7JW8N7+h/jBxa+T\nEqmfn+1wNPH50T9wpLqrQURG9HRuyH6C6JCUYYrMO4omKG13uIXwZIuNvFY7Lf0Ut8Rg/YSMN3Rx\nBA5X/bNHYezMVydeYlzCfCQpMBaxdxV/0KMwnkbR7GzKe5U7Zrw0hFEFHsdrtvLpkd/T7mhwP2Y0\nWFg05gFmpd8WML9PAKuikd/qEr+TLTZOttooaLXj0HxPKAmSJZyaQPXylMWp+pTaG7o4Ajnln3ht\n02gt5z82LAiYD1PntBNPFNTvosVWQ0RQwhBEFFjYnG2sP/4iORWfdnk8NTKbJZOeJC40o899NtgV\nStodmA0SYyKCMBn8n4Y32jtPi+2cbLVR2u6gL3l1MWaZMRFBjA4PYkxEEGMiLKSGmPmsrInnc6s8\nPm92XCgXx4X6HfuFgi6OQIut2qd2imYf5EgGnprWvAtOHAvqd/HJ4Wdosde4HzNIRhaMvo85GXdg\n6GOpgMoOB/99vIbtNa3uEVm0WebWjBjuyIpF7mWtUhOCSqvTPSU+2eoSwzq70qcY0kLMjImwMCY8\niNERLjGMtfT8c1w/IhoNeO14DW2K5n5cAq5MieCRickYzpH11eFEF0dcKRznKyu/Xc7ouLlMTbmO\nMfFzz2s3FofSwVcnXmFf2YddHk8MH8uNk54kMXx0n/us6HDwwK4iGuxd1/gaHSpvnKylsNXOE1NT\nMEiu6Wxhm/3UBskZIexQNQ+9d8ckSWSFWxh9SgjHRAQxKtxCiLFv2QdLRkRzZXIkW6tbqbQ6CDXK\nzEsIIzlET/72FV0cgYlJ36Gq9XivbcxyCN+f/qeAsbHakvcGJ+u2e22nCZUTtVs5UbuVYFMkk5Ku\nYkrKdSRHjD9ndmd9objhAJ/k/raL0askycwb+UMuy/qR318K/3W0upswdmZDVQsNDoVWp0pRmx2l\nD/PiMKPBNR0OD3KJYUQQGaEWjP2Yrncm2Gjg6tTIAenrQkQXR2Ba6k3sKn6fdkejxzaXjryLEdFT\nhzCq3lk09qcUNuzpdapvkUOxq2eSnK3OZvaUrmJP6SriQkcyNeU6JidfQ3hQ/FCEPCg4VRub817n\nm+L3odOKXVxoJjdOeqpfu/VVViff1LZ5bXegocNrm4Qgo1sIXeuEFpKCTefVF9T5hn62+hRVLSf4\n4MDDtNpru92bOWIp14x/JGA2Y06TV7eTD3NW4FC7fjglDHxn7IPMSr+dvLqvOVjxKSdqt6OJ7utc\nEgayYmcxJWUx4xIuxyQHDVX4/aa8OZePDz9NfXtRp0cl5mR8nwWj7+938v7XNa08vr+sT8+RJUgP\ntTA63HJqk8QlhJFmfRxyrqGLYyccipXcqvUcr92GotqIDU3notQbSY4YNyD9DwYdjmZyKj6hqGEv\nqqaSHDGeaWk3EhOS1q1dbtV6DlZ85tGn0GIMZWLiFUxJWcyIqKkBO6pRNSdb899kR9HfEOLMlDc6\nOI0lk54gPfqifr/G6U2YLdWtXttaDBI/m5DI6IggssIsWPQzy+cFujhegNS2FXKw4jMOVX7e40gZ\nIDo4lckpi5mSvDigkqSrWk/w8aHfUNOW1+XxmSOW8p0xD2I2BvvdtxCCnMYOVhU3sqO6FV+3Ua5K\nieD/TUn1+3V1AhNdHC9gNKFSWL+Hg5Wfc6x6k8f1y/ToaUxNWcyExO9gMQ5PfpymKewoeoet+W90\nWR6ICEpkSfYTjIy92O++7arGxsoWVhc3cLK1b+laEvDanEwmRPovyjqBiS6OOoCrgt7R6o3kVHzm\nsZKe0WBhfOJCpqYsJjNm5pCZW9S1F/Hxod90Ww64KPUGrhz7c4JM/qVi1dmcfFzaxMeljTQ5uu9I\nJwQZWTIiil217Rxq6jnp/hcTE7k5Pcav19cJbHRx1OlGY0c5Bys/51DFZx5rIEdYEpiUcg1TU64j\nLjRzUOLQhMru4pVsynuty6g2zBzLddkrGBs/z69+jzZbWV3UwKaqlh5Tb6ZEB7M0I4Z5CeEYDa78\nxU/LmlhX2khhmx2zwcCsuFBuy4xhcnSIvz+eToCji6OOR4QQlDZ9S07FZxyp+qrbrvhpUiKymZqy\nmOzkKwk2DUxeXW9lC64Zv5wQc99eR9EEW6tbWVXcQG4Po0CTJLEoOYKlGdGM06fIOujiqOMjTtXG\nsZotHKz4lML6PYgetitkycSY+HlMTbmOUXFzkPt4TA88ly0INkWyeMJjfS5b0OxQ+KSsiTUljdTa\nuqcyxZhlbkyP5sYR0cR4OI6nc2Gii6NOn2mx1XCo8gsOVnxKXZccwzOEmKKZlOw6jZMUPrZLWlB5\n02H2lK6movkISBLpUVOZmb6UYFNkj2ULxsbP57qJj/epbEFBq40PixtZX9GMvQdXm3ERQSzNiGFh\ncjhmg556o9MdXRx1/EYIQWXLUXIqPiW3an0XE9nOJISNZkrKYiYnX8Pe0tVsK3irx3ZGgxlFc7iv\n+1q2QBOCnbVtrC5uYF999yUAWYL5ieEszYhhUlRwwOZx6gQGujjqDAiK5iCv9mtyKj4lr24Hmui+\n+yth6HE63hNZsbO5IftXPpUtaFdUPi9r5sOSBso7urthh5sMXJ8WzS3p0brJq47PDOoiy7PPPktO\nTg6SJLFixQqmTJkCQHV1NcuXL3e3Ky0t5ZFHHiEhIYGHHnqIMWPGADB27FieeOKJwQxRZ4AwGsyM\nT1zA+MQFtDsaya1cT07Fp10MPXwRRgkDiyc+6lPZgvIOBx8WN/BZWXOPzjcZoWaWZsRwVUokwUZ9\n6qzTNwZNHHfv3k1xcTErV64kPz+fFStWsHLlSgASExN55513AFAUhbvuuotFixZx+PBhZs2axUsv\nXdju1ec6oeZoZmXczqyM26lpzedgxafkVH5GRy/GHqcRaL1Oo4UQ7G/oYFVRAztr23o0h50TH8bS\njGhmxobqU2cdvxk0cdy5cydXXHEFAKNGjaK5uZm2tjbCwrom7K5Zs4arr76a0FDdmfh8JCF8FFeM\n+xlTUq/j9a+/79NzFM3ezQDDrmqsr2hmdbEr1/BsgmUD16ZGcmtGNCNCA7dapM65w6CJY11dHdnZ\n2e7rmJgYamtru4njqlWreOutMwv0eXl5/OQnP6G5uZkHH3yQuXPnen2tffv2DVzgOoOCIuwYMKHR\ne4U8sxRGbs4JtwNSkyqx3Wpkp81Ih+g+CowxaFwWrDA7SCHY2kbNsXJqurXSGSzO5/X+IUvs6mnf\n58CBA2RlZbkFMzMzkwcffJBrr72W0tJS7r77btavX4/Z3LvB7ED9glqdKp+XN7OzthWbIhgRaub6\nEVFMDuCdzTqbk3WlTRxo6EARgrERQdw4Ioqs8MCzHqvJvZZvy9fRLhIpEwtpIQsQRHGCNGkzwVI9\nMzNvYcbomeQ2WVld3MiWupYei0VNiwlhaUYMlyaE9VqmQEfHXwZNHBMSEqirq3Nf19TUEB/f1VR1\n8+bNzJkzx32dmJjI4sWLAUhPTycuLo7q6mpGjBgxWGG6OdJk5bF9pTR3KnmZ22zli4pmFqdG8stJ\nyQH3IdxW3crTOeVd8vhym6ysKWnkntFx/GB0YJnYXj7qx3xZJXPQeQNwZoOkiXEUi2uZaVqHLegW\n7v+miGPNtm7PNxskrkyJYGlGDKMCUPx1zi8GbQtv7ty5fPnllwDk5uaSkJDQbUp96NAhxo8f775e\nt24db775JgC1tbXU19eTmOg9laO/NNoVHj1LGDvzWXkz/5vXs7XXcJHXYuOpb8t7THAGeDOvji/K\nm4Y4qt452BLCQeeN9PRnJzCyx3ELz+U2dRPGOIuRe8fEs3rBaB6blKILo86QMGgjx+nTp5Odnc2y\nZcuQJIknn3ySjz76iPDwcK688krAJYCxsWdOPSxatIjly5ezYcMGnE4nTz31lNcp9UCwrqzJa5H0\n/ytqYEpUSMAYmb6TX4fTS4rqGydrXVb8BMaI93+O9201cGJkEN/NjOHyxIgBq6uio+MrehI4cM+O\ngj77+OkMHguTwrk9M5aJUboBhM7woZ+0B9oV30tn6gw+D09M0muu6Aw7+l8gkBxiosLae4oJQLjR\nEDDTuyaH2mMC9NlEmmQCIWQBPRrKnk2Y0UCYaWhMdHV0ekMXR2BxalSPRgWdGRFi5p3LsjAEyI71\nWydreTu/rtc2s+NC+cPM9CGKyDtP55TzVWXP5hSnuSY1MuCyAnQuTAJjd2GYWZgUwZRoz+tbBuDB\nCYkBI4wA382MITXEs4lCsGzgJ+MShjAi7/zL6HgiehkVxgcZuSMrbggj0hluXn75ZT7++OMuj+3a\ntYtf/epXwxTRGXRxBIwGiednjOCK5Ihub0i8xcgz09OYE+9fnZLBItwk89KsDGbEdrfpzwg188eL\n0wMu5SUt1Mx/zUpnTHj3432To4J5eVYGsbrh7DnPrl27WLBgAffff7/7X39PsX300UfMmzePpqYz\n6Wl33XVXf0PtFf0v8RQhRplfT03lvrEJ7Kptw6ZqjAg1MysuLGDWGc8mPsjEHy/OoKDVxoGGDtRT\nJ2SmRocE7ImeUeFBvHHpSA43WTnabMMATI4O1ksTnGfMnTuX3/3ud10ey83N5eWXXyYmJga73c7v\nf/97973q6moef/xxRo4cSUdHB7LcfYaxcOFCfvvb3/Kf//mfXR6/6667+MMf/kBSUhKPP/44S5cu\nZefOnVRUVJCUlEROTg7z58+npqaGkpISXnnlFZ9+Bn3keBZJwSZuTI/m9pGxXHqqwFKgkxUexK0Z\nMdyWGctFMYHvRCNJEpOjQ7gtM4almTG6MF4g/PGPf+Sxxx7j2WefJS0tzX1IBFweCzfeeCO//vWv\n3daGZzNt2jQSExP5/PPPfXq9UaNG8dBDDxEZGUlSUhKPPvooNTU1NDQ0+PR8XRx1dHQGnO3bt3PP\nPfe4/+Xn51NeXk56umuDMC0tjcrKSnf7mpoakpOTAXo9Lvzzn/+cv//9712OJnvi9HFli8VCXFyc\n+/8Oh6O3p7nRp9U6OjoDzrx587pNq9PS0igpKWHkyJGUlpYyfvx48vPzAZeQVVRUAFBYWOixX7PZ\nzGOPPcbTTz/d5bHTgtdZcPuLLo46OjpDwsMPP8wf/vAHoqOjUVWVn/3sZ7z66qsA3HrrrTz22GPs\n3bvXaz+TJ08mKyuLTz75BIDrr7+e5557jvHjx6NpA3egQz8+qKOjo9MD+pqjjo6OTg/o4qijo6PT\nA7o46ujo6PSAviFzFqJdQStoQzg0DHEWpPTATajW0dEZPHRxPIVwaihfVKLmNNK5aIkUa8F4XQry\nyMA6PqijozO46OIICE3gXFmMlt/W/V69Hee7RXBnpi6QOjr9RDS1oB46gWhqBYsZeWwmUkZKQM7O\ndHEEtGMtPQrjmQYC5fMKDP86JiB/iTo6gY4QAnXzbtTdB7s8rh04gpQcj+mWq5DCupuo+Mqzzz5L\nTk4OkiSxYsWKLkcQv/76a1588UVkWWb+/Pn89Kc/9alPfUMGUPd7P2spau2IcusQRKOjc/6h7tjf\nTRhPIyprcf7f5wjVuxlyT+zevZvi4mJWrlzJ7373u24nc5555hlefvll3n//fXbs2EFeXp5P/eri\nCIgG385ainq9zoyOTl8RdodHYXS3qW1AO+752GBv7Ny5kyuuuAJwmU00NzfT1uaaCZaWlhIZGUly\ncjIGg4HLL7+cnTt3+tSvLo4AZh/fBl/b6ejouNHyisGpeG93xLcR3dnU1dURHR3tvo6JiaG21lVK\nuba2lpiYmB7veUP/tAOGcRG+NTynD1rq6AwPot235SjRYfPeyJd+BuhEtC6OgHFmjE+jQueqEpyb\nqhGarpI6Or7i60aLFOrfhkxCQkIXC7Oamhq3XdnZ96qrq0lI8K18iC6OgBRuwvy9DLB4eDs6bVCr\nW2twvluIaPNerXAoEEKglVej7DyAsn0fal4xYgCdSQYDIQRaQSnKjv0oX+9HK6kcsG/7wUKoKuqx\nApTt+1C++Ratun64Q/KKsDtQD51A2bYPZfdBRGPvxc0GC8PoDDB7rnfkbpc92q/+586d6zbOzc3N\nJSEhgbAwV9pdWloabW1tlJWVoSgKmzZtYu7cuT71q7vynEJYbTjXbkUrUECNAgwg2SCkCXnWBLT9\nAtHUSRDDjJiXpmPICO33a/sdc1MLznUbEZVnraFEhGG6bgGG9OThCawXtMpalE82dvugSvExGJcs\nwhAX7eGZw4eaV4zyxTY4a3ooZaRgun5hv1JQBgv1wBGUzbvB0fVL3DBhFMZrLkPyQawGEmVXDurm\n3R7vS8nxmO5YgiT7N1574YUX2Lt3L5Ik8eSTT3LkyBHCw8O58sor2bNnDy+88AIAV111Fffcc49P\nferiCAhFxfneJyg1OTjNh9DkcpAEkhaC7ByHyTEJ4+Jr0XINaMdbzzxRAuOiJOS5cUOe/yg6rDj+\nuhZaPORnGmVM378BQ3L8kMbVG1pdI853Pu72gXUTEoz5BzchRQROsr1WWIZz1Rfg4WMixUZhuvum\nIReb3lC/PYry5XaP96XMVEy3XTukf7NCCNSvD6B+vR/OWpaS0pMx3XgFUkhgFYTTk8AB7Wg+jrot\nOEO6bvELQweK5QCqsZCg7cGY7/8h2s56lA1Vrs0ZAcqGKrTSdkw3pSEFD93bqe497FkYARQVZese\nzLcvHrKYvKFu3+dZGAE6rCi7cjBd6du0Z7ARQqBs2uVRGAFEfRPqweMYZ04awsg8I5wKypY9vbcp\nKkcrKEMe5bkcwUAjSRLGudORp45HzT2JaGpFspgwjB2JlBwfkIcrdHEEnDnbcVo85z4JuQmHYz2m\n/EXI05OQ4o04P6mENlfSqnaiFftrJzHdmIwheWi+/dSDx722EUXlaDX1SOHDN/V3x2Kzo53wnsem\nHTqBduk0JMPwL4drNQ2IWu8HBNSco8h+rpcNNOrJYrB5z8fVDh0fUnE8jRQWgnH21CF/XX/Qp9VA\n63//GIXDvTcSEkHtt2EQrimfEEZwjgYtqlMjDYzFIFcTgF+EOjpupOQEzHffONxhBDTD//UcAKhS\nhfdGkkCTq89cSgqYjoFcxpkESAMoI8E5GiH0t1YngAkyD3cEAY/+CQYw+7q6cNZCsgSSqcwlknRa\nS9PiwDEZoen1mHUCE3l81nCHEPDoa46AMXUKzoKvvLaz3HEPcmR6j/dEq4JzXSWi/FSWvwgG7SLk\n7yQgT/bxBE4f0OqbUD74FFTPOY3yNZchj80c8Nf2F62oHGXdRs8NJAnjd6/FkBQ7dEF5Qc05htrb\nBofZhOmOGwJiXfc0ypY9aDnHPN6XoiMwTBg1hBGdQW0ux3H8U7TmciRLGKaRl2NMm4kkBd44TRdH\nIGjqdz2Lo8CVspN2McaksR77kILB/KPRKBuqUHeeyshXBMrn1YhKB8bFKUimgfsDkNOSkG69Gufa\nr7rvAEsS8sLZGKeOH7DXGwjkCaPA4URZvwPOTlQ3yhivX4g8MnV4gvOAPHsqOBTUnQe63wwJwnTL\nVRgSAkfMAYxXzkVRFLTc7meVpegITN+9Fsk0tB99ITSsO/+M/dv36DwDsx/+EDlhImHXPo8hNM7v\n/k+cOMEDDzzAD3/4Q+68884u9/y1LNM3ZE7RsfMV7Afe7flmcAwRt/wPcmSaT32px1pwri0F+xkB\nkBKDMH03HUOspd+xdkZYbS7z0JJKhKZhSIxDnjoOKWrgR6sDhWhpQz14HK2iBkmSkNKSkKeMQwoN\n3GUIra4RLecYWl0jktGIYWQahuzRSJbAXLsTQiAqa1EPHkc0tyKZzRjGZbpSZ4zykMdj3fMGtj1v\neLwvx44mfOn/Isl9zxft6Ojg/vvvJzMzk3HjxnUTx8WLF/Pmm2+SmJjInXfeydNPP83o0d6zC/SR\n4ymCL/kpcnQmtgPvojUWdbkXMuten4URQB4fgXT/GJyrihGVrmm2qLbh+J88TEvSkLMjByxuKTgI\n46wpMGuK98YBghQRhnHeuVVr3BAXjeE7c4Y7DJ+RJAkpJQFDim/niAcTYW/D5mngcQq1Pg9n/kbM\nY6/uc/9ms5m//OUv/OUvf+l2r7NlGeC2LPNFHANvoj9MSJKEZfz1RCx7n4g7PsSYNst9T6n4ts/9\nGaLNmP9lFPKMM3ZJODScq0twfl6B6GWtUEfnfMJRtA0U7447jpPr/erfaDQSFNRzfrFuWTaASJKE\nHJmKZfx17secxTsQat+NJiSjAdP1qZhuGQGd1hvV3fU4/rcA0eSbya4vCCHOObcgoZ2jMZ9jK1FC\nHd6YhbXRp3aaj+2GCn1a7QFTxlwwGEFTEI42lPJ9mNIv8asveXIUUlIQzlUliFrX6QVRbsX+eh6m\nm9OQx/q/PtiSr1J3QKG9TAMBQXESMVOMxEySkeTAy0QXmqDpqEp9joK1WoAEIckGYi+SiRwrB+Qx\nMk0R1OcoNB5SsTcKJAOEZRiIm24kLH3o1+98QbEJ6vcrNOaqONsEkhEiRsvEzzASnDC0YyJfN1oM\nIQO/saVblg0CkiUMY9rF7mtHweZ+9WeID8L849EYpnQ6UWNTcb5fjHNDlV8jqKrtTorXOWgv1dwb\ngLY6QcVGJ0XrHGhqYI1whCYo/dxJ2XqnSxgBBHRUaJR+5qRiozPgRmWqQ1C42kHVVgV7oys2oUFr\noUbhhw5q9wWGdV1nnG2C/Pfs1OxScLadilmB5mMq+e/baT7pX60WfzFlzgOTd+ci87hrB/y1+2NZ\nJj/11FNPDXhEQ0hlZSUpKSmD07liw1nkcjfR2qqxTP1ev/KxJFnCMD4CKdzkqnZ4Wh9KOtCK25FH\nhSFZfBuJtBSoVGz0/MF0NAkkIGxE4Ixs6g+o1O33bJdvrRaYoySC4wPnO7tys5OWfM/rw23FGuGZ\nMqbwwBnxFq9zYKvx8CUjXLON6GwjsnloYpZkE5LBgFLmOV9UTphI8JwH/TpTf/jwYR555BF2797N\noUOHWL9+Pc3NzdTV1TFq1CjGjh3LU089xUcffcQ111zDokWLfItbT+XxjNbRQPPb13FaxcJueg1T\nykUD03el1TXNbuy07hhqxHTrCJ/qYxestrtGjL0gGSF+phEpAPRRCKjbp6B58UQwhkHs1MBY7dGc\nULtH8VoewxInETUuAN5kQGkX1H/rfWSYcImRxDlDZ7MmhMC29y1s+94CrWt8xtQZhF71OwzBUR6e\nPTwMqjh6qiVbXV3N8uXL3e1KS0t55JFHuOGGG3qtP9sTgymOAK1r/xWlwpUAbJm6jJC5Px+wvoVN\nxbmuDO1oJ+NXCYwLEpEv82zjJITg8Es20De8dfwkNNVA1m0Dm3PrC1pHPY7jn6O2lCOZQzFnLURO\nmBiQa806j6y6AAAgAElEQVSD9hXduZZsfn4+K1asYOXKlQAkJibyzjvvAKAoCnfddReLFi3q9TnD\nhWnkArc4Ogs2Iy59aMB+kVKQjOm76ai76lH+WekSOwHKpmqXR+TNI5BCuv+K7A2aLow6/UIM09+P\nISSWoGl3em8YAAyaOHqqJXu6tsNp1qxZw9VXX01oaKjPzxlKTFnzse74IwBaaxVq3XGM8QN3LE+S\nJIyXxGFIDcaxuhRaXOuIWl4b9tfzMH83HUNaCEII2ko06vYrtBX5+JctQfhIAwFxbFVAS6F3UZeM\nEJ4ZCAG7NjFafXiv5RAITQmMmFWroL3c+2QwODHwRmqBxqCJY11dHdnZ2e7r08mXZwvdqlWreOut\nt/r0nLPZt2/fAEbendTQdCztJQCU7PiAxhE3DMrryJdA6gEj4bWnPmgtTuxv5VGdaqbZmYDB2rfj\ndWpiI/VplYMQqX/I9gTkst7TOpwjqqlLCZziVcbWNAz1EQgEEj0Lii2zBGtcL67sQ4kAY2MWho7e\nTZer5BNU7vNuiuuNwVzSGm6GbOW7p6XNAwcOkJWV5VH8fF0OHexfkJXF2Ha9BkCs9ThZM54atNcS\nlwjUbbUom6tBgCQkksqchBqbqAk2o53aXTFHSUgy2Ot7fo8s0RJZNyVjDB6knXw/UCcJClbbPe6k\nhqYZyLw+A4Mxc2gD6wXHGI38lXaUtp6FMWqiTNpVYwNqzawjTaNwtR3NQzJD4qVGEmYHRlmHQMbr\nXMBqtbrLHgL83//9Hx0dHV477q2W7Gk2b97MnDlz+vSc4cCctcD9f62xEPWss9cDib1eUGWLojw0\nBaXTNnO40s6I9jIi4xxk3Ghm7A8tjLnDQsIlRoydUsgkI8RMlsm63YIxOHA+sACyRSJrqYXYi2QM\nnfwa5CCIv9hI5s1mDMbAitkcYWD0siCixstddv1N4RJJ842kXWUKKGEECEkyMGqZhfCRXT/elhiJ\ntGtMJMwevmJg1rZy8g+/Tu6uJzm2/3kaqncjhmsB1AteR46PPfYY06dPd183NTXx6KOP8sorr/T6\nvLlz5/Lyyy+zbNmybrVkT3Po0CEWL17cp+cMB3J0JoaoDLSmYgAcBVsInpE5YP0LIWgtdK0nutNz\nDCGUhI4gyVpFiOo6l2rWnCQUl2LMTgGikWSJxDkm4mcZsTcIhOr6AAxV/po/yBaJlIVmkuYJ7A2u\nEzKWGCngRLEzpnCJEdeaSVkosDcJDEZXzJIhcGMOijOQeZMFZ7vA2SowmF2zieESciE08g6+QsmJ\nrpZl5fkfEhGTzZRLn8cS7L9l2fPPP8++fftQFIX777+fq666yn3PX8syr+LY2NjID3/4Q/f1fffd\nx1133eW14+nTp5Odnc2yZcvctWQ/+ugjdy1ZcB0Kj42N7fU5gYI5awG2/X8FwFm4heAZP+h3n5pT\n0HhEpf7AmdMXXQg14pydgdRaj9h9akStCpRPytGK2zFdl4pkNiBpAovVitAEBiWoD87mw4ekaFis\nVpAkJDUIhsFGq68YnAoWqx3JKIEWDAEsjqeRHU4MVgeSZnAZMA9TyIVH3qLkxN97vNfSkMu32x7i\n4ivexmDo+6j2m2++4eTJk6xcuZLGxkZuvvnmLuL4zDPPdLEsu/rqqwfGsszhcFBUVERmZiYAR48e\nxen07chU51xGgPHju+7yfvLJJ16fEyiYsha6xVGtOYLWWoUhPMmvvpxtgvpvFRoOKag9mJVYYiXi\nphuJGi+fGlElo44KxbmmDGynKh4ebMJRaUVKDXblSZ72jjRIGLIjMF2ZjBQeOLWUTyM6FJR/VqEe\nbgLl1BeC2YA8NRrjFYlI5sATSa2uHceaHERFKNKplShhtCFPNWNaPCUgR5BqUT2OfxxHqg93PyYs\n7chzojHNH9o1UsXZRvHxd3pt09acR03ZRpLS+25ZdvHFF7vzoSMiIrBaraiqiizL/bIs8yqOjz/+\nOD/+8Y+x2+0IIQgNDeW5557r8w9wriPHj8MQloTWVgWAo3ALQVNu71Mf1mrX1LnphNpjSktY5mkz\nA0O3P155bASG+0fjWF2CKLcCIGrtbiMLN5pAO9SMo7QD87+MCiiBFFbF5UZUd1bMDg11Tz1aRQfm\nH2QNqGN6f9FqWrD/5SiSEt5l0CUpQWj7wF61E8s9cwJq3VE5UYHzgyokEd7lcckeirbZgb1+L0G3\nXOzh2QNPbcU2tJ5GAWdRVfKlX+IoyzIhIa6F99WrVzN//nxk2fUl25NlWWlpqU/9ehXHadOmsX79\nehoaGpAkicjISIzGwJ+2DTSSJGHKuhz7QVdSurNgs0/iKDRBS4FLFDvKuyuiJEP0RJnYaUaCYnsX\nBSnKjPlHWSjrq1B3957uIpqcODdUYb5p6GsTe0LZUtNdGDshyq2o39RhvGz4DVpPY/+/vUhKvOdU\nnvJwnDuPYL40u/u9YUAIgfOjfCTRy1G8Q0Eo06owjvRv5tNXnHbfrMic9qZ+vc5XX33F6tWr3amB\n/cWryv3zn//kww8/5LXXXKkst912G/fee6973fBCwpS1wC2OSmUOmrURQ3B0j21Vh6DxsEr9twqO\n5u7ricZQ1xnimCnGPu0qS7IB07UpqMda3AnjntBymrC3OANi2icEiELvuYDK1hrU4vbhWhrrgnA4\nod61SeApxxFA3dCOLefroQqrV4RNINm9n1F2bs4dMnE0B/lmRWYOivHeyAPbtm3jtdde44033iA8\n/MyIuT+WZV7F8c033+T11193X7/xxhsXrDgak6YgBUe7zDuFhrNwG5aJS7q0cbRo1H+r0nBIQevB\nyzYoQSJumpHIcTIGP/0WhRBehdHdtrDdm29CYKEIRH5bwMTcmyi622jBUDMEwfiAr39Rom7o0mfi\nUi5DNoagKr2nACZl+GdZ1trayvPPP8/bb79NVFTXL4bOlmVJSUls2rSJF154wad+vYqjEILIyDM1\nTyIiIjD4YSt0PiAZZEyZl+E4ug5weTyeFsf2CpX6/SrNeWqPLi4RowzETjcSmtp9PdEvDBKcYy7a\nOgGE5Nk6bqAxGkMYOeFfyDvkOf0vIiab+JTL/er/s88+o7GxkZ///IwpzOzZsxk3bhxXXnklTz31\nFI888gjgKrY1cuRI3+L21mDChAk88sgjzJ49G03T2LZtW7dd5wsJc9YCtzg6S/fRlNtO3UEZa1V3\noTKYIDrbtZ5oiRq4LxRJkjBkhaLleZmmmg3ICxICYrNA4Dr5Q4cXO61IE8ZL/M93G0i09mbU7c1I\n9L6ppZlKEEmBcUxTWCWMdbO8N4z3fpBjIEkfdyeqZqfoyFsI0fVvIDp+BpPmPIvB4N9exu23387t\nt3te/7/44ov9MrDxalmmaRpr165124hNmzaNG264IWBGjwNtWSaEwN4oEE4wRUoYg7oKi1AdNLy1\nlHbrAtrUm1Hpvm5jCpeIvUgmZpIROWhwhEnNb8X5blGvbYwLEzHOD5zNDWV3PcrnFb22MS0dgZwd\nOL5+Ha+uxFAz0eN9gYbh2mYss+YPYVSeEUKj44WPkTs8p6oIbBh/FIkp3XMd9sHCbqunqvgzrG0V\nGE2hxKctJCI6MC3L+uzneODAAdasWcPTTz89WDH1iYESRyEEjYdUavcpOJpcb4lkgIgxMklzjZgj\nDdibNOoPKDQctCK07vWKg5Nc+YmRo4emfouytQZlU3WP9wwTIjAtTQ+IzZjTCCFQ1pWjftvz7qV8\naRzGK5IC6oNiP7EFZXU1snNCt3sCDS1rPyF3/qBfDvEDjTP/IM4PqjAoyd3uCRwwI4/g6/uWhnYh\n4tM4tra2lrVr17JmzRpUVe11CHuuUrnVSf3+rsN9oUHzcZW2IpWgBEMn5+3OwqgSMVomboaZ0JSh\nTWA2zk9AGhGCuqserbANNIGUHIxxZgyGSVEBJYxwyp5tSSqGMeGuvMayDpAkDBkhyLPikMeEe+9k\nCBGqgv3AG6iRBci2SzHa5iMpqSA50YJPYrjITMh37g4oYQQwjZoCPzTh+Gw7UnUGkhoHkhUt/ATy\nnEQss28b7hDPCTyKo9PpZOPGjXz44Yfs3buXhQsXoqpqFxOK84X2MrWbMHZGtdOtJIFEO6Hyp4TJ\nHxE5eTnmlMsGO8wekUeG+VRWIVCQJAl5YiTyxEjvjYcZ+8H3UetPggRq8DYsS67CEGpAksOR425H\nkgMnwf5sTKkTMN07weVB2lKBZIlHjp0TcEIeyHgUx3nz5pGQkMAdd9zBCy+8QEREBDfddNNQxjZk\n1B/0vRqbKcKVimOu+BNqyVeAKyHcnDk84qgzOKjNZVj3vOG+No+9BsuYcy99zRCe5Pcx1wsdj+J4\n9dVX88UXX/Dll18SFhbGVVddFVBrQQOJtdq3nK/YaTLJ801IBgl7yBw6Totj4TaEqiDJF97JofMR\nIQQdW54HxXWaR7JEEDz3oWGO6vygsaOcnIp/0GitIMgYyriEBYyMmRmQI1qPn+ann36aFStW8MUX\nX/DBBx/w9NNPI8syx44dO+9SeXz9vYQkG9zreKbMea6zf0JF2FtQKg9gShu686o6g4fjxBcoZbvd\n18FzH/J4EkrHN4TQ2HDiFXYWd7Us21v6ISkR2dw27XnCLf6lcFmtVh5//HHq6+ux2+088MADLFy4\n0H3fX8uyXmUhKCiIm266iXfffZeVK1dy66238uMf/5hly5b59UMEKqFpPqijRJcNF0NQJMbUMz6X\nzoItgxGazhCjWZuw7vgv97UxdSbmcYt7eYaOL2wteIudxX+npxMSFS25vLfvIVRP1uVe2LRpE5Mm\nTeLdd9/lT3/6E//xH//R5f4zzzzDyy+/zPvvv8+OHTvIy8vzqV+fx7IjR45k+fLlbNmyhfvuu69v\n0Qc4sVONXs9dRY7pXri9s0O4o3BLwDoa6/iO9euXELZTBgiyhZAFj523y0lDhc3Zxs7C3i3Latry\nOFK90a/+Fy9ezL333gtAZWUliYmJ7nudLcsMBoPbsswX+jzRl2WZRYsW9fVpAU1QnIHURZ53HoPi\nJFJ6uG8aeSbxV7TXolYfGZT4dIYGZ+luHMc/c18HX3wPcmTguBqdq5yo3YZT825ZllvZv0yYZcuW\nsXz5clasWOF+rCfLstraWp/6C7xV0GEiZoqRrO+aiRh1ppSpKUIi8VKjx3oshtB45MTJ7mtH4eYh\nilZnoBFOGx1bzviUyjGjsEz9/jBGdP7Q4fDNsqzd0T/Lsg8++IBXX32VX/7ylz4X5+sNr+K4d+/e\nbo9t3Ojf8DfQCU2TyVhiIftnQWT/WxDj7wkiYbap15osnafWzoLNA/JL0Rl6bHvfQmspP3UlEbLw\n3/XsgwEizOKbZVmYxT/LssOHD1NZ6TrbPmHCBFRVpaGhAeifZZlHcayoqGD37t387ne/Y8+ePe5/\nO3fu5JlnnvHrhzhXkCTfCz6Zss44iWjNZWgN+YMVls4godSdxPbtmfomlslLMSbqpUsHirHxl2GW\nQ7y2m5zsn2XZ3r173Qa3dXV1dHR0EB3tyi7obFmmKAqbNm1i7ty5PvXr8auxsrKSNWvWUFpayh//\n+Ef34waDgaVLl/r1Q5yPyJFpyLGjUetdO2COgs0Ex3qvT6ETGAhNpWPz7+GUU4wUmkDw7J8Mc1Tn\nF2ZjCJdl/QsbTnq2LEuJyGZcgn+WZcuWLeNXv/oV3//+97HZbPz6179m7dq17mJ+/lqWeTWe+PLL\nL7n66r7XdRgqBtqVB0DRNJxCI8gg+7RTad3zBrZTpynk2DFE3N77zpxO4GA7uBLr9jNf/qGL/6Cf\ndhoEhBBsK3iTbQVvoZ1lWZYZPYNbpz5LiDlw3JjAB+OJ0NBQPvnkE2644QYee+wxcnJyWL58OVdc\nccVQxDek7G+o44OSAnbV16AKQYIliOtT07ktPYswo+fdbHPWArc4qvUnUZvLkCPThihqHX/RWquw\nfvOa+9o0apEujIOEJEnMH/VjpqfdzMGKz2iyVmA2hjIhcSEpEYFpWeZ1Q+aVV17h0ksvZevWrVit\nVlatWsVf//rXoYhtSFlTVsTP9u/k67pq1FOD6Rq7jbcKTvCve3bQ5PBcGMoQMwpDxBkx1BPCAx8h\nBB1b/wCKq5KjZA4jZN7DwxzV+U+YJZZLR97F4omPccXYB0mNzA5IYQQfxNFisRAbG8uWLVu4+eab\nCQ8PDxij24GioK2FF48d8ni/sL2V/+zl/unKhKdxFGweyPB0BgFn/kacxTvc18FzfoohNDAcyHUC\nA68qZ7fbefvtt9myZQtz5syhtLSU1tbWoYhtyPiwtMhrQactNZXU2Kwe75uzzpzlVKsPobXXeWyr\nM7xothY6tv2n+9qYPBXzxBuHMSKdQMSrOD711FOUlJTw7LPPEhQUxMaNG/nFL34xFLENGd829l4D\nGkAD3i48QVF7K1oPe1hy4kSk0Hj3tbNQn1oHKtZv/hthdeXBYTAScvnjAekKozO8eN2QGT9+PHfc\ncQclJSUA3HrrrYSFnTvmqr6g+lgIdF15CevKSwgzmpgYGUV2RDSToqKZEBFFhMmMeeR87Ic/BE5V\nJpx062CGreMHzopvcRxZ674Omv4D5BjfUjt0+k+5tZFPqw5SYW0i1Gjh8rhxzIzOxBCA645exfFv\nf/sba9euRVEUFi5cyEsvvURsbCz333//UMQ3JIwPj6Kso93n9m2Kk931teyuP3NGMzM0jPHB0xgd\nmc84azkjKg4QamvGEBT4jtcXCkJ1uHIaT2GIyiBoxg+GMaILB00I/lywkfdLd3UZinxUsZ+J4Sk8\nN2kpcZb+DbpsNhvXX389DzzwALfccov78UGxLAP4+OOPWbVqlbt29WOPPcZXX33lZ/iByc0jMry2\niTVbSA8J9Xi/qL2NL5qtvJJ4Hf+WeR/fG/lzfr5nK3/JP8bOumqaHY6BDFnHD2z7/orWVOy+Dlnw\n70hy90JpOgPP/xZv572zhPE0R1or+MXBD3Bqvjvy98Srr77q1qnO+GtZ5nXkGBYWhiyf8TGUZbnL\n9fnA1KhYvpcxiveLez76F2O28OeZc0kLCaXF6SC3udH970hzE+1q9wLpVtnCfjvsLzzpfmxESCjZ\nkdFMiowmOzKakaHhGM+znf9ARW0oxLb/TAqaeeJNmFIuGsaILhzaFBvvlnzTa5u89ho21h7j6sRs\nv14jPz+fvLw8FixY0OXxzpZlgNuybPRo76fYvIpjWloar776Kq2trWzYsIHPPvvM5+M35xIPjJ5A\nRkgY7xfnU9zRBoBJMrAoMZl7R40nKdh1NjTCZGZOXCJz4lyecaoQFLe3kdvcwOHmRg7XllPs7NnX\nsbSjndKOdr6oLAMgWJYZHxHlFsvsyGiizZY+xy6EoNZuQxWCeEvQOSG4qhDU2qxIkkS8JWhQ15yE\n0OjY8h+gub7EpJBYguf4NrXqjFPTqLXbMBsMxJotAZuf1xmbqlBvtxNqNBLlx9/WQLC9Lg+bD0a2\n66tz/RbH5557jieeeIK1a9d2ebwny7LS0lKf+vQqjk8++SRvv/02sbGxrFq1ihkzZnDXXXf1MfTA\nR5Ikrk9N57qUEZRZ27GpKklBIYSbeq8wJ0sSWWHhZIWFc0NqBkIZT9nbN3HCGM3xoDTyRizkiNNA\nm9L9j8OqqhxorOdAp93y1OAQt1BmR0YzOizCo9hpQrCmrIhVpYXuNdMok5kbUtO5K3MMIcbAc5Wx\nqyrvFeeztqyI+lOJ9clBwdwyYiTfHTFyUITdcWQdSmWO+zpk3sMYLL6XgW12OHi78ASfV5bSprgE\ndlRYOMvSR3FNclpAimSFtYO3C07wVXU5Ds31ZT01KoY7M0e7v9iHikanb+v5Tc4Ov/pfu3YtF110\nESNGDKz3psdPz7p161iyZAlms5n77rvvvHP/9oQkSYwI8X9hWDIGET1iBtPzNzC9oxBzbDDBV/yG\n0o4218iyuZEjzY0UtLX2uP5Sbu2g3NrB+iqXfZbFYGB8RJRbLCdFRhNrCUITgt/lfsuXVWVdnt/k\ndPBOUR6762t5acYcQns59jjU2FWVRw58w7dNDV0er7RZ+fPJIxxsque3k2cOqEBq7XVYd54xPDBl\nzsM0ynez5kaHnQf27qD0rA27/LZWfnfkW/LaWnhwTGAdfytsa+Xf9n1Nk7PrOndOUwM53+7m4XGT\nuWVE5pDFE2v27fMUa/a8pt8bmzdvprS0lM2bN1NVVYXZbCYpKYlLL720X5ZlHsVx9erVLFmyxK9g\nL3TMWQtw5m8AwFG8gxBNISM0nIzQcK5LSQegXXFytKWJw02NHGlxiWaLs/vo0q5prj/qToKSHBRM\njNlCbotnc9Djrc38Jf84Px8XONZb7xbldRPGzmyrrebj8mJuHTFwyzYd219EOFzLJJhCCLnsl30S\nsv86nttNGDuzsqSA2bHxzIr17QM32AgheCb3QDdh7Mx/nTjMrNh40nrZYBxI5sWOIUQ206H2vil5\njZ82cX/605/c/3/55ZdJTU3l0ksvBbpaliUlJbFp0yZeeOEFn/oNvHnXeYAp41IwmEBzgqMdpXwv\npvQ5XdqEGk3MjIlnZowrcVwIQWlHe5fNnvy2Fnpavay0Wans5bTOadaWFWFVFYwBkOCsCcH6s0a5\nPfGX/GMUtA3MCSyttRJnqwUSXD6BxqTJyGXVQLVPz7erKhuqy722e/7oQWYHiDg2Ouwcb23utY0q\nBB+XF/PTMROHJKYQo5kfZczlzwWbPLaZGJ7C/LixA/aaH3300eBZlk2ePJnY2O4OvkIIJEli8+bN\n/kc+gAyGZdlA0PqPh1FKvgbAPPFGQhf8e5/76FAUjrU0ucXycHNjryMCHR1fmRwZzasXzxuy1xNC\n8Fbxdv63eAfqWYXoZkRl8MzEm4kyezfEHUo8jhwnTpzIiy++OJSxnFeYsxa4xdFZuBUx/1EkQ99S\noEKMRqbHxDE9xmWIIISgwtrBoeYGnsn9dqBD1rmAGOo1UkmSuCfzMm5Knsbn1YepsDUSKltYED+e\nieHJAbVmexqP4mg2m0lNTR3KWM4rTCMvgy0GEBrC2ohSdRBTyrR+9SlJEqkhoaSGhLKuvISDvazf\nAZgkiSWpGQGR2qMJwScVJdjU3hN9I40mrknp366j1l6HI++f7mtj/ESMKVP73I9NVVlXXuz1cGlq\ncAjz4pP63P9g0GC3808flgImR/pXr6W/xFrCuDP9kmF57b7iURynTJkylHGcdxiCozEmT0WpOAC4\nPB77K46duTUt06s43jpiJA+O9S9vbDCINJl5o+B4r23uGz2BG9O8n1jyhFAVWlc9i3qqlo8hIo2I\nef8PyRjkV382Ve2WEXA2j02Y6h7dBwIVtg5ymz1X/JMlqV/v8YWCxyHFL3/5y6GM47zElLXA/f+B\nrky4KDGFJanpHu9PjozmnlHjBuz1BoLvZ45iVky8x/vfSUzhhl5+Jl+w57znFkaAkMsf9VsYAR4a\nl01WqOecyLszxwSUMAL8v+yLiPWQ8C0Bj06YQkpwYK3vBSJea8j0h2effZacnBwkSWLFihVdRqOV\nlZU8/PDDOJ1OJk6cyNNPP82uXbt46KGHGDNmDABjx47liSee6PU1AnVDBkBrrab5nTM+geFL38aY\nMH7A+hdC8FllKatKCslrawEgwRLEjWkZLEsfhSUAj3k6NY3VpYV8VFZEpdWV9JsREsbS9JEsSc1A\n7sfak9pcSssHd4LqSi43j1tM6Hd+3e+Y2xQnfy/K4x8VJTSeOiM/MSKKZRmjWJSY0u/+B4Nam5W/\nFeXxZWUZHaeOt86KjefOjNEBJ+aByqCJ4+7du3nzzTd5/fXXyc/PZ8WKFaxcudJ9/6GHHuL666/n\nyiuv5De/+Q333nsvpaWl/P3vf+ell17y+XUCWRwBWlb9CLX2KOCyxwq+5F8H53WcDlQhiDSZA9L+\n6WyEEDQ5HRiQiDCZ+r0gL4Sg7ZN/Qylz1VmXgqKI+N4HGIIHrmiTKgTNDgcmg8HryalAQdE0mpwO\nQmRjQJyYKu9o5/PKUiqsHYQaTcyPT2JGTFxA/s0O2ru1c+dOdxGuUaNG0dzcTFtbG2FhYWiaxr59\n+9y74U8++SSAz2ceB5Omdo2CSg2nIogJN5CVbEA2+P+LM2UtcIujo3DLoIljhOnccpeRJMmvc+Se\ncBz/zC2MAMFzHxpQYQTXWl2MZXjOJ/uL0WAgzuL/ssJAoQnBq3lH+aA4v8sG15qyIiZERPH7qRf7\nHae3Gae/lmWDJo51dXVkZ5/ZDIiJiaG2tpawsDAaGhoIDQ3l97//Pbm5ucycOdOdpJmXl8dPfvIT\nmpubefDBB30uwN1f7E7B53ucHC3tupsaHgxXzTAzNtW/Kao5awG2Xa8CoDUWoTYU6uaqA4xmbcS6\n47/c18a0WZjHXjOMEemczduFJzy6Xh1taeKRA7t4Y9ZlmPzMrJg1a5bHGeczzzzDm2++SWJiInfe\neSdXX331wLjyDBSdZ+9CCKqrq7n77rtJTU3lvvvuY/PmzUyYMIEHH3yQa6+9ltLSUu6++27Wr1+P\n2dz7qGjfvn39ik0TsLc8kyZb90XqVit8uN3OtORS4kPb/Oo/LTgJs7UKgKLt79GUqn9wB5L4vLcJ\nt7vWXDWDiYLY61D27x/mqC4MfFnSalOcvFfUszCeJr+thU01FVyVNLAljQfVssxfzj7wXVNTQ3y8\na6cyOjqalJQU0tNdO5Nz5szh5MmTLFiwgMWLFwOQnp5OXFwc1dXVXt02+rvmmFus0JTfm6WSRFFr\nBldf5p9NlVW5Btu+twGIt+cxKoDXSM81nCXf0LZrj/s6dPZ9TJ129TBGpHM2O2qrsflgZPtVVbnf\n4uhpxtkfy7JByw6eO3cuX375JQC5ubkkJCS4a88YjUZGjBhBUVGR+/7IkSNZt24db775JuD6oerr\n60lMHHx7pYOF3n9xjW2CsrqefRq90TmlR609itZa5Vc/Ol0RTisdW55zX8txY7FM/d4wRqTTE429\n1CaZFZoAACAASURBVHzv2s6/o7GZmZk8+OCDvPrqqzz33HP86le/wjEAzvuDNnKcPn062dnZLFu2\nDEmSePLJJ7scBl+xYgWPP/44QgjGjh3LokWL6OjoYPny5WzYsAGn08lTTz3ldUo9EDS1+bZh39Qm\nGOE5Tc8jctw4DOFJblF0FGwhaOrtfe9IpwvWPW+gtVa6LiSDq4qgYfh3ZHW6EuvjRoun3ExvJCYm\nepxxDopl2UCwfPnyLtfjx5/J8cvIyOD999/vcj8sLIzXXnttMEPqEYuPWRkWs3+71pIkYRq5APvB\nDwBwFm7WxbGfKLXHsed84L62TL4NY+LQuMzo9I25cYmEyEZ3vqUnrk72b0q9bt06amtrueeee7rN\nOHXLsn4yLk2muqn3X5xsgIz4/qX0nBZHpTIHraMBQ8jwnG891xGa6qoiKFzLIYawJIJnXxhmzOci\nIUYjPxg5hlfzjnpsMyEiisv8PJ++aNGibjPOf/zjH4NnWXauMBBJ4O02wV8+t2H1skwxY4zMldP8\nS1gWmkrzX69HWF1nXkMW/DuWiTd6eZZOT9hy3u+SuhN23YsuD02dgEUIwduFJ3m78ATqWZIzPTqW\npyfPGLYaN57QR45AaJDEbfMtrNpmp6OXteN9J1XsTrjuYhOGPiaGSwYZ08jL3QXlHQWbdXH0A7Wl\nAuuu193XptFX6MJ4DiBJEj/KGsuS1HS+rCyjwtpBiNHIgoRkJkREnVuWZRcaKbEG7l8cxKEilbwK\nFacCMeES49IMbD2sUNPk+rY7XKTicApunGPGKPftF2rOOiOOStkehL0NqZ+FzC8khBB0bP0DKDYA\nJEs4IfN+McxR6fSFWEsQ38/0nmMYCOji2Ikgs8TFY41cPLbr25IWL7Nqq4Pyelcqz4lyjVXbHNw6\nz4zZ6LtAGlNngjkUHO2gKThLvsY85qoB/RnOZ5x5X6GU7HRfB895EENId7d6HZ2BYPhdUM8Bgs0S\nyxaYyUw883YVVWt8sNmB1eH7kq0kmzBnnLGmdxRsHsgwz2s0Wwsd28840xtTpmGecMMwRqRzvqOL\no4+YjRLfvczM2NQzb1l5vcZ7G+2023wXyC4ej8VfI05NEXV6x7rzZfdmFgaTK6cxAAqH6Zy/6H9d\nfcAoS9x8qZnJmWdMKGqaBe9ssNPc7tvpGVP6JSCf2pVTbDhLdw9GqOcVzvJ9OI5+4r4Omvkj5Gjd\nyfpcpKLDwZsna/ltTjkvHqlib107WoAmzOji2EcMBonrZpmYMeaMQDa2Cd7d6KC+xbtASqZgl0Ce\nwqlPrXtFKPYuRwQN0SMJmnbXMEak4w+aEPz3sWq+tzWfv+bX8c/KFtaWNPLw3hL+9Zsi6my9eRt4\nZ926dSxZsoRbbrmlW2XUr7/+mqVLl3L77bfz5z//2ec+dXH0A0mSuHKaibkTz2zctHQI3t1op7rR\nu0B2mVoXbUd4OTlwIWPb/1e0phL3deiCx5Hkc8NoVucMf82v44Oihh6LlR1ttvHLfaU4Nf9GkI2N\njfz5z3/mvffe47XXXmPDhg1d7j/zzDO8/PLLvP/+++zYsYO8vDyf+tXF0U8kSWL+ZBOLpp4RyA47\n/H2TnbK63o0sTBlz4VSZVmFvQanQ7bV6Qm0owLb/b+5rS/YtGJP7XkVQZ3hpc6q8X1jfa5v8Vjub\nq1r86n/nzp3/v70zj4+qOv//+86aZCYJ2TfIBiEQ9lU2ASmILO5aRcFaae2m3bCtUi24VNCvpVrU\n1lb9fvujVlHEfSlq2XeCBghLEpJA9pCdLJOZuXN+fwxMMmQbs07gvF8vXuTec+65z2yfe5bnPA9T\np07FbDYTHh7Ok08+6SprHrJMo9G4QpZ5ghTHLnLVMD0LJjb1ZBpt8NY2KznFbQukxicAXXTTrh5b\nzvYetbE/IoSDuq1rwOHsVSt+ofhO+WkfWyXpDHvO1WJRO+4VflnUOXHMz8/HYrHw4x//mLvuustN\n/FoLWXbu3DmP2pXi2A2MHazjpql6Lm6asanwzk4rp/LbFkhD4mzX39bs7QjRuXBolyvW9PdQS466\njv2uXiEd5vsplY2eTRtVWTs/vVRVVcWLL77I2rVreeSRR7ol06cUx25ieKyOW2cY0F1Yp1Ed8N4e\nK0dyWv/A9QkzcSbKBFFfhlqS3kuWej+O2lLq973sOtYnzHSbp5X0L0KMnu01CfawXov2Q0IYN24c\nOp2O2NhYTCYTFRXOnO5dCVkmxbEZQgiqytLIOvISp75+jrysd7BZPe/qD4nWcsdMAwbdxfbgkwM2\nDma0FEiNKRRt5EjXcWcdwoVQKSvaRWbaC5z6+k8U5X6C6uW+kw7VSvHZ/5DxzToyvnme0oJtOBxN\n71H9rnXOXUQAej/8rn7IK/feSjxjWrg/ftqOpWZ+dGCn2p8xYwb79u3D4XBQWVlJfX09QUFBgHvI\nMrvdztatWz3OSyWj8lygseEcR/eupLr8iNt5jdZI0uifM3DIbR63VVThYOP2RrcoP1eP1DE9Ref2\nI7d88wYNe9Y77xMwkIC73/lWIlBbfZqjex6mvvas23mdIYCUSY8RFj3T47Z6i4rSQ6Tv/wNWi/sE\nvY9fFKOmPo1P1TnqPv+d67zv1Q/hM8rz917infw7u5y/ZZS2WT480IeXropH18lMn2+99RabNm0C\n4Cc/+QnV1dWukGUHDx50xXC89tprWb58uUdtSnEEVLuFg1/dS11NTpt1Uib9gaj4RR63WVbt4M3t\njdQ2NJ2bnKxjzpgmgVSrC6h541ZXuf93N6ALTfKofUtDKQe+uAdbY2Wr5YqiZdysFwkKG++xzT1N\nTeUJUv/7IxyO1kMf6XQmks8bMdY6X5M2YiT+N7+Coulc5keJ9yCE4J+ny/jn6TIuXZsZF+zH42Nj\nGGDwrlAP3mVNH1F05tN2hREg6+jLhMbMRuPhDzXIDHfNhrd3QNWFEeKBU3YaGu3Mn4Azibk5BCV0\nCGpZJgCWrC/xDWo/mdhFzpzc0KYwgnO4ffroXxk3a71H7fUG2cf+3qYwAtjtdRRTRxwKaLROn0Yp\njJcFiqJw75Awrh8UxJbCagrrrfjptMyK8Gd4oI9XTpvIniNw6L8/bDGc7i6sIphT4k800BR9OJj/\nkqg8jUaRzt+XohEwpgx8x9+L75Sf9LU5kisYuSADNFrKOq7USQxKBcOUX2DiuOtcBXPIFE+hCu+K\nfOwNOBQQgTH4TLyvr02RXOFIcQQMxqAebV+v1DBMWYE/TTthqplChngWuzD16L37HQK0k76PopMP\nDknfIuccgYjYa6mpaN/PUGcIZNKc19B0YV/vNBU+SbWRXeJs4zxjyA/4kAU+r6E9+W8ANKHJ+C98\ntsO2so7+jZKzn7VbxxyYxJgZnmVa6w2O7f8D1WVp7VdS4PCRNQysP01Cyn3oDQG9Y5xEcglyzhGw\n22rZv2UplvqiNusMHftrBiV1PZ2q6hB8vN/G8bNNu2dC/KwsKLgbs8M5vA/83kdoTO0nyG6oLWD/\nF8tQ7XVt1FAYM+NPhEZ55tPVG1QW7eXwrl/SIGIpFTdQxzBA4M9RwpUPMSrFbvX1hkASRvyQmMSb\n0ch81JcFlbUOjuaoVNUJjHoYGqMlPkIjF2R6gu7yc6yvzSNt10PUn8+9pERDQspyElKWd9sHKITg\nP6k2vj7dJJD+jlIWV/+KQEchvjMfwmdkx7591eVHObL7t1gbK9wt1hhJnvBbouMXd4u93UX93hfZ\nntHANz4/5dIZHQUbo63P4W/ahVV1F3w//3iGjv0lIZFTe9FaSXcihGBrmp39p1ouQkYFK9w2w4jZ\nt3O/r3feeYcPP/zQdXzs2DG+/vpr1/GePXtYt24dWq2WmTNn8rOf/cyjdqU4NsPhsFNetJtzhTtx\nqBb8/OOIil+MrymqW9pvjhCCbUfs7DvZ9GXxc5SzuOYhwqNC8b/hRY/aUVULpXlfUVF6COGw4x80\njKj4RRiMA7rd5q4gVBvfbHicz31/32YdRahcrz6Fz5hEzmb8C4fq7vYTEjmVIWN+jjkgsafNlXQz\nO4/Z2JXetndGeKDCvfOMaL9l0rpLOXDgAJ999hmrVq1ynVu4cCGvvfYaERERLF26lCeeeIIhQzpO\n8iXHKs3QaHSExcwiLGZWj99LURSuGaPHxwDbjji/NPWaED4IeIGFxY8w1FKNxqfj7VRarQ9R8Yu+\nlYN6X+A4X8Rh3S3t1hGKljTHPO5MmkxM4o2cPvoyxWc/d5WXF++louQAMYk3kzDih173AJC0jsUq\n2H+yfbe10mrByXyVEXFdk6SXXnrJtRsG3EOWAa6QZZ6Io1yt7mOmDtczf0KzkGeaAD4O+B9OHzvW\nh1Z1P+cbdZTqh3dY76z+KlShxccvghFXPc7E77xOYMgoV7kQKvmnN7H3s1s5m/EmDkfXIkhLep6s\nQhVb+yFOAUg/40Gldjhy5AhRUVGEhTXN18uQZf2c8UN0XH+VHgVn2DKb4sd7p0eRWdC1L4s3UGcR\npGXb+fR4cMeVcfYebYqv6zgweAQTrvkHI6c8hY9fpOu83VZLZtrz7PvPEs4VbO+WEFWSnqG+0bPP\nxtN6bbFp0yZuvvnmLrXRHCmOXsLIeB03DM1GK5zRKlT0vLu7kfQz/WsXjRCCkkoHu9Nt/PMLC3/5\nwMKnB22cKfX0iy/44rCV8vNN8S0VRSFi0DymXLeRwSN/glbbJJ4NtXkc2fNbvt7xAOerMrv51Ui6\nA5OPZ/OIZg/rtcX+/fsZN26c2zkZsuwyYfiooSysX4VOOKNVCKHw4T4bh7O8WyDtquB0kcp/Uq28\n/HEjr29pZMcxO4UVnekJKBw/6+AfnzXy0X4rFc1EUqv1IX74vUxdsImo+Ou5GA8ToLL0EAe+uIcT\nh56m0dJ+SH5J75IUrcWTmBIj4zs/31hSUoLJZMJgMLid70rIMrkg40UoOiPxMSauP7OCT/3X0qhx\nOkD/J9VGo00wdbj3JJaqbXAKYlahSk6JA1s7+u1HDbGWHYTZT7Hf7wdYNa0vNCmKMwYmOP8/lquS\nfkZlZJyW6SN0BJmdz3Kjbygpkx5l0JDbyUh7nqpzF3ceOSjM+YCSvC+IH/59BiXdgVYrd9r0NQa9\nwvQUHVuPtP0liQpWSIrpfF/t0rnFzZs3u0KWrV69mhUrVgDOleuEhIS2mnFDuvJ4GdbML6j74jHK\ntQl8EvAc9ZqmD3zqcB2zRun6xGFWCEFplSCrUCWzUKWog15hxACFwZGCmNyXCC54F+VC3rma+Dv5\nSvsjSqrc6w8K07Bgoo78MsHu43aq69zbVxQYHa9l2ggdA0xNPyIhBOcKtpF1ZD0NdQVu1/iYohky\n+gHCY+Z4pZPxlYQQzs91d7qdS5MMxoVruGmaAT+jd31GUhwvIeN8CXsqsmhQbcT6BTMnbBi+WkPH\nF3YTwlpH1evXgcNGtSaaTyJep8ba1PsZN1jL/Al6149dCMHX1Wc5XHkGVTgY6h/J1SFJ6Loh1Jdd\nFZwpdZBZoHK6yEFNfdtfFa0G4iM0DInWMiRai7/RRt3nj2A7s9tVR584G9O8p1AVhS05+WSWNqAo\nMCLKxDWxA51h3ABVFRzNVdl93N7inhoFRidomZaiI7CZSDpUK3lZb5Nz/PUWu4YGhI4laewvCQjq\neLW8LRpVO9vKTpFTdw6DRsdVwYmk+Ed5tejW2i18VXqCQks1Jq2BmaFDiTeF9q1NDYJjZ+xU1QqM\neoXkgVqighWvfB+lOF6gylrPqhMfcKDSPa6jWWvkV0nXsjByVBtXdj+1n6xwiUrj0Lv40Ho/5eeb\nPqYRcVoWTdZTaKnk98ffI7O2xO36UIOZx4Zdz+Rgz4YPbve+MFzOLFTJLXa064Jh8nGmhkiK1hIX\nocGguyDYqo26Lb/HlrPDVVcffzWm+U9zrLaEVSc+oMhS7dZWvF8IT6bczBBz02S5qgrSclT2HLdz\nvuESkdTAmAsiGeDXJJJWSwXZ6f+gIPt9oHnSMoWouIUMHvUTjL7tb828lG3nTrL21GdU2xvczo8J\nHMgTKTcTbvT/Vu31BhvzD/C37O1YLnF1uiY0md8PW4xJBvboECmOOHsF93/9TzIuEZnmPJlyE3PD\nU7p0H4/tOfER9Vv/CIDiMwD9ko95e6ed4sqmjyouUrBR93+U2qpabUOvaHl57FJGBsa0e6+Lw+XM\nQuf8oSfD5SExTkGMDGr5xBeqnbovHsXWLCeOPm46puvWcLqhivsP/5OGNnwTA3W+/O/E+4i6xPnd\nrgrSslX2nLC5RVYHZ491TKKWqcP1BPg12VJbnUVm2gtUlBxwq6/R+hA/7B5ih96NVufT7msF2Ft+\nmoeOvo2j1XT0EOsbzOsTvu9VYrOp4BB/ytzSZvmEAXG8MGYJWkWux7aHdvXq1av72oiuUFRURHR0\ndJfa+KzkKO8Xfd1unRM1Rdw+cKJr6NeTaMwRNKb9GxBgt+AbN5ERIwZSUN40tK2uVTA2hFBkzEIv\njIQ1xuFvD0WgYtM04kBQaKliYeToFu3bVUFOiYMDp+xsOezcwni21NFCeAB0WkiI1DA5Wcd1Ew1M\nGa4nLlyLv28rwuiwU/flKmzZW5uuHzQF83VrUXRG/ifjc7Lq2s4j0uiwY3XYmR7inipCo1GIDtEw\nYYgOk1GhtMrBxSyeQkBRheBwlp26RkH4AA1GvYLBJ5jI2AUEBA/nfOUpbNbqC/XtVJ5LpejMpxh8\nQjAFDG5zSCeE4OH0d6mwtRXcA6rtDQzQ+zEqcGCbdXoTi2rjoaPvYHW0vfhRZKlmuH8UsX4hvWhZ\n/0OuVgOfFnccBby4sZpdZZmMHeBZGoMuoTNQHz0ee7EzvJclext+4cO4bir8Zz+cLXH+mMNsscwt\nuw+9MKKhaY6x1HCGo/5fkVp1hszzJYT7+FNvgdxiOFMEeaVgV9sWeT+jIC4K4iNhYDjodSqgIoDq\nNjakCIeD+h3PYsvdARfCuumix2Gau4oaoVJbX8mOslMdvvTPi4+xPG5mm4mWkhIgIRbSc+DwKWho\nvDBP6YDUTJVvTtsZkQDjk8HPB/Sh4xl2zWsU5XxE7qkNqLbzANRbKjl44En8M99h8MifEBDccj4y\nq7aU7LqOd1N8VJTWq9Mu7bH9XAbnPcg++WnJUWZ4mK/oSkUOq4Fb971MoaX14am3oQgN46sXENM4\ntN16jUodXwduYYAtgojGRILske3Wr9KVUGLModiYTbWupLkLodeiFTri6keTVD8Ro8M9aLCKnVy/\nNLL8DtGore8jC72XEf7RvDrh3l6/r7XKQeVxFWu1QGOEwMFaTLHeGbJM9hyBQL1vvxFHoThIDfwU\nvwp/guytRwsSCIzCxJSqtrdSqdg5ZzhLiTGbEmMOFm1tT5ncY6iKnWzTYc74HSG+fgxD6iZiFH4A\naNExuH4CcfWjnSJpOoRV08q8wRVKgN6340rdiBCC4p12ylLdh/sVaSq+kQpx1xvRmzsnkHV1dfzu\nd7+juroam83Gz372M66++mpXeWdDlklxBOaEDefE+bYD3XodikDvaHsBQGmj22fR1FJszKHEkE2Z\n8SzqZZLgS1XsnDalkut7hISGMQyum4hROH/8OvQMqZ9IfMNocnzTOG06hFXT8bDzcuc74Z13a+oM\npftaCuNFGooFue81MvguI5pOhCx77733SEhIYMWKFZSUlPC9732Pzz9viub01FNPuYUsmz9/vgxZ\n5ik3RI1hY/4Byqxt956Wx83g9oETe9EqsKRtxJL6OgCagEEE3PYqAA4Bf3vPsy/RALNgyECIj4Kw\nASYUZSQwssu2CSFo2PMi1lMfu85pw4Zhnr8GxeDX6jWHK8+y8vjmNtvUoeHPY+4gyRzRJdusNjh6\nWvBNJjTanO+TThhIqp9EinUioxJhbBL4GMHhUCk+8zm5J/8Pu9Xdvcg8IInsmMW8WtL2XKm/1oe/\njltKqNHcJZu7k79n72Bz0eE2y2N9g5kb1jueFwBqo+DcofYfxJYyQU2myoBh316SgoKCOHXK+RnV\n1NQQFNSUE6orIct6VByffvpp0tLSUBSFlStXMnp008ppUVERv/71r7HZbKSkpPDEE090eE1PEaD3\n5YUxS1hx5G2KG6tblN816CqWx1/d6/Mi5iHfoebAK86DymzM50vQBicghECrsaA62r8e4NbpPoQP\n6F6XDSEEDbv+jDj+HhedYbRhw/Ff9GeUdkTimvBh/MFxPWtPfYpVuDtQ+mkNrB5+IxODvr1vZgv0\nMGcUTB8mOJRh58ApO5YLC0k2u8LhDDiWDROH6pic7MvwobczJP46ck/8L3mZGxHC+UN2VB4nrvI4\nc80j+conEXHJ529WLfwmMIDBZs8CGfQWP44eTt6ZzzhoaPmQCbOf55HgZIza3usX1WSrCA8GKVUn\nOyeOixYtYvPmzcybN4+amhpeeeUVV1lrIcvy8vI8arfH3qEDBw5w5swZNm7cyOnTp1m5ciUbN250\nla9du5b77ruPefPm8fjjj1NYWEh+fn671/QkiaYw3pr8I7aeO8mu8kwaVBtxfsHcEDW2z3YVaAfE\noglOxFGRDYA1exu+wQkoisLgKA0ZBe2rY4CfQmhA9wq6s8f4FxqPvt1kZ+hQzNe/0K4wXmRB5Cgm\nByXwUXEax2sKURSFsYGDWBg5msBungcz6hWmj9AzIUnHwQw7BzPsNF4QSasd9hy3k5ppd4rkUDNJ\nY35OzOCbyTryIucKtgHOdanZtccYVZ/DId84SrX+6FBJspYy2lKApkKlImYKweG9O6poj1MHn+TG\n6qNM0gVy2CeWCo0fPsJOSmMhw63FVFRtpyF6Kr6mrrnAeYq9nZ1VbvUaOrc2/MEHHxAdHc1rr73G\nyZMnWblyJZs3tz1C8ZQeE8e9e/cyd+5cAAYPHkx1dTW1tbWYzWYcDgepqamsW7cOwBXS/J133mnz\nmt7AqNVxXeRIrovs+rCzuzAkzsZyQRxtOdvwnfh9ACYn68gosLZ77eRkHZo2XGI6gxCChn0v0Zj2\npuucNiQJ8w3r0fh4niUwxGjm3rjeS/zlY1C4eqSeSUN1HMiwc6iZSDbaYHe689zkZB0TkwYyetoz\nVJamcurrP1FXc9pps6OO+XXHW23/ZOoawqJn9tbLaZdGSznV5UcBiLZXE117tEUdIVQKTr/HkNGe\nLUx0Fb3Js++gzq9z39XDhw8zY8YMAIYNG0ZpaSmqqqLVarsUsqzHxLGsrIwRI0a4ji9G4DWbzVRU\nVGAymVizZg3p6elMnDiRFStWtHtNe6SmpvbUy+hzDJZwLroXq+dOkbZnC3aj03l3eFgQJ85F0prf\nTUxAJUpNEd321ghBUP7HBBU2TXRbfaMpjFuOIz2rm27S85iAaQM15FaFcLYqGFU4/UMbbbDzmJ29\nxxuJH1BO7AANwvdaqPmr61qH0GMlFA1W9JRzcZTdUJvP2Yx/98Gr6RhV+GAjGC116JWmKaP8M7up\ntk3pcvueuNH5J2rRGGw42n+WEzS8c3IUFxdHWloa8+fPp6CgAJPJhFbr/FybhyyLjIxk69atbmkU\n2qPXJh6au1MKISgpKeGee+4hJiaG+++/n23btrV7TXtcTlF5LkWI8dSc+SeOC6vpyaYKfMZcC8AE\nYGqlg9QsO2dKHDgERAYpjBuiIyEiCkXpvmFTw8FXsTQTRk1QPGE3vkyEn2cRvr2NKTgjTx84ZedQ\npt0Vcs3u0JJVEU5+bTijov1xiP/DgZ5CcQ9lXIeK80HtSzaRvE0on+OFLno0ikjyxb1UMAeBM3CK\nWRwlWvkXA5T9mE2mXvvdaA0K4ZN1FO9qe+LRN1IhYHDn5sbvuOMOVq5cydKlS7Hb7axevbpbQpb1\nmDhe2p0tLS115XYICgoiOjqa2NhYAKZOnUpmZma711ypKIqCPnG2ayhry96Gz5g7XeURQRoWTurZ\nqEENh/4Xy8FXXceaAXH43/gSmn4qjBfxMyrMHq1n8lAd+0855x8vBtqwWOFgbgxaNgIOVNyTeTWQ\nSI54mHoGM9TvXUKiut4L6w6slkryCnI5IV7AfonNtYwiQzxDHM8TG9K6R0FPETpRh0OF0v1293gg\ngGmQhthFBpROZh40mUy88MILbZZPmjSpU2sXPSaO06dPZ/369dx5552kp6cTHh7uGh7rdDoGDRpE\nbm4u8fHxpKens2jRIoKDg9u85krGkDjLJY72ojQc9eVoemlfrOXw/8NyoGn1TxM4EP8bX+y1+/cG\nfj7OTJCTk3XsP2knNcuO/YJIqrQ/l1rC7UxIHM/wFO+Yp3Y4HOzcnIFdbTsz4xnxILMjW3pl9CSK\nohAxRU/wKB1VJ+zOHTIGhcAkLb4R3hmyrMfEcfz48YwYMYI777wTRVFYtWqVW1d35cqVPPzwwwgh\nGDp0KHPmzEGj0bS4RgLaiFEovsGIhgpAYMvdiTHlph6/r+Wbf9Ow72XXsSYgBv8bX0Zjujx78yYf\nhTljnSK576SNw1mqR+5Su04PprCugwm1XqKuUXBejeuglpYTReFEd82dtFPoTQphE70non17yL3V\n/YS6bc9gPf4eALrYKfgvfr5H72c5spGGXX92HWv8IzHf9Fe0/q1vWbwcOZpr5+P9l2fq14GhGpZ9\nx3vCrHkjMqBbP8GQONv1tz3/EKKx5/ZCW45tchNGxRyB+caXryhhBLwubL+kd5HbB/sJupjxKAYz\nwloLDju2M7sxDJ3f7fdpTH+fhh1Nrg6KKQz/G19CG9A7DsPeRFSQBo0GHB0MrYPMkBTtHT+lOosg\n/WzH+c4Hhsp+UUd4xycq6RBFq0cfPwNrhtOdxpq9rdvFsfHER9RvX9t0T79QpzB6SSDX3sbPRyEl\nVsux3PbFZsFEA3ERXc/Z011U1TVSUN62omsUZy6ivsBRaUX9phJRaUUxatAMD0STYLqyFmQk3Y8+\ncbZLHG1n9yJsFhR9x6H+PaHx5CfUb33adaz4BuN/44toB8R2S/v9lblj9ZRUOjhX3frU/LQUAWFN\nxgAADLRJREFUnVcJI8Diq/S8sbWx1cjuAAsm6Rlg7t2eoxAC+xfFqHvL3M6rhypQYnwx3BGH4t+5\nhRqHw8GqVavIzMxEr9ezevVqBg8e7CrvbMgy2bfuR+gHTYGLuUrsFmx5+7ul3caMz6n/71NwIU+K\n4hvkFMag+G5pvz/ja1RY9h0j01J0+DVbv4gOUbh5moFZo7xv5TXYX8O9c30YP0SLoVn3JzFSw13X\nGBid0Pt9InV7aQthvIgoaMD6Ri7CE9eAVvjqq684f/48b731Fn/84x959tln3cqfeuop1q9fz5tv\nvsnu3bvJyvJsR5fsOfYjFL0P+kFTsOVsB8CWsx1D4qwutWnN/IL6r57AJYw+gfjfsB5tcGJXzb1s\nMOoVZo3Sc/UIHQ1WZ1IvH4P3DQOb4++nMH+CgbnjBA2NYNDjyg7Z2wiLin1P++kmRIkFx/EatKPa\n9s9si9zcXFf0rtjYWAoLC117q7sSskz2HPsZ+sTZrr9tuTsRaucD1lpP/5e6L1eDcD6xFWMA5hvW\now3p+ItzJaLRKJh8FK8XxuZoNQpmX6XPhBHAkVEDto49BtWjnYvGP3ToUHbt2oWqqmRnZ5OXl0dl\nZSXQesiyc+c6zgsEsufY79DHzwCNFhwqovE89sJU9IOu+tbtWHN2UPfFY3AhrqJiMGO+/i/oQtvP\nTSORfFtEnWcPcFHfuQf9rFmzOHz4MHfffTfJyckkJiZ6HJehPaQ49jM0Rn90MZOw5+0DwJa9/VuL\nozV3F3X/WQmOC6uwBhPm619AFz6su82VSFDMns3LKubOy9GvfvUr199z584lJMS5vbUrIcvksLof\n0nye0ZqzHSE8n8i2nd1L3eePwMW8xno//Bc/jy5iRPsXSiSdRJPsD4aOpUY7OqjDOq1x8uRJHnnk\nEQB27NhBSkoKGo3zfs1DltntdrZu3cr06Z7FEpU9x36IPmEmbH8WEIj6ctTiY+iiOk4nYcvbT+1n\nvwPHhS1xOl/8F69D5yU5lyWXJ4pBi25mOPYvi9uuE+OLJtnzgMnNGTp0KEIIbrvtNoxGI88991y3\nhCyTe6v7Keff+xH2ojQAjGPuwm/6z9utb8s/RO0nK0BtdJ7QGTEv+jP6mPE9bapEghACdUcp9h2l\nLUKWaeJN6G+PRfHzrr6ad1kj8Rh9wiyXONpytiOmPdjmLgNbwWFqP32oSRi1RswL/ySFUdJrKIqC\nblYE2gnBqGlViEorGDVoUwJRon3lDhlJ96FPnE3Dnr8A4KgpQC3PQhea1KKevegbZ4/RfiFXs9aA\neeGz6Hs5zaxEAs7FGd30/hHyTi7I9FO0AdFom7nd2LK3tahjLz7K+Y9/DfYL+8g0eszXre2U649E\ncqUhxbEf4+YQfok42kvSOf/xL8FW7zyh0WG6bg36uGm9Zp9E0p+R4tiPaR7jUa04Te3WP2LN3IKt\n+Ci1H/8SrHXOQo0W07V/xBA/o0/slEj6I3LOsR8jUECjd7nm2E58hO3ERzhTtV5wQlC0mOY92eU9\n2BLJlYbsOfZTHHVl1H74QJPPohsXvbMUTHNXYxg8pxctk0guD6Q49lMsaW8i6svbraMZEIchaV4v\nWSSRXF5IceynWE991mEdR1UujvNt70qQSCRtI8WxHyIc6oU0rR3jqGs9wKhEImkfKY79EEWjRTGY\nPavr07n9qhLJlY4Ux36KfkjHc4nasGQ0gYN6wRqJ5PJDimM/xWfsEtD7tV9n4nKv3LMqkfQHpDj2\nU7QDYjEvWofi00rODY0ev9kPY0iY2fuGSSSXCdIJvB+jjx5L4LLNWDO2YCtIBYcdbVgyxuHXo/EL\n6WvzJJJ+jRTHfo6i98M44iaMI27qa1MkkssKOayWSCSSVpDiKJFIJK0gxVEikUhaQYqjRCKRtIIU\nR4lEImkFKY4SiUTSClIcJRKJpBUuCz/H1NTUvjZBIrliuVzzxitCCNFxNYlEIrmykMNqiUQiaQUp\njhKJRNIKUhwlEomkFaQ4SiQSSStIcZRIJJJWkOIokUgkrSDFEcjIyGDu3Ln861//alG2Z88ebrvt\nNu644w5eeumlPrCudZ599lnuuOMObr31VrZs2eJW5m02NzQ08Itf/IKlS5dy++23s3XrVrdyb7O3\nORaLhblz57J582a3895o8/79+5kyZQrLli1j2bJlPPnkk27l3mizVyOucOrq6sTSpUvFo48+KjZs\n2NCifMGCBaKwsFCoqiqWLFkiMjMz+8BKd/bu3St+8IMfCCGEqKioELNmzXIr9zabP/nkE/H3v/9d\nCCFEfn6+uPbaa93Kvc3e5qxbt07ccsst4t1333U7740279u3Tzz44INtlnujzd7MFd9zNBgM/OMf\n/yA8PLxFWV5eHoGBgURFRaHRaJg1axZ79+7tAyvdmTRpEi+88AIAAQEBNDQ0oKoq4J02L1y4kB/+\n8IcAFBUVERER4SrzRnsvcvr0abKyspg9e7bbeW+2uS36o819zRUvjjqdDh8fn1bLzp07R3BwsOs4\nODiYc+fO9ZZpbaLVavHzc2Ye3LRpEzNnzkSr1QLeazPAnXfeyUMPPcTKlStd57zZ3meeeYaHH364\nxXlvtjkrK4sf//jHLFmyhN27d7vOe7PN3splsbf6SuXLL79k06ZNvP76631tike89dZbnDhxgt/8\n5jd8+OGHXp029v3332fs2LEMGtR/8n7Hx8fzwAMPsGDBAvLy8rjnnnvYsmULBoOhr03rl0hxbIfw\n8HDKyspcxyUlJa0Ov/uCnTt38re//Y1XX30Vf39/13lvtPnYsWOEhIQQFRXF8OHDUVWViooKQkJC\nvNJegG3btpGXl8e2bdsoLi7GYDAQGRnJtGnTvNbmiIgIFi5cCEBsbCyhoaGUlJQwaNAgr7XZm7ni\nh9XtMXDgQGpra8nPz8dut7N161amT5/e12Zx/vx5nn32WV555RUGDHDPW+2NNh86dMjVuy0rK6O+\nvp6goCDAO+0FeP7553n33Xd5++23uf322/npT3/KtGnTAO+1+cMPP+S1114DnMPo8vJy1/yut9rs\nzVzxUXmOHTvGM888Q0FBATqdjoiICObMmcPAgQOZN28eBw8e5LnnngPg2muvZfny5X1sMWzcuJH1\n69eTkJDgOnfVVVeRnJzslTZbLBZ+//vfU1RUhMVi4YEHHqCqqgp/f3+vtPdS1q9fT0xMDIBX21xb\nW8tDDz1ETU0NNpuNBx54gPLycq+22Zu54sVRIpFIWkMOqyUSiaQVpDhKJBJJK0hxlEgkklaQ4iiR\nSCStIMVRIpFIWkGKo8RFfn4+I0eOdEV1ufjv1Vdf7bZ77N+/nyVLlnRYLzk5mb/+9a9u55YtW0Z+\nfn632SKRtIfcISNxIzg4mA0bNvS1GYSEhPD+++9z0003ERUV1dfmSK5AZM9R4jEpKSm8+OKLLFu2\njFtuuYWMjAwA0tLSWLJkCcuWLeOee+4hKysLgNzcXJYtW8bdd9/NfffdR0lJCQAOh4NVq1bx3e9+\nl6VLl1JXV9fiXj4+Pjz44IOsXbu2Rdmlvc+HH36Yd955h/z8fBYtWsTTTz/NLbfcwvLly/nggw+4\n7777mD9/PidPnuyJt0VymSLFUeIxqqqSlJTEhg0bWLJkCX/5y18A+O1vf8sjjzzChg0b+P73v8/j\njz8OwKpVq1i+fDlvvPEGt956K5999hngDAX24IMP8vbbb6PT6di1a1er91u8eDHl5eXfKrRWTk4O\nS5YsYfPmzeTk5JCXl8frr7/O4sWLeffdd7v4DkiuJKQ4StyoqKhoMed45MgRV/mMGTMAGD9+PFlZ\nWdTU1FBeXs7o0aMBmDx5MseOHQPgyJEjTJ48GYBFixZx7733ApCYmEhoaCgAkZGR1NTUtGnPo48+\nypo1a7Db7R7ZHxQU5NpWGRERwfjx4133qa2t9fRtkEjknKPEnY7mHJvvNlUUpUXYsUt3ozocjhZt\nXIw96QnDhg1j0qRJbiksLr2nzWZrs+3mx3KnrOTbIHuOkm/Fvn37AEhNTSU5ORl/f3/CwsJIS0sD\nYO/evYwdOxZw9i537twJwMcff8y6des6dc9f/OIXvPHGG5SXlwNgNpspKSlBCEFDQ4Pr3hJJdyJ7\njhI3Lg6rmzNw4EDWrFkDwPHjx3nzzTeprq7mmWeeAZwRs9euXYtWq0Wj0bB69WoAHnvsMR577DHe\neOMNdDoda9as4ezZs9/apoCAAO6//34effRRwNmbTE5O5uabbyY2NpZx48Z14RVLJK0jo/JIPCY5\nOZn09HR0OvlMlVz+yGG1RCKRtILsOUokEkkryJ6jRCKRtIIUR4lEImkFKY4SiUTSClIcJRKJpBWk\nOEokEkkr/H8qS+BjEM43XAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e4440cb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "sns.factorplot(x='Epoch Num', y='Test Acc', hue='Fold Num', data=fold_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch Num          5.000000\n",
       "Fold Num           4.500000\n",
       "Test Acc           0.680784\n",
       "Test Loss          0.895922\n",
       "Training Acc       0.999002\n",
       "Training Loss      0.211877\n",
       "Validation Acc     0.645043\n",
       "Validation Loss    0.994096\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_results[fold_results['Epoch Num']==5].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
