{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TF net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Start working on fold(s) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Beginning fold 1 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/0\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params: 185650\n",
      "2017-08-20 15:29:15.141713: step 0, loss = 0.69 (26.4 examples/sec; 2.421 sec/batch)\n",
      "2017-08-20 15:30:00.739742: step 100, loss = 0.69 (160.4 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 15:30:42.964705: step 200, loss = 0.67 (164.4 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 15:31:29.863257: step 300, loss = 0.68 (162.4 examples/sec; 0.394 sec/batch)\n",
      "Epoch 1 of 10 took 168.975s\n",
      "  training loss:\t\t0.680416\n",
      "  training accuracy:\t\t56.93 %\n",
      "  validation loss:\t\t0.640002\n",
      "  validation accuracy:\t\t67.95 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.670807\n",
      "  test accuracy:\t\t62.00 %\n",
      "2017-08-20 15:32:29.935195: step 400, loss = 0.64 (162.5 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 15:33:16.532635: step 500, loss = 0.60 (160.2 examples/sec; 0.400 sec/batch)\n",
      "2017-08-20 15:34:03.609891: step 600, loss = 0.62 (161.9 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 15:34:52.646531: step 700, loss = 0.69 (164.5 examples/sec; 0.389 sec/batch)\n",
      "Epoch 2 of 10 took 173.862s\n",
      "  training loss:\t\t0.638097\n",
      "  training accuracy:\t\t67.64 %\n",
      "  validation loss:\t\t0.667709\n",
      "  validation accuracy:\t\t62.35 %\n",
      "2017-08-20 15:35:54.309709: step 800, loss = 0.62 (162.6 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 15:36:52.399462: step 900, loss = 0.51 (157.1 examples/sec; 0.407 sec/batch)\n",
      "2017-08-20 15:37:56.475168: step 1000, loss = 0.45 (163.5 examples/sec; 0.391 sec/batch)\n",
      "Epoch 3 of 10 took 207.911s\n",
      "  training loss:\t\t0.560044\n",
      "  training accuracy:\t\t76.66 %\n",
      "  validation loss:\t\t0.849926\n",
      "  validation accuracy:\t\t52.17 %\n",
      "2017-08-20 15:38:57.876093: step 1100, loss = 0.43 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 15:39:50.392700: step 1200, loss = 0.52 (164.3 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 15:40:46.755297: step 1300, loss = 0.41 (167.4 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 15:41:41.859190: step 1400, loss = 0.40 (162.6 examples/sec; 0.394 sec/batch)\n",
      "Epoch 4 of 10 took 202.566s\n",
      "  training loss:\t\t0.425809\n",
      "  training accuracy:\t\t88.00 %\n",
      "  validation loss:\t\t0.928726\n",
      "  validation accuracy:\t\t50.76 %\n",
      "2017-08-20 15:42:47.473143: step 1500, loss = 0.34 (161.1 examples/sec; 0.397 sec/batch)\n",
      "2017-08-20 15:43:45.998655: step 1600, loss = 0.37 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 15:44:44.151033: step 1700, loss = 0.40 (163.0 examples/sec; 0.393 sec/batch)\n",
      "Epoch 5 of 10 took 208.344s\n",
      "  training loss:\t\t0.391405\n",
      "  training accuracy:\t\t90.16 %\n",
      "  validation loss:\t\t1.033775\n",
      "  validation accuracy:\t\t49.50 %\n",
      "2017-08-20 15:45:47.671202: step 1800, loss = 0.36 (160.9 examples/sec; 0.398 sec/batch)\n",
      "2017-08-20 15:46:45.031773: step 1900, loss = 0.31 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 15:47:37.750676: step 2000, loss = 0.34 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 15:48:32.695286: step 2100, loss = 0.33 (161.7 examples/sec; 0.396 sec/batch)\n",
      "Epoch 6 of 10 took 205.945s\n",
      "  training loss:\t\t0.358821\n",
      "  training accuracy:\t\t91.96 %\n",
      "  validation loss:\t\t1.091620\n",
      "  validation accuracy:\t\t48.78 %\n",
      "2017-08-20 15:49:38.842787: step 2200, loss = 0.36 (166.0 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 15:50:32.157752: step 2300, loss = 0.28 (162.6 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 15:51:30.925277: step 2400, loss = 0.28 (159.5 examples/sec; 0.401 sec/batch)\n",
      "Epoch 7 of 10 took 208.583s\n",
      "  training loss:\t\t0.341140\n",
      "  training accuracy:\t\t93.19 %\n",
      "  validation loss:\t\t1.084347\n",
      "  validation accuracy:\t\t49.28 %\n",
      "2017-08-20 15:52:41.951759: step 2500, loss = 0.33 (163.5 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 15:53:40.030078: step 2600, loss = 0.31 (162.7 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 15:54:36.148282: step 2700, loss = 0.36 (161.8 examples/sec; 0.396 sec/batch)\n",
      "2017-08-20 15:55:32.149311: step 2800, loss = 0.30 (164.4 examples/sec; 0.389 sec/batch)\n",
      "Epoch 8 of 10 took 210.812s\n",
      "  training loss:\t\t0.339005\n",
      "  training accuracy:\t\t93.25 %\n",
      "  validation loss:\t\t1.113590\n",
      "  validation accuracy:\t\t48.48 %\n",
      "2017-08-20 15:56:42.714689: step 2900, loss = 0.32 (165.5 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 15:57:37.676115: step 3000, loss = 0.30 (161.5 examples/sec; 0.396 sec/batch)\n",
      "2017-08-20 15:58:34.770174: step 3100, loss = 0.30 (160.5 examples/sec; 0.399 sec/batch)\n",
      "Epoch 9 of 10 took 212.534s\n",
      "  training loss:\t\t0.336326\n",
      "  training accuracy:\t\t93.45 %\n",
      "  validation loss:\t\t1.101535\n",
      "  validation accuracy:\t\t48.74 %\n",
      "2017-08-20 15:59:43.298981: step 3200, loss = 0.30 (162.2 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 16:00:39.096717: step 3300, loss = 0.32 (159.7 examples/sec; 0.401 sec/batch)\n",
      "2017-08-20 16:01:36.608838: step 3400, loss = 0.29 (164.8 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 16:02:31.388895: step 3500, loss = 0.43 (162.8 examples/sec; 0.393 sec/batch)\n",
      "Epoch 10 of 10 took 208.206s\n",
      "  training loss:\t\t0.334178\n",
      "  training accuracy:\t\t93.67 %\n",
      "  validation loss:\t\t1.114612\n",
      "  validation accuracy:\t\t48.82 %\n",
      "Beginning fold 2 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/1\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 16:03:40.600021: step 0, loss = 0.69 (45.1 examples/sec; 1.420 sec/batch)\n",
      "2017-08-20 16:04:23.235531: step 100, loss = 0.69 (168.1 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 16:05:04.419077: step 200, loss = 0.65 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 16:05:48.882968: step 300, loss = 0.65 (163.7 examples/sec; 0.391 sec/batch)\n",
      "Epoch 1 of 10 took 159.493s\n",
      "  training loss:\t\t0.669659\n",
      "  training accuracy:\t\t60.56 %\n",
      "  validation loss:\t\t0.709189\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.754672\n",
      "  test accuracy:\t\t55.00 %\n",
      "2017-08-20 16:06:49.928372: step 400, loss = 0.59 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 16:07:39.867306: step 500, loss = 0.61 (164.3 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 16:08:32.404139: step 600, loss = 0.69 (165.4 examples/sec; 0.387 sec/batch)\n",
      "Epoch 2 of 10 took 182.143s\n",
      "  training loss:\t\t0.601473\n",
      "  training accuracy:\t\t73.09 %\n",
      "  validation loss:\t\t0.680156\n",
      "  validation accuracy:\t\t72.47 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.824423\n",
      "  test accuracy:\t\t55.38 %\n",
      "2017-08-20 16:09:43.286942: step 700, loss = 0.44 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:10:35.213592: step 800, loss = 0.50 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:11:32.400338: step 900, loss = 0.53 (167.0 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 16:12:29.373940: step 1000, loss = 0.44 (163.0 examples/sec; 0.393 sec/batch)\n",
      "Epoch 3 of 10 took 196.349s\n",
      "  training loss:\t\t0.477934\n",
      "  training accuracy:\t\t83.85 %\n",
      "  validation loss:\t\t0.893590\n",
      "  validation accuracy:\t\t66.11 %\n",
      "2017-08-20 16:13:44.916761: step 1100, loss = 0.33 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:14:54.870010: step 1200, loss = 0.35 (166.6 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 16:15:57.642562: step 1300, loss = 0.25 (166.9 examples/sec; 0.383 sec/batch)\n",
      "Epoch 4 of 10 took 227.815s\n",
      "  training loss:\t\t0.320846\n",
      "  training accuracy:\t\t94.29 %\n",
      "  validation loss:\t\t0.824104\n",
      "  validation accuracy:\t\t69.44 %\n",
      "2017-08-20 16:16:56.342187: step 1400, loss = 0.27 (167.0 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 16:17:47.964927: step 1500, loss = 0.31 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:18:45.482020: step 1600, loss = 0.26 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 16:19:55.156838: step 1700, loss = 0.26 (164.8 examples/sec; 0.388 sec/batch)\n",
      "Epoch 5 of 10 took 214.843s\n",
      "  training loss:\t\t0.292635\n",
      "  training accuracy:\t\t95.92 %\n",
      "  validation loss:\t\t0.826335\n",
      "  validation accuracy:\t\t70.69 %\n",
      "2017-08-20 16:21:20.259603: step 1800, loss = 0.26 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 16:22:33.099491: step 1900, loss = 0.33 (161.4 examples/sec; 0.396 sec/batch)\n",
      "2017-08-20 16:23:42.999961: step 2000, loss = 0.34 (166.9 examples/sec; 0.384 sec/batch)\n",
      "Epoch 6 of 10 took 262.083s\n",
      "  training loss:\t\t0.277801\n",
      "  training accuracy:\t\t96.82 %\n",
      "  validation loss:\t\t0.778846\n",
      "  validation accuracy:\t\t73.44 %\n",
      "Test results:\n",
      "  test loss:\t\t\t1.307046\n",
      "  test accuracy:\t\t46.74 %\n",
      "2017-08-20 16:25:20.530731: step 2100, loss = 0.24 (165.1 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 16:26:36.303485: step 2200, loss = 0.25 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 16:27:56.650919: step 2300, loss = 0.25 (164.9 examples/sec; 0.388 sec/batch)\n",
      "Epoch 7 of 10 took 277.709s\n",
      "  training loss:\t\t0.264731\n",
      "  training accuracy:\t\t97.52 %\n",
      "  validation loss:\t\t0.827791\n",
      "  validation accuracy:\t\t70.87 %\n",
      "2017-08-20 16:29:29.045946: step 2400, loss = 0.32 (165.9 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 16:30:42.446164: step 2500, loss = 0.27 (167.5 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 16:31:51.647604: step 2600, loss = 0.22 (169.4 examples/sec; 0.378 sec/batch)\n",
      "2017-08-20 16:32:50.767136: step 2700, loss = 0.33 (163.0 examples/sec; 0.393 sec/batch)\n",
      "Epoch 8 of 10 took 241.985s\n",
      "  training loss:\t\t0.263356\n",
      "  training accuracy:\t\t97.65 %\n",
      "  validation loss:\t\t0.815748\n",
      "  validation accuracy:\t\t71.70 %\n",
      "2017-08-20 16:34:02.173370: step 2800, loss = 0.22 (162.3 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 16:35:01.488799: step 2900, loss = 0.24 (167.4 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 16:36:10.980704: step 3000, loss = 0.35 (162.9 examples/sec; 0.393 sec/batch)\n",
      "Epoch 9 of 10 took 237.431s\n",
      "  training loss:\t\t0.262614\n",
      "  training accuracy:\t\t97.72 %\n",
      "  validation loss:\t\t0.805011\n",
      "  validation accuracy:\t\t72.47 %\n",
      "2017-08-20 16:37:37.074592: step 3100, loss = 0.28 (165.6 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 16:38:36.197938: step 3200, loss = 0.24 (165.9 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 16:39:37.503922: step 3300, loss = 0.25 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 16:40:31.952381: step 3400, loss = 0.25 (166.6 examples/sec; 0.384 sec/batch)\n",
      "Epoch 10 of 10 took 215.367s\n",
      "  training loss:\t\t0.259890\n",
      "  training accuracy:\t\t97.84 %\n",
      "  validation loss:\t\t0.822276\n",
      "  validation accuracy:\t\t71.22 %\n",
      "Beginning fold 3 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/2\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 16:41:47.421015: step 0, loss = 0.69 (47.4 examples/sec; 1.349 sec/batch)\n",
      "2017-08-20 16:42:49.730589: step 100, loss = 0.69 (165.0 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 16:43:53.510822: step 200, loss = 0.68 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 16:44:51.111341: step 300, loss = 0.69 (165.3 examples/sec; 0.387 sec/batch)\n",
      "Epoch 1 of 10 took 220.879s\n",
      "  training loss:\t\t0.683019\n",
      "  training accuracy:\t\t56.14 %\n",
      "  validation loss:\t\t0.663785\n",
      "  validation accuracy:\t\t62.85 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.684873\n",
      "  test accuracy:\t\t56.39 %\n",
      "2017-08-20 16:46:14.671120: step 400, loss = 0.65 (158.6 examples/sec; 0.403 sec/batch)\n",
      "2017-08-20 16:47:05.755738: step 500, loss = 0.61 (166.8 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 16:48:07.575899: step 600, loss = 0.68 (165.2 examples/sec; 0.387 sec/batch)\n",
      "Epoch 2 of 10 took 218.851s\n",
      "  training loss:\t\t0.642870\n",
      "  training accuracy:\t\t66.01 %\n",
      "  validation loss:\t\t0.617352\n",
      "  validation accuracy:\t\t72.74 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.707048\n",
      "  test accuracy:\t\t58.51 %\n",
      "2017-08-20 16:49:40.902904: step 700, loss = 0.65 (170.5 examples/sec; 0.375 sec/batch)\n",
      "2017-08-20 16:50:42.159080: step 800, loss = 0.47 (160.6 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 16:51:46.317702: step 900, loss = 0.56 (164.8 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 16:52:54.217384: step 1000, loss = 0.52 (167.9 examples/sec; 0.381 sec/batch)\n",
      "Epoch 3 of 10 took 237.220s\n",
      "  training loss:\t\t0.546159\n",
      "  training accuracy:\t\t77.81 %\n",
      "  validation loss:\t\t0.667362\n",
      "  validation accuracy:\t\t70.97 %\n",
      "2017-08-20 16:54:12.179887: step 1100, loss = 0.36 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 16:55:13.680293: step 1200, loss = 0.37 (166.2 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 16:56:30.871318: step 1300, loss = 0.34 (163.9 examples/sec; 0.390 sec/batch)\n",
      "Epoch 4 of 10 took 248.996s\n",
      "  training loss:\t\t0.407608\n",
      "  training accuracy:\t\t88.91 %\n",
      "  validation loss:\t\t0.720926\n",
      "  validation accuracy:\t\t68.68 %\n",
      "2017-08-20 16:58:03.456364: step 1400, loss = 0.33 (168.3 examples/sec; 0.380 sec/batch)\n",
      "2017-08-20 16:59:11.254392: step 1500, loss = 0.41 (167.4 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 17:00:13.004897: step 1600, loss = 0.53 (166.8 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 17:01:15.137089: step 1700, loss = 0.36 (162.1 examples/sec; 0.395 sec/batch)\n",
      "Epoch 5 of 10 took 237.009s\n",
      "  training loss:\t\t0.373269\n",
      "  training accuracy:\t\t91.26 %\n",
      "  validation loss:\t\t0.761827\n",
      "  validation accuracy:\t\t69.03 %\n",
      "2017-08-20 17:02:27.450414: step 1800, loss = 0.36 (165.5 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:03:32.063794: step 1900, loss = 0.31 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:04:43.672275: step 2000, loss = 0.28 (168.0 examples/sec; 0.381 sec/batch)\n",
      "Epoch 6 of 10 took 245.283s\n",
      "  training loss:\t\t0.348138\n",
      "  training accuracy:\t\t92.67 %\n",
      "  validation loss:\t\t0.811022\n",
      "  validation accuracy:\t\t67.29 %\n",
      "2017-08-20 17:06:18.413378: step 2100, loss = 0.34 (166.3 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 17:07:32.151503: step 2200, loss = 0.40 (166.9 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 17:08:35.084846: step 2300, loss = 0.36 (164.4 examples/sec; 0.389 sec/batch)\n",
      "Epoch 7 of 10 took 244.800s\n",
      "  training loss:\t\t0.330889\n",
      "  training accuracy:\t\t93.74 %\n",
      "  validation loss:\t\t0.810427\n",
      "  validation accuracy:\t\t67.81 %\n",
      "2017-08-20 17:09:46.256145: step 2400, loss = 0.27 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:10:58.983913: step 2500, loss = 0.35 (165.4 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:12:13.507411: step 2600, loss = 0.32 (169.1 examples/sec; 0.378 sec/batch)\n",
      "2017-08-20 17:13:27.524656: step 2700, loss = 0.33 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 8 of 10 took 263.783s\n",
      "  training loss:\t\t0.326307\n",
      "  training accuracy:\t\t93.91 %\n",
      "  validation loss:\t\t0.818416\n",
      "  validation accuracy:\t\t67.47 %\n",
      "2017-08-20 17:14:35.645034: step 2800, loss = 0.37 (166.3 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 17:15:16.302121: step 2900, loss = 0.33 (166.4 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 17:15:56.979683: step 3000, loss = 0.29 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 152.349s\n",
      "  training loss:\t\t0.324340\n",
      "  training accuracy:\t\t94.18 %\n",
      "  validation loss:\t\t0.809690\n",
      "  validation accuracy:\t\t67.60 %\n",
      "2017-08-20 17:16:48.512906: step 3100, loss = 0.34 (160.8 examples/sec; 0.398 sec/batch)\n",
      "2017-08-20 17:17:29.528825: step 3200, loss = 0.27 (162.7 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:18:10.601695: step 3300, loss = 0.36 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:18:52.174653: step 3400, loss = 0.28 (165.9 examples/sec; 0.386 sec/batch)\n",
      "Epoch 10 of 10 took 148.300s\n",
      "  training loss:\t\t0.323817\n",
      "  training accuracy:\t\t94.09 %\n",
      "  validation loss:\t\t0.816704\n",
      "  validation accuracy:\t\t68.02 %\n",
      "Beginning fold 4 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/3\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 17:19:27.668137: step 0, loss = 0.69 (42.2 examples/sec; 1.516 sec/batch)\n",
      "2017-08-20 17:20:09.796803: step 100, loss = 0.69 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:20:50.490162: step 200, loss = 0.68 (161.1 examples/sec; 0.397 sec/batch)\n",
      "2017-08-20 17:21:31.107878: step 300, loss = 0.67 (160.1 examples/sec; 0.400 sec/batch)\n",
      "Epoch 1 of 10 took 149.143s\n",
      "  training loss:\t\t0.681101\n",
      "  training accuracy:\t\t56.82 %\n",
      "  validation loss:\t\t0.652806\n",
      "  validation accuracy:\t\t66.08 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.664627\n",
      "  test accuracy:\t\t63.99 %\n",
      "2017-08-20 17:22:26.059165: step 400, loss = 0.67 (168.3 examples/sec; 0.380 sec/batch)\n",
      "2017-08-20 17:23:06.653210: step 500, loss = 0.66 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:23:47.684319: step 600, loss = 0.64 (163.5 examples/sec; 0.391 sec/batch)\n",
      "Epoch 2 of 10 took 146.212s\n",
      "  training loss:\t\t0.634839\n",
      "  training accuracy:\t\t68.70 %\n",
      "  validation loss:\t\t0.689398\n",
      "  validation accuracy:\t\t63.68 %\n",
      "2017-08-20 17:24:35.385268: step 700, loss = 0.70 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 17:25:16.137158: step 800, loss = 0.57 (171.0 examples/sec; 0.374 sec/batch)\n",
      "2017-08-20 17:25:56.680223: step 900, loss = 0.46 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:26:37.707765: step 1000, loss = 0.55 (163.2 examples/sec; 0.392 sec/batch)\n",
      "Epoch 3 of 10 took 148.337s\n",
      "  training loss:\t\t0.545779\n",
      "  training accuracy:\t\t78.29 %\n",
      "  validation loss:\t\t0.726581\n",
      "  validation accuracy:\t\t60.52 %\n",
      "2017-08-20 17:27:27.117613: step 1100, loss = 0.43 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:28:07.682033: step 1200, loss = 0.31 (166.6 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 17:28:48.412007: step 1300, loss = 0.38 (164.4 examples/sec; 0.389 sec/batch)\n",
      "Epoch 4 of 10 took 146.120s\n",
      "  training loss:\t\t0.407895\n",
      "  training accuracy:\t\t89.46 %\n",
      "  validation loss:\t\t0.918183\n",
      "  validation accuracy:\t\t56.04 %\n",
      "2017-08-20 17:29:36.339058: step 1400, loss = 0.34 (165.7 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 17:30:16.909535: step 1500, loss = 0.36 (168.2 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 17:30:57.807206: step 1600, loss = 0.34 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:31:38.534727: step 1700, loss = 0.35 (167.2 examples/sec; 0.383 sec/batch)\n",
      "Epoch 5 of 10 took 146.442s\n",
      "  training loss:\t\t0.363690\n",
      "  training accuracy:\t\t91.97 %\n",
      "  validation loss:\t\t0.897212\n",
      "  validation accuracy:\t\t57.71 %\n",
      "2017-08-20 17:32:26.197383: step 1800, loss = 0.31 (163.5 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:33:06.922696: step 1900, loss = 0.35 (166.6 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 17:33:47.905120: step 2000, loss = 0.31 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 6 of 10 took 149.331s\n",
      "  training loss:\t\t0.343604\n",
      "  training accuracy:\t\t93.39 %\n",
      "  validation loss:\t\t0.940299\n",
      "  validation accuracy:\t\t57.29 %\n",
      "2017-08-20 17:34:38.525996: step 2100, loss = 0.30 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 17:35:19.778404: step 2200, loss = 0.28 (165.4 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:36:00.644614: step 2300, loss = 0.35 (166.6 examples/sec; 0.384 sec/batch)\n",
      "Epoch 7 of 10 took 146.681s\n",
      "  training loss:\t\t0.327684\n",
      "  training accuracy:\t\t94.33 %\n",
      "  validation loss:\t\t0.925561\n",
      "  validation accuracy:\t\t57.60 %\n",
      "2017-08-20 17:36:48.261593: step 2400, loss = 0.37 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:37:28.999743: step 2500, loss = 0.34 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:38:09.698667: step 2600, loss = 0.26 (165.0 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 17:38:50.363577: step 2700, loss = 0.36 (172.0 examples/sec; 0.372 sec/batch)\n",
      "Epoch 8 of 10 took 146.377s\n",
      "  training loss:\t\t0.326041\n",
      "  training accuracy:\t\t94.51 %\n",
      "  validation loss:\t\t0.949529\n",
      "  validation accuracy:\t\t57.85 %\n",
      "2017-08-20 17:39:38.175470: step 2800, loss = 0.28 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 17:40:18.861790: step 2900, loss = 0.33 (161.9 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 17:40:59.737794: step 3000, loss = 0.29 (166.1 examples/sec; 0.385 sec/batch)\n",
      "Epoch 9 of 10 took 148.633s\n",
      "  training loss:\t\t0.323931\n",
      "  training accuracy:\t\t94.64 %\n",
      "  validation loss:\t\t0.935318\n",
      "  validation accuracy:\t\t58.51 %\n",
      "2017-08-20 17:41:49.791429: step 3100, loss = 0.29 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:42:30.303083: step 3200, loss = 0.27 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:43:10.972266: step 3300, loss = 0.34 (163.4 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 17:43:51.727848: step 3400, loss = 0.33 (170.2 examples/sec; 0.376 sec/batch)\n",
      "Epoch 10 of 10 took 146.214s\n",
      "  training loss:\t\t0.321816\n",
      "  training accuracy:\t\t94.56 %\n",
      "  validation loss:\t\t0.956669\n",
      "  validation accuracy:\t\t57.92 %\n",
      "Beginning fold 5 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/4\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 17:44:27.386842: step 0, loss = 0.70 (42.7 examples/sec; 1.500 sec/batch)\n",
      "2017-08-20 17:45:09.966906: step 100, loss = 0.69 (160.5 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 17:45:50.436969: step 200, loss = 0.70 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:46:31.050857: step 300, loss = 0.66 (143.4 examples/sec; 0.446 sec/batch)\n",
      "Epoch 1 of 10 took 149.377s\n",
      "  training loss:\t\t0.681809\n",
      "  training accuracy:\t\t56.49 %\n",
      "  validation loss:\t\t0.553734\n",
      "  validation accuracy:\t\t85.49 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.649153\n",
      "  test accuracy:\t\t65.94 %\n",
      "2017-08-20 17:47:26.102467: step 400, loss = 0.59 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:48:06.782130: step 500, loss = 0.63 (166.3 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 17:48:47.775142: step 600, loss = 0.58 (164.4 examples/sec; 0.389 sec/batch)\n",
      "Epoch 2 of 10 took 146.403s\n",
      "  training loss:\t\t0.607821\n",
      "  training accuracy:\t\t72.23 %\n",
      "  validation loss:\t\t0.651332\n",
      "  validation accuracy:\t\t67.60 %\n",
      "2017-08-20 17:49:35.540110: step 700, loss = 0.50 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 17:50:16.068874: step 800, loss = 0.39 (162.8 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 17:50:56.764020: step 900, loss = 0.44 (165.9 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 17:51:37.933215: step 1000, loss = 0.26 (165.3 examples/sec; 0.387 sec/batch)\n",
      "Epoch 3 of 10 took 148.243s\n",
      "  training loss:\t\t0.404919\n",
      "  training accuracy:\t\t88.56 %\n",
      "  validation loss:\t\t0.728421\n",
      "  validation accuracy:\t\t66.28 %\n",
      "2017-08-20 17:52:27.202052: step 1100, loss = 0.27 (167.5 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 17:53:07.860038: step 1200, loss = 0.23 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:53:48.690458: step 1300, loss = 0.25 (164.1 examples/sec; 0.390 sec/batch)\n",
      "Epoch 4 of 10 took 146.171s\n",
      "  training loss:\t\t0.270499\n",
      "  training accuracy:\t\t97.25 %\n",
      "  validation loss:\t\t1.155318\n",
      "  validation accuracy:\t\t52.22 %\n",
      "2017-08-20 17:54:36.366697: step 1400, loss = 0.33 (169.5 examples/sec; 0.378 sec/batch)\n",
      "2017-08-20 17:55:17.029831: step 1500, loss = 0.28 (159.5 examples/sec; 0.401 sec/batch)\n",
      "2017-08-20 17:55:59.139898: step 1600, loss = 0.24 (159.1 examples/sec; 0.402 sec/batch)\n",
      "2017-08-20 17:56:39.864693: step 1700, loss = 0.26 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 5 of 10 took 147.605s\n",
      "  training loss:\t\t0.261668\n",
      "  training accuracy:\t\t97.70 %\n",
      "  validation loss:\t\t1.117831\n",
      "  validation accuracy:\t\t56.63 %\n",
      "2017-08-20 17:57:27.548832: step 1800, loss = 0.25 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 17:58:08.265375: step 1900, loss = 0.29 (166.5 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 17:58:49.147863: step 2000, loss = 0.25 (166.4 examples/sec; 0.385 sec/batch)\n",
      "Epoch 6 of 10 took 148.050s\n",
      "  training loss:\t\t0.256211\n",
      "  training accuracy:\t\t97.99 %\n",
      "  validation loss:\t\t1.152450\n",
      "  validation accuracy:\t\t52.60 %\n",
      "2017-08-20 17:59:38.619655: step 2100, loss = 0.23 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:00:19.574621: step 2200, loss = 0.23 (166.0 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:01:00.398444: step 2300, loss = 0.23 (166.8 examples/sec; 0.384 sec/batch)\n",
      "Epoch 7 of 10 took 147.663s\n",
      "  training loss:\t\t0.248235\n",
      "  training accuracy:\t\t98.51 %\n",
      "  validation loss:\t\t1.158719\n",
      "  validation accuracy:\t\t53.82 %\n",
      "2017-08-20 18:01:49.306157: step 2400, loss = 0.24 (168.1 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 18:02:29.868392: step 2500, loss = 0.25 (163.5 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:03:10.576836: step 2600, loss = 0.22 (167.0 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 18:03:51.313724: step 2700, loss = 0.24 (162.4 examples/sec; 0.394 sec/batch)\n",
      "Epoch 8 of 10 took 146.243s\n",
      "  training loss:\t\t0.247182\n",
      "  training accuracy:\t\t98.63 %\n",
      "  validation loss:\t\t1.190734\n",
      "  validation accuracy:\t\t53.12 %\n",
      "2017-08-20 18:04:38.936728: step 2800, loss = 0.25 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:05:19.831158: step 2900, loss = 0.24 (150.8 examples/sec; 0.424 sec/batch)\n",
      "2017-08-20 18:06:00.678719: step 3000, loss = 0.26 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 148.303s\n",
      "  training loss:\t\t0.246560\n",
      "  training accuracy:\t\t98.64 %\n",
      "  validation loss:\t\t1.210113\n",
      "  validation accuracy:\t\t52.85 %\n",
      "2017-08-20 18:06:50.300415: step 3100, loss = 0.25 (166.8 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:07:30.879207: step 3200, loss = 0.25 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:08:11.616750: step 3300, loss = 0.27 (164.2 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:08:52.439696: step 3400, loss = 0.25 (166.2 examples/sec; 0.385 sec/batch)\n",
      "Epoch 10 of 10 took 146.292s\n",
      "  training loss:\t\t0.245981\n",
      "  training accuracy:\t\t98.62 %\n",
      "  validation loss:\t\t1.200293\n",
      "  validation accuracy:\t\t53.47 %\n",
      "Beginning fold 6 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/5\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 18:09:27.607548: step 0, loss = 0.70 (43.3 examples/sec; 1.477 sec/batch)\n",
      "2017-08-20 18:10:11.434951: step 100, loss = 0.69 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:10:51.629588: step 200, loss = 0.68 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:11:32.018738: step 300, loss = 0.64 (158.7 examples/sec; 0.403 sec/batch)\n",
      "Epoch 1 of 10 took 149.749s\n",
      "  training loss:\t\t0.676409\n",
      "  training accuracy:\t\t58.32 %\n",
      "  validation loss:\t\t0.717621\n",
      "  validation accuracy:\t\t51.94 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.620830\n",
      "  test accuracy:\t\t71.08 %\n",
      "2017-08-20 18:12:26.391240: step 400, loss = 0.64 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:13:06.862371: step 500, loss = 0.62 (161.3 examples/sec; 0.397 sec/batch)\n",
      "2017-08-20 18:13:47.158189: step 600, loss = 0.53 (163.7 examples/sec; 0.391 sec/batch)\n",
      "Epoch 2 of 10 took 145.085s\n",
      "  training loss:\t\t0.600188\n",
      "  training accuracy:\t\t73.06 %\n",
      "  validation loss:\t\t0.802491\n",
      "  validation accuracy:\t\t57.57 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.657806\n",
      "  test accuracy:\t\t68.89 %\n",
      "2017-08-20 18:14:41.544905: step 700, loss = 0.60 (165.0 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 18:15:21.881428: step 800, loss = 0.39 (165.6 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:16:02.392064: step 900, loss = 0.46 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:16:42.813319: step 1000, loss = 0.34 (162.7 examples/sec; 0.393 sec/batch)\n",
      "Epoch 3 of 10 took 146.846s\n",
      "  training loss:\t\t0.502072\n",
      "  training accuracy:\t\t81.09 %\n",
      "  validation loss:\t\t0.845699\n",
      "  validation accuracy:\t\t61.01 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.665054\n",
      "  test accuracy:\t\t71.25 %\n",
      "2017-08-20 18:17:38.775632: step 1100, loss = 0.38 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:18:19.268429: step 1200, loss = 0.32 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:18:59.725664: step 1300, loss = 0.37 (167.9 examples/sec; 0.381 sec/batch)\n",
      "Epoch 4 of 10 took 145.320s\n",
      "  training loss:\t\t0.341860\n",
      "  training accuracy:\t\t93.20 %\n",
      "  validation loss:\t\t0.834935\n",
      "  validation accuracy:\t\t63.51 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.719937\n",
      "  test accuracy:\t\t69.97 %\n",
      "2017-08-20 18:19:54.001032: step 1400, loss = 0.39 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:20:34.314432: step 1500, loss = 0.36 (165.8 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:21:14.668358: step 1600, loss = 0.26 (165.8 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:21:55.187830: step 1700, loss = 0.26 (166.9 examples/sec; 0.383 sec/batch)\n",
      "Epoch 5 of 10 took 145.072s\n",
      "  training loss:\t\t0.318690\n",
      "  training accuracy:\t\t94.64 %\n",
      "  validation loss:\t\t0.841099\n",
      "  validation accuracy:\t\t64.76 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.742549\n",
      "  test accuracy:\t\t69.34 %\n",
      "2017-08-20 18:22:49.212288: step 1800, loss = 0.32 (170.5 examples/sec; 0.375 sec/batch)\n",
      "2017-08-20 18:23:29.624425: step 1900, loss = 0.30 (150.8 examples/sec; 0.424 sec/batch)\n",
      "2017-08-20 18:24:10.193066: step 2000, loss = 0.26 (163.4 examples/sec; 0.392 sec/batch)\n",
      "Epoch 6 of 10 took 146.605s\n",
      "  training loss:\t\t0.298030\n",
      "  training accuracy:\t\t95.87 %\n",
      "  validation loss:\t\t0.867687\n",
      "  validation accuracy:\t\t65.45 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.808780\n",
      "  test accuracy:\t\t65.66 %\n",
      "2017-08-20 18:25:05.866559: step 2100, loss = 0.26 (167.7 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 18:25:46.222981: step 2200, loss = 0.27 (163.8 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:26:26.682671: step 2300, loss = 0.29 (171.0 examples/sec; 0.374 sec/batch)\n",
      "Epoch 7 of 10 took 145.191s\n",
      "  training loss:\t\t0.287997\n",
      "  training accuracy:\t\t96.54 %\n",
      "  validation loss:\t\t0.844884\n",
      "  validation accuracy:\t\t65.97 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.779448\n",
      "  test accuracy:\t\t68.85 %\n",
      "2017-08-20 18:27:21.122838: step 2400, loss = 0.36 (163.7 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:28:01.405540: step 2500, loss = 0.28 (165.9 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 18:28:41.776801: step 2600, loss = 0.24 (166.5 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:29:22.102316: step 2700, loss = 0.25 (170.5 examples/sec; 0.375 sec/batch)\n",
      "Epoch 8 of 10 took 144.894s\n",
      "  training loss:\t\t0.285933\n",
      "  training accuracy:\t\t96.68 %\n",
      "  validation loss:\t\t0.856136\n",
      "  validation accuracy:\t\t65.56 %\n",
      "2017-08-20 18:30:09.269970: step 2800, loss = 0.33 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:30:49.489132: step 2900, loss = 0.24 (164.7 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 18:31:29.891913: step 3000, loss = 0.28 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 146.258s\n",
      "  training loss:\t\t0.283224\n",
      "  training accuracy:\t\t96.77 %\n",
      "  validation loss:\t\t0.863087\n",
      "  validation accuracy:\t\t65.97 %\n",
      "2017-08-20 18:32:18.755079: step 3100, loss = 0.31 (166.4 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 18:32:59.134181: step 3200, loss = 0.26 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:33:39.428450: step 3300, loss = 0.23 (163.3 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 18:34:19.726186: step 3400, loss = 0.27 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 10 of 10 took 144.939s\n",
      "  training loss:\t\t0.281375\n",
      "  training accuracy:\t\t96.78 %\n",
      "  validation loss:\t\t0.855611\n",
      "  validation accuracy:\t\t65.76 %\n",
      "Beginning fold 7 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/6\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 18:34:55.410207: step 0, loss = 0.70 (42.2 examples/sec; 1.517 sec/batch)\n",
      "2017-08-20 18:35:37.350411: step 100, loss = 0.70 (166.6 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:36:17.509742: step 200, loss = 0.68 (165.3 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:36:57.716895: step 300, loss = 0.63 (166.0 examples/sec; 0.385 sec/batch)\n",
      "Epoch 1 of 10 took 147.693s\n",
      "  training loss:\t\t0.681593\n",
      "  training accuracy:\t\t56.56 %\n",
      "  validation loss:\t\t0.670868\n",
      "  validation accuracy:\t\t60.24 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.657891\n",
      "  test accuracy:\t\t67.57 %\n",
      "2017-08-20 18:37:52.068502: step 400, loss = 0.62 (162.3 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 18:38:32.389477: step 500, loss = 0.63 (163.2 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 18:39:12.874195: step 600, loss = 0.62 (163.4 examples/sec; 0.392 sec/batch)\n",
      "Epoch 2 of 10 took 144.807s\n",
      "  training loss:\t\t0.619339\n",
      "  training accuracy:\t\t70.63 %\n",
      "  validation loss:\t\t0.620227\n",
      "  validation accuracy:\t\t70.90 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.655417\n",
      "  test accuracy:\t\t67.78 %\n",
      "2017-08-20 18:40:06.833699: step 700, loss = 0.63 (167.2 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 18:40:47.051690: step 800, loss = 0.58 (166.5 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:41:27.350033: step 900, loss = 0.46 (164.9 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 18:42:07.738184: step 1000, loss = 0.41 (161.1 examples/sec; 0.397 sec/batch)\n",
      "Epoch 3 of 10 took 146.281s\n",
      "  training loss:\t\t0.535220\n",
      "  training accuracy:\t\t79.77 %\n",
      "  validation loss:\t\t0.552614\n",
      "  validation accuracy:\t\t75.21 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.703803\n",
      "  test accuracy:\t\t67.26 %\n",
      "2017-08-20 18:43:03.357256: step 1100, loss = 0.46 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:43:43.670453: step 1200, loss = 0.43 (163.4 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 18:44:23.982181: step 1300, loss = 0.30 (165.8 examples/sec; 0.386 sec/batch)\n",
      "Epoch 4 of 10 took 144.762s\n",
      "  training loss:\t\t0.393347\n",
      "  training accuracy:\t\t90.83 %\n",
      "  validation loss:\t\t0.562376\n",
      "  validation accuracy:\t\t77.71 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.805605\n",
      "  test accuracy:\t\t65.62 %\n",
      "2017-08-20 18:45:18.100551: step 1400, loss = 0.29 (166.2 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 18:45:58.320818: step 1500, loss = 0.45 (168.0 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 18:46:38.598722: step 1600, loss = 0.39 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:47:18.876807: step 1700, loss = 0.30 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 5 of 10 took 144.588s\n",
      "  training loss:\t\t0.364152\n",
      "  training accuracy:\t\t92.39 %\n",
      "  validation loss:\t\t0.588287\n",
      "  validation accuracy:\t\t75.21 %\n",
      "2017-08-20 18:48:06.060328: step 1800, loss = 0.38 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:48:46.422452: step 1900, loss = 0.30 (166.7 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:49:26.785135: step 2000, loss = 0.40 (163.0 examples/sec; 0.393 sec/batch)\n",
      "Epoch 6 of 10 took 146.443s\n",
      "  training loss:\t\t0.341187\n",
      "  training accuracy:\t\t93.31 %\n",
      "  validation loss:\t\t0.578726\n",
      "  validation accuracy:\t\t76.98 %\n",
      "2017-08-20 18:50:15.613728: step 2100, loss = 0.31 (166.7 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:50:55.951481: step 2200, loss = 0.30 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:51:36.237475: step 2300, loss = 0.43 (163.8 examples/sec; 0.391 sec/batch)\n",
      "Epoch 7 of 10 took 144.633s\n",
      "  training loss:\t\t0.324865\n",
      "  training accuracy:\t\t94.24 %\n",
      "  validation loss:\t\t0.593918\n",
      "  validation accuracy:\t\t75.94 %\n",
      "2017-08-20 18:52:23.413723: step 2400, loss = 0.29 (165.5 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 18:53:03.734469: step 2500, loss = 0.26 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 18:53:44.046124: step 2600, loss = 0.31 (163.7 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:54:24.412116: step 2700, loss = 0.36 (162.9 examples/sec; 0.393 sec/batch)\n",
      "Epoch 8 of 10 took 144.996s\n",
      "  training loss:\t\t0.321988\n",
      "  training accuracy:\t\t94.44 %\n",
      "  validation loss:\t\t0.589565\n",
      "  validation accuracy:\t\t76.70 %\n",
      "2017-08-20 18:55:11.725865: step 2800, loss = 0.31 (166.5 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 18:55:52.149492: step 2900, loss = 0.31 (164.3 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 18:56:32.480303: step 3000, loss = 0.31 (163.3 examples/sec; 0.392 sec/batch)\n",
      "Epoch 9 of 10 took 146.724s\n",
      "  training loss:\t\t0.318768\n",
      "  training accuracy:\t\t94.38 %\n",
      "  validation loss:\t\t0.595500\n",
      "  validation accuracy:\t\t76.39 %\n",
      "2017-08-20 18:57:21.576367: step 3100, loss = 0.29 (166.9 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 18:58:02.041387: step 3200, loss = 0.37 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 18:58:42.417989: step 3300, loss = 0.25 (162.0 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 18:59:22.761303: step 3400, loss = 0.30 (162.9 examples/sec; 0.393 sec/batch)\n",
      "Epoch 10 of 10 took 145.180s\n",
      "  training loss:\t\t0.317111\n",
      "  training accuracy:\t\t94.55 %\n",
      "  validation loss:\t\t0.601745\n",
      "  validation accuracy:\t\t76.49 %\n",
      "Beginning fold 8 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/7\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 18:59:58.039756: step 0, loss = 0.69 (45.2 examples/sec; 1.416 sec/batch)\n",
      "2017-08-20 19:00:40.273533: step 100, loss = 0.68 (163.9 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:01:20.471092: step 200, loss = 0.66 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:02:00.699907: step 300, loss = 0.61 (164.6 examples/sec; 0.389 sec/batch)\n",
      "Epoch 1 of 10 took 147.806s\n",
      "  training loss:\t\t0.672245\n",
      "  training accuracy:\t\t59.10 %\n",
      "  validation loss:\t\t0.698312\n",
      "  validation accuracy:\t\t54.55 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.742998\n",
      "  test accuracy:\t\t56.11 %\n",
      "2017-08-20 19:02:54.958442: step 400, loss = 0.64 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:03:35.065499: step 500, loss = 0.62 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:04:15.378343: step 600, loss = 0.54 (166.3 examples/sec; 0.385 sec/batch)\n",
      "Epoch 2 of 10 took 144.515s\n",
      "  training loss:\t\t0.589576\n",
      "  training accuracy:\t\t73.89 %\n",
      "  validation loss:\t\t0.766431\n",
      "  validation accuracy:\t\t52.81 %\n",
      "2017-08-20 19:05:02.599355: step 700, loss = 0.55 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:05:42.775230: step 800, loss = 0.55 (158.4 examples/sec; 0.404 sec/batch)\n",
      "2017-08-20 19:06:22.966347: step 900, loss = 0.43 (167.5 examples/sec; 0.382 sec/batch)\n",
      "2017-08-20 19:07:03.204890: step 1000, loss = 0.40 (165.4 examples/sec; 0.387 sec/batch)\n",
      "Epoch 3 of 10 took 146.149s\n",
      "  training loss:\t\t0.508507\n",
      "  training accuracy:\t\t81.34 %\n",
      "  validation loss:\t\t0.573182\n",
      "  validation accuracy:\t\t75.42 %\n",
      "Test results:\n",
      "  test loss:\t\t\t1.117281\n",
      "  test accuracy:\t\t37.08 %\n",
      "2017-08-20 19:07:58.942948: step 1100, loss = 0.30 (165.6 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:08:39.167201: step 1200, loss = 0.33 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:09:19.497354: step 1300, loss = 0.39 (163.9 examples/sec; 0.391 sec/batch)\n",
      "Epoch 4 of 10 took 144.645s\n",
      "  training loss:\t\t0.351309\n",
      "  training accuracy:\t\t92.82 %\n",
      "  validation loss:\t\t0.765343\n",
      "  validation accuracy:\t\t62.92 %\n",
      "2017-08-20 19:10:06.757515: step 1400, loss = 0.33 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:10:47.103454: step 1500, loss = 0.35 (161.8 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 19:11:27.342839: step 1600, loss = 0.25 (163.3 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:12:07.596652: step 1700, loss = 0.28 (165.0 examples/sec; 0.388 sec/batch)\n",
      "Epoch 5 of 10 took 144.792s\n",
      "  training loss:\t\t0.321142\n",
      "  training accuracy:\t\t94.36 %\n",
      "  validation loss:\t\t0.790306\n",
      "  validation accuracy:\t\t63.16 %\n",
      "2017-08-20 19:12:54.748148: step 1800, loss = 0.27 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:13:35.066907: step 1900, loss = 0.33 (165.8 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 19:14:15.237786: step 2000, loss = 0.35 (163.8 examples/sec; 0.391 sec/batch)\n",
      "Epoch 6 of 10 took 146.096s\n",
      "  training loss:\t\t0.298942\n",
      "  training accuracy:\t\t95.56 %\n",
      "  validation loss:\t\t0.813672\n",
      "  validation accuracy:\t\t63.37 %\n",
      "2017-08-20 19:15:04.088031: step 2100, loss = 0.24 (165.2 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:15:44.355034: step 2200, loss = 0.31 (164.3 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:16:24.755741: step 2300, loss = 0.25 (167.0 examples/sec; 0.383 sec/batch)\n",
      "Epoch 7 of 10 took 144.732s\n",
      "  training loss:\t\t0.286413\n",
      "  training accuracy:\t\t96.39 %\n",
      "  validation loss:\t\t0.849662\n",
      "  validation accuracy:\t\t62.71 %\n",
      "2017-08-20 19:17:11.945267: step 2400, loss = 0.25 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:17:52.152055: step 2500, loss = 0.40 (166.1 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 19:18:32.322174: step 2600, loss = 0.23 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:19:12.569308: step 2700, loss = 0.28 (163.8 examples/sec; 0.391 sec/batch)\n",
      "Epoch 8 of 10 took 144.613s\n",
      "  training loss:\t\t0.284295\n",
      "  training accuracy:\t\t96.65 %\n",
      "  validation loss:\t\t0.837669\n",
      "  validation accuracy:\t\t62.92 %\n",
      "2017-08-20 19:19:59.858388: step 2800, loss = 0.33 (163.2 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:20:40.005090: step 2900, loss = 0.29 (163.0 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 19:21:20.287271: step 3000, loss = 0.30 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 146.004s\n",
      "  training loss:\t\t0.280926\n",
      "  training accuracy:\t\t96.72 %\n",
      "  validation loss:\t\t0.847253\n",
      "  validation accuracy:\t\t62.57 %\n",
      "2017-08-20 19:22:08.985791: step 3100, loss = 0.30 (163.4 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:22:49.257659: step 3200, loss = 0.26 (164.5 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:23:29.516749: step 3300, loss = 0.28 (162.0 examples/sec; 0.395 sec/batch)\n",
      "2017-08-20 19:24:09.685117: step 3400, loss = 0.31 (166.4 examples/sec; 0.385 sec/batch)\n",
      "Epoch 10 of 10 took 144.608s\n",
      "  training loss:\t\t0.281357\n",
      "  training accuracy:\t\t96.77 %\n",
      "  validation loss:\t\t0.845162\n",
      "  validation accuracy:\t\t63.02 %\n",
      "Beginning fold 9 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/8\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 19:24:45.231028: step 0, loss = 0.70 (46.0 examples/sec; 1.392 sec/batch)\n",
      "2017-08-20 19:25:27.371559: step 100, loss = 0.70 (167.0 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 19:26:07.810079: step 200, loss = 0.67 (165.4 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:26:48.405465: step 300, loss = 0.66 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 1 of 10 took 148.598s\n",
      "  training loss:\t\t0.683269\n",
      "  training accuracy:\t\t56.60 %\n",
      "  validation loss:\t\t0.654307\n",
      "  validation accuracy:\t\t67.43 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.675717\n",
      "  test accuracy:\t\t60.14 %\n",
      "2017-08-20 19:27:43.211447: step 400, loss = 0.63 (167.1 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 19:28:23.770040: step 500, loss = 0.67 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 19:29:04.349996: step 600, loss = 0.60 (165.6 examples/sec; 0.386 sec/batch)\n",
      "Epoch 2 of 10 took 145.646s\n",
      "  training loss:\t\t0.636694\n",
      "  training accuracy:\t\t68.32 %\n",
      "  validation loss:\t\t0.588214\n",
      "  validation accuracy:\t\t72.88 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.695989\n",
      "  test accuracy:\t\t59.86 %\n",
      "2017-08-20 19:29:58.972364: step 700, loss = 0.66 (166.7 examples/sec; 0.384 sec/batch)\n",
      "2017-08-20 19:30:39.589489: step 800, loss = 0.67 (160.6 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 19:31:20.112946: step 900, loss = 0.52 (169.0 examples/sec; 0.379 sec/batch)\n",
      "2017-08-20 19:32:00.659562: step 1000, loss = 0.51 (165.3 examples/sec; 0.387 sec/batch)\n",
      "Epoch 3 of 10 took 147.435s\n",
      "  training loss:\t\t0.566703\n",
      "  training accuracy:\t\t76.11 %\n",
      "  validation loss:\t\t0.651794\n",
      "  validation accuracy:\t\t66.70 %\n",
      "2017-08-20 19:32:49.921594: step 1100, loss = 0.49 (164.2 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:33:30.451617: step 1200, loss = 0.36 (168.6 examples/sec; 0.379 sec/batch)\n",
      "2017-08-20 19:34:11.517457: step 1300, loss = 0.37 (170.4 examples/sec; 0.376 sec/batch)\n",
      "Epoch 4 of 10 took 146.184s\n",
      "  training loss:\t\t0.418477\n",
      "  training accuracy:\t\t88.48 %\n",
      "  validation loss:\t\t0.681828\n",
      "  validation accuracy:\t\t69.38 %\n",
      "2017-08-20 19:34:59.112073: step 1400, loss = 0.34 (158.0 examples/sec; 0.405 sec/batch)\n",
      "2017-08-20 19:35:39.832094: step 1500, loss = 0.52 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:36:20.534070: step 1600, loss = 0.33 (168.1 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 19:37:01.171184: step 1700, loss = 0.33 (166.7 examples/sec; 0.384 sec/batch)\n",
      "Epoch 5 of 10 took 146.292s\n",
      "  training loss:\t\t0.376453\n",
      "  training accuracy:\t\t91.15 %\n",
      "  validation loss:\t\t0.754591\n",
      "  validation accuracy:\t\t66.81 %\n",
      "2017-08-20 19:37:48.870713: step 1800, loss = 0.28 (165.7 examples/sec; 0.386 sec/batch)\n",
      "2017-08-20 19:38:29.445598: step 1900, loss = 0.32 (165.3 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:39:10.054964: step 2000, loss = 0.50 (166.1 examples/sec; 0.385 sec/batch)\n",
      "Epoch 6 of 10 took 147.475s\n",
      "  training loss:\t\t0.346368\n",
      "  training accuracy:\t\t92.78 %\n",
      "  validation loss:\t\t0.725402\n",
      "  validation accuracy:\t\t70.03 %\n",
      "2017-08-20 19:39:59.315997: step 2100, loss = 0.29 (162.6 examples/sec; 0.394 sec/batch)\n",
      "2017-08-20 19:40:41.603075: step 2200, loss = 0.45 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:41:22.251304: step 2300, loss = 0.33 (164.8 examples/sec; 0.388 sec/batch)\n",
      "Epoch 7 of 10 took 147.565s\n",
      "  training loss:\t\t0.329086\n",
      "  training accuracy:\t\t93.90 %\n",
      "  validation loss:\t\t0.740043\n",
      "  validation accuracy:\t\t69.38 %\n",
      "2017-08-20 19:42:09.898626: step 2400, loss = 0.31 (164.7 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:42:50.516435: step 2500, loss = 0.27 (172.9 examples/sec; 0.370 sec/batch)\n",
      "2017-08-20 19:43:31.289439: step 2600, loss = 0.27 (148.3 examples/sec; 0.432 sec/batch)\n",
      "2017-08-20 19:44:12.056785: step 2700, loss = 0.28 (166.1 examples/sec; 0.385 sec/batch)\n",
      "Epoch 8 of 10 took 146.331s\n",
      "  training loss:\t\t0.324326\n",
      "  training accuracy:\t\t94.08 %\n",
      "  validation loss:\t\t0.763067\n",
      "  validation accuracy:\t\t68.75 %\n",
      "2017-08-20 19:44:59.747902: step 2800, loss = 0.28 (165.4 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 19:45:40.519796: step 2900, loss = 0.39 (163.1 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 19:46:21.256271: step 3000, loss = 0.28 (165.5 examples/sec; 0.387 sec/batch)\n",
      "Epoch 9 of 10 took 147.939s\n",
      "  training loss:\t\t0.322283\n",
      "  training accuracy:\t\t94.20 %\n",
      "  validation loss:\t\t0.758514\n",
      "  validation accuracy:\t\t69.34 %\n",
      "2017-08-20 19:47:10.615889: step 3100, loss = 0.30 (169.1 examples/sec; 0.379 sec/batch)\n",
      "2017-08-20 19:47:51.231863: step 3200, loss = 0.31 (171.2 examples/sec; 0.374 sec/batch)\n",
      "2017-08-20 19:48:31.906695: step 3300, loss = 0.33 (164.4 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 19:49:12.753608: step 3400, loss = 0.28 (164.3 examples/sec; 0.390 sec/batch)\n",
      "Epoch 10 of 10 took 146.347s\n",
      "  training loss:\t\t0.321392\n",
      "  training accuracy:\t\t94.40 %\n",
      "  validation loss:\t\t0.754682\n",
      "  validation accuracy:\t\t69.41 %\n",
      "Beginning fold 10 out of 10\n",
      "Splitting the data...\n",
      "Preprocessing data...\n",
      "****************************************\n",
      "subset: train\n",
      "data_dir: /braintree/data2/active/users/bashivan/Data/fmri_conv\n",
      "num_epochs_per_decay: 3\n",
      "num_gpus: 1\n",
      "train_dir: /braintree/data2/active/users/bashivan/results/temp/9\n",
      "batch_size: 64\n",
      "model_type: lstm\n",
      "num_checkpoints_tosave: 5\n",
      "initial_learning_rate: 0.0001\n",
      "num_examples: None\n",
      "num_epochs: 10\n",
      "checkpoint_dir: .\n",
      "learning_rate_decay_factor: 0.1\n",
      "fold_to_run: -1\n",
      "num_folds: 10\n",
      "seed: 0\n",
      "eval_dir: /om/user/bashivan/temp\n",
      "num_time_steps: 64\n",
      "log_device_placement: False\n",
      "****************************************\n",
      "Using ADAM optimizer...\n",
      "Using default loss (softmax-Xentropy)...\n",
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/tf_model.py:205: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /braintree/home/bashivan/anaconda2/envs/dldata/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-20 19:49:46.174500: step 0, loss = 0.69 (45.0 examples/sec; 1.424 sec/batch)\n",
      "2017-08-20 19:50:28.419918: step 100, loss = 0.69 (170.0 examples/sec; 0.376 sec/batch)\n",
      "2017-08-20 19:51:08.922947: step 200, loss = 0.69 (165.1 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 19:51:49.698361: step 300, loss = 0.68 (166.6 examples/sec; 0.384 sec/batch)\n",
      "Epoch 1 of 10 took 160.612s\n",
      "  training loss:\t\t0.688358\n",
      "  training accuracy:\t\t54.24 %\n",
      "  validation loss:\t\t0.619095\n",
      "  validation accuracy:\t\t76.91 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.636278\n",
      "  test accuracy:\t\t71.70 %\n",
      "2017-08-20 19:52:38.847259: step 400, loss = 0.64 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:53:19.770843: step 500, loss = 0.65 (167.1 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 19:54:00.518642: step 600, loss = 0.60 (162.7 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 19:54:41.388224: step 700, loss = 0.61 (164.0 examples/sec; 0.390 sec/batch)\n",
      "Epoch 2 of 10 took 158.554s\n",
      "  training loss:\t\t0.640583\n",
      "  training accuracy:\t\t66.77 %\n",
      "  validation loss:\t\t0.459790\n",
      "  validation accuracy:\t\t89.81 %\n",
      "Test results:\n",
      "  test loss:\t\t\t0.557880\n",
      "  test accuracy:\t\t78.82 %\n",
      "2017-08-20 19:55:30.508265: step 800, loss = 0.68 (164.2 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 19:56:10.974792: step 900, loss = 0.53 (160.5 examples/sec; 0.399 sec/batch)\n",
      "2017-08-20 19:56:51.863710: step 1000, loss = 0.68 (167.2 examples/sec; 0.383 sec/batch)\n",
      "2017-08-20 19:57:34.187361: step 1100, loss = 0.59 (168.2 examples/sec; 0.381 sec/batch)\n",
      "Epoch 3 of 10 took 159.532s\n",
      "  training loss:\t\t0.604752\n",
      "  training accuracy:\t\t71.92 %\n",
      "  validation loss:\t\t0.512191\n",
      "  validation accuracy:\t\t85.36 %\n",
      "2017-08-20 19:58:19.066600: step 1200, loss = 0.56 (167.9 examples/sec; 0.381 sec/batch)\n",
      "2017-08-20 19:58:59.701854: step 1300, loss = 0.54 (163.9 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 19:59:40.456841: step 1400, loss = 0.52 (165.5 examples/sec; 0.387 sec/batch)\n",
      "2017-08-20 20:00:21.315041: step 1500, loss = 0.54 (163.4 examples/sec; 0.392 sec/batch)\n",
      "Epoch 4 of 10 took 158.378s\n",
      "  training loss:\t\t0.545133\n",
      "  training accuracy:\t\t79.57 %\n",
      "  validation loss:\t\t0.486661\n",
      "  validation accuracy:\t\t85.76 %\n",
      "2017-08-20 20:01:06.754992: step 1600, loss = 0.52 (163.9 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 20:01:47.488310: step 1700, loss = 0.41 (163.4 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 20:02:28.336738: step 1800, loss = 0.45 (162.0 examples/sec; 0.395 sec/batch)\n",
      "Epoch 5 of 10 took 159.448s\n",
      "  training loss:\t\t0.483737\n",
      "  training accuracy:\t\t84.56 %\n",
      "  validation loss:\t\t0.483889\n",
      "  validation accuracy:\t\t85.07 %\n",
      "2017-08-20 20:03:14.123019: step 1900, loss = 0.43 (163.5 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 20:03:54.795891: step 2000, loss = 0.40 (164.3 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 20:04:40.131916: step 2100, loss = 0.38 (163.2 examples/sec; 0.392 sec/batch)\n",
      "2017-08-20 20:05:20.739200: step 2200, loss = 0.51 (164.8 examples/sec; 0.388 sec/batch)\n",
      "Epoch 6 of 10 took 162.710s\n",
      "  training loss:\t\t0.428053\n",
      "  training accuracy:\t\t87.96 %\n",
      "  validation loss:\t\t0.479271\n",
      "  validation accuracy:\t\t84.55 %\n",
      "2017-08-20 20:06:05.692266: step 2300, loss = 0.37 (165.0 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 20:06:46.502815: step 2400, loss = 0.38 (166.0 examples/sec; 0.385 sec/batch)\n",
      "2017-08-20 20:07:27.209566: step 2500, loss = 0.39 (164.8 examples/sec; 0.388 sec/batch)\n",
      "2017-08-20 20:08:07.910506: step 2600, loss = 0.38 (163.7 examples/sec; 0.391 sec/batch)\n",
      "Epoch 7 of 10 took 158.092s\n",
      "  training loss:\t\t0.417669\n",
      "  training accuracy:\t\t88.92 %\n",
      "  validation loss:\t\t0.493272\n",
      "  validation accuracy:\t\t83.51 %\n",
      "2017-08-20 20:08:52.718101: step 2700, loss = 0.49 (162.9 examples/sec; 0.393 sec/batch)\n",
      "2017-08-20 20:09:33.583131: step 2800, loss = 0.41 (163.6 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 20:10:14.223303: step 2900, loss = 0.42 (163.7 examples/sec; 0.391 sec/batch)\n",
      "2017-08-20 20:10:55.029209: step 3000, loss = 0.35 (163.1 examples/sec; 0.392 sec/batch)\n",
      "Epoch 8 of 10 took 160.692s\n",
      "  training loss:\t\t0.413289\n",
      "  training accuracy:\t\t89.03 %\n",
      "  validation loss:\t\t0.482016\n",
      "  validation accuracy:\t\t84.09 %\n",
      "2017-08-20 20:11:42.307201: step 3100, loss = 0.38 (164.4 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 20:12:22.882071: step 3200, loss = 0.40 (164.0 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 20:13:03.644685: step 3300, loss = 0.37 (164.7 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 20:13:44.264068: step 3400, loss = 0.37 (164.2 examples/sec; 0.390 sec/batch)\n",
      "Epoch 9 of 10 took 157.785s\n",
      "  training loss:\t\t0.408170\n",
      "  training accuracy:\t\t89.31 %\n",
      "  validation loss:\t\t0.494625\n",
      "  validation accuracy:\t\t82.81 %\n",
      "2017-08-20 20:14:29.016929: step 3500, loss = 0.39 (164.6 examples/sec; 0.389 sec/batch)\n",
      "2017-08-20 20:15:09.730954: step 3600, loss = 0.37 (164.1 examples/sec; 0.390 sec/batch)\n",
      "2017-08-20 20:15:50.431880: step 3700, loss = 0.37 (165.2 examples/sec; 0.387 sec/batch)\n",
      "Epoch 10 of 10 took 157.997s\n",
      "  training loss:\t\t0.406861\n",
      "  training accuracy:\t\t89.39 %\n",
      "  validation loss:\t\t0.492873\n",
      "  validation accuracy:\t\t84.03 %\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-19312a2ecec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Aggregate results and save as a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mfold_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m fold_results.to_pickle(\n\u001b[1;32m     65\u001b[0m 'cnn_{0}_results_sgd_{1}_fold{2}.pkl'.format(FLAGS.model_type, FLAGS.initial_learning_rate, ''.join([str(i) for i in fold_to_run])))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/braintree/home/bashivan/dropbox/Codes/3D_fMRI_CNN/tf_pipeline/')\n",
    "import os\n",
    "from tf_trainer import Trainer\n",
    "import logging \n",
    "from tf_model import TFModel\n",
    "from tf_dataset import TFDataset\n",
    "from tf_trainer import log_info_string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,2'\n",
    "\n",
    "FLAGS.initial_learning_rate = 0.0001\n",
    "FLAGS.batch_size = 64\n",
    "FLAGS.num_gpus = 1\n",
    "FLAGS.num_epochs_per_decay = 3\n",
    "FLAGS.num_time_steps = 64\n",
    "FLAGS.data_dir = '/braintree/data2/active/users/bashivan/Data/fmri_conv'\n",
    "\n",
    "\n",
    "if FLAGS.fold_to_run == -1:\n",
    "    fold_to_run = range(FLAGS.num_folds)\n",
    "else:\n",
    "    fold_to_run = [FLAGS.fold_to_run]\n",
    "\n",
    "# Load the dataset\n",
    "fold_pairs = []\n",
    "\n",
    "model = TFModel()\n",
    "dataset = TFDataset(data_dir=FLAGS.data_dir)\n",
    "tr = Trainer(model=model, dataset=dataset)\n",
    "print(\"Loading data...\")\n",
    "tr.load_data(random=False)\n",
    "\n",
    "sub_nums = tr.subjects\n",
    "subs_in_fold = np.ceil(np.max(sub_nums) / float(10))\n",
    "# n-fold cross validation\n",
    "fold_results = []\n",
    "for i in fold_to_run:\n",
    "    '''\n",
    "    for each kfold selects fold window to collect indices for test dataset and the rest becomes train\n",
    "    '''\n",
    "    test_ids = np.bitwise_and(sub_nums >= subs_in_fold * (i), sub_nums < subs_in_fold * (i + 1))\n",
    "    train_ids = ~ test_ids\n",
    "    fold_pairs.append((np.nonzero(train_ids)[0], np.nonzero(test_ids)[0]))\n",
    "\n",
    "train_dir = FLAGS.train_dir\n",
    "log_info_string('Start working on fold(s) {0}'.format(fold_to_run))\n",
    "for fold_num, fold in enumerate([fold_pairs[i] for i in fold_to_run]):\n",
    "    log_info_string('Beginning fold {0} out of {1}'.format(fold_num + 1, len(fold_pairs)))\n",
    "    FLAGS.train_dir = os.path.join(train_dir, str(fold_num))\n",
    "\n",
    "    print('Splitting the data...')\n",
    "    tr.split_data(fold)\n",
    "    print('Preprocessing data...')\n",
    "    tr.preprocess_data()\n",
    "\n",
    "    fold_results.append(tr.train(fold_num=fold_num))\n",
    "\n",
    "# Aggregate results and save as a pickle\n",
    "fold_results = pd.concat(fold_results)\n",
    "fold_results.to_pickle(\n",
    "'cnn_{0}_results_sgd_{1}_fold{2}.pkl'.format(FLAGS.model_type, FLAGS.initial_learning_rate, ''.join([str(i) for i in fold_to_run])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "fold_num = 0\n",
    "fold = fold_pairs[0]\n",
    "\n",
    "FLAGS.train_dir = os.path.join(FLAGS.train_dir, str(fold_num))\n",
    "\n",
    "print('Splitting the data...')\n",
    "tr.split_data(fold)\n",
    "print('Preprocessing data...')\n",
    "tr.preprocess_data()\n",
    "\n",
    "tr.train(fold_num=fold_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fold_results.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAEYCAYAAADPkTRJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd9834c790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "sns.factorplot(x='Epoch Num', y='Test Acc', hue='Fold Num', data=fold_results.aggregate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
